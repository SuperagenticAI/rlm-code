"""
DSPy Evaluation Framework Templates

This module provides templates for evaluating DSPy programs with various metrics.
Includes common metrics like accuracy, F1, precision, recall, and custom metrics.

Generated by RLM Code - Evaluation Template
"""


class EvaluationTemplates:
    """Registry of evaluation templates and metrics."""

    def __init__(self):
        self.metrics = {
            "accuracy": "Simple accuracy metric",
            "f1": "F1 score for classification",
            "precision": "Precision metric",
            "recall": "Recall metric",
            "exact_match": "Exact match for QA tasks",
            "answer_correctness": "Answer correctness for QA",
            "context_relevance": "Context relevance for RAG",
            "faithfulness": "Faithfulness metric for RAG",
            "rouge": "ROUGE score for summarization",
            "bleu": "BLEU score for generation",
            "semantic_similarity": "Semantic similarity using embeddings",
            "custom": "Custom metric template",
        }

    def generate_evaluation_script(
        self, metrics: list[str] = None, task_type: str = "classification"
    ) -> str:
        """Generate complete evaluation script with specified metrics."""

        if not metrics:
            metrics = ["accuracy"]

        return f'''"""
DSPy Evaluation Script

Complete evaluation framework with metrics: {", ".join(metrics)}

Generated by RLM Code - Evaluation Template
"""

import dspy
from dspy.evaluate import Evaluate
import json
from pathlib import Path
from typing import List, Dict, Any

# ============================================================================
# 1. YOUR MODULE (Replace with your actual module)
# ============================================================================

class YourSignature(dspy.Signature):
    """Your task signature."""
    input_text = dspy.InputField(desc="Input text")
    output = dspy.OutputField(desc="Output")


class YourModule(dspy.Module):
    """Your DSPy module to evaluate."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(YourSignature)

    def forward(self, input_text):
        return self.predictor(input_text=input_text)


# ============================================================================
# 2. METRIC FUNCTIONS
# ============================================================================

{self._generate_metric_functions(metrics, task_type)}


# ============================================================================
# 3. DATA LOADING
# ============================================================================

def load_test_data():
    """Load test dataset."""

    # Example data format - replace with your actual test data
    test_examples = [
        dspy.Example(input_text="example 1", output="label 1").with_inputs("input_text"),
        dspy.Example(input_text="example 2", output="label 2").with_inputs("input_text"),
        # Add 20-50 test examples
    ]

    return test_examples


# ============================================================================
# 4. EVALUATION RUNNER
# ============================================================================

def run_evaluation():
    """Run complete evaluation."""

    print("\\n" + "="*70)
    print("DSPy Program Evaluation")
    print("="*70 + "\\n")

    # Configure DSPy
    lm = dspy.LM(model='ollama/gpt-oss:20b')
    dspy.configure(lm=lm)
    print("✓ Configured language model\\n")

    # Load test data
    testset = load_test_data()
    print(f"✓ Loaded {{len(testset)}} test examples\\n")

    # Create module
    module = YourModule()
    print("✓ Created module\\n")

    # Run evaluation with each metric
    results = {{}}

{self._generate_evaluation_calls(metrics)}

    # Display results
    print("\\n" + "="*70)
    print("Evaluation Results")
    print("="*70 + "\\n")

    # Create results table
    print(f"{"Metric":<25} {"Score":<15} {"Status"}")
    print("-" * 70)

    for metric_name, score in results.items():
        status = "✓ Pass" if score >= 0.7 else "⚠️  Needs Improvement" if score >= 0.5 else "✗ Fail"
        print(f"{{metric_name:<25}} {{score:>6.2%}}     {{status}}")

    print("-" * 70)
    print(f"{"Average":<25} {{sum(results.values())/len(results):>6.2%}}")
    print()

    # Detailed analysis
    print("="*70)
    print("Analysis")
    print("="*70)
    print(f"Total examples evaluated: {{len(testset)}}")
    print(f"Metrics computed: {{len(results)}}")

    best_metric = max(results.items(), key=lambda x: x[1])
    worst_metric = min(results.items(), key=lambda x: x[1])

    print(f"\\nBest performing metric: {{best_metric[0]}} ({{best_metric[1]:.2%}})")
    print(f"Worst performing metric: {{worst_metric[0]}} ({{worst_metric[1]:.2%}})")
    print()

    # Save results
    output_dir = Path("evaluation")
    output_dir.mkdir(exist_ok=True)

    results_file = output_dir / "results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"✓ Saved results to {{results_file}}\\n")

    return results


# ============================================================================
# 5. MAIN
# ============================================================================

if __name__ == "__main__":
    results = run_evaluation()

    print("="*70)
    print("Next Steps:")
    print("="*70)
    print("1. Analyze results to identify weaknesses")
    print("2. Optimize module with /optimize")
    print("3. Re-evaluate to measure improvement")
    print()
'''

    def _generate_metric_functions(self, metrics: list[str], task_type: str) -> str:
        """Generate metric function code."""
        metric_code = []

        if "accuracy" in metrics:
            metric_code.append('''def accuracy_metric(example, pred, trace=None):
    """Calculate accuracy."""
    predicted = pred.output.strip().lower()
    expected = example.output.strip().lower()
    return 1.0 if predicted == expected else 0.0''')

        if "f1" in metrics:
            metric_code.append('''def f1_metric(example, pred, trace=None):
    """Calculate F1 score."""
    # Simplified F1 - extend for multi-class
    predicted = pred.output.strip().lower()
    expected = example.output.strip().lower()

    if predicted == expected:
        return 1.0  # Perfect precision and recall
    else:
        return 0.0  # Simplified - implement proper F1 for your task''')

        if "exact_match" in metrics:
            metric_code.append('''def exact_match_metric(example, pred, trace=None):
    """Exact match metric for QA."""
    predicted = pred.output.strip()
    expected = example.output.strip()
    return 1.0 if predicted == expected else 0.0''')

        if "answer_correctness" in metrics:
            metric_code.append('''def answer_correctness_metric(example, pred, trace=None):
    """Answer correctness for QA tasks."""
    predicted = pred.answer.strip().lower() if hasattr(pred, 'answer') else pred.output.strip().lower()
    expected = example.answer.strip().lower() if hasattr(example, 'answer') else example.output.strip().lower()

    # Check exact match
    if predicted == expected:
        return 1.0

    # Check if answer contains expected key terms
    expected_terms = set(expected.split())
    predicted_terms = set(predicted.split())
    overlap = len(expected_terms & predicted_terms)

    if len(expected_terms) > 0:
        return overlap / len(expected_terms)
    return 0.0''')

        if "context_relevance" in metrics:
            metric_code.append('''def context_relevance_metric(example, pred, trace=None):
    """Context relevance for RAG tasks."""
    # Check if retrieved context is relevant to the question
    question = example.question if hasattr(example, 'question') else example.input_text
    context = pred.context if hasattr(pred, 'context') else ""

    if not context:
        return 0.0

    # Simple relevance: check keyword overlap
    question_words = set(question.lower().split())
    context_words = set(context.lower().split())
    overlap = len(question_words & context_words)

    if len(question_words) > 0:
        return min(overlap / len(question_words), 1.0)
    return 0.0''')

        if "faithfulness" in metrics:
            metric_code.append('''def faithfulness_metric(example, pred, trace=None):
    """Faithfulness: answer is grounded in context."""
    answer = pred.answer if hasattr(pred, 'answer') else pred.output
    context = pred.context if hasattr(pred, 'context') else ""

    if not context or not answer:
        return 0.0

    # Check if answer terms appear in context
    answer_words = set(answer.lower().split())
    context_words = set(context.lower().split())
    overlap = len(answer_words & context_words)

    if len(answer_words) > 0:
        return overlap / len(answer_words)
    return 0.0''')

        if "rouge" in metrics:
            metric_code.append('''def rouge_metric(example, pred, trace=None):
    """ROUGE score for summarization tasks."""
    try:
        from rouge_score import rouge_scorer
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

        predicted = pred.output if hasattr(pred, 'output') else str(pred)
        expected = example.output if hasattr(example, 'output') else str(example)

        scores = scorer.score(expected, predicted)
        # Return average of ROUGE-1, ROUGE-2, ROUGE-L F1 scores
        return (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3.0
    except ImportError:
        print("⚠️  rouge-score not installed: pip install rouge-score")
        return 0.0''')

        if "bleu" in metrics:
            metric_code.append('''def bleu_metric(example, pred, trace=None):
    """BLEU score for generation tasks."""
    try:
        from nltk.translate.bleu_score import sentence_bleu

        predicted = pred.output.split() if hasattr(pred, 'output') else str(pred).split()
        expected = example.output.split() if hasattr(example, 'output') else str(example).split()

        return sentence_bleu([expected], predicted)
    except ImportError:
        print("⚠️  nltk not installed: pip install nltk")
        return 0.0''')

        if "semantic_similarity" in metrics:
            metric_code.append('''def semantic_similarity_metric(example, pred, trace=None):
    """Semantic similarity using embeddings."""
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')

        predicted = pred.output if hasattr(pred, 'output') else str(pred)
        expected = example.output if hasattr(example, 'output') else str(example)

        embeddings = model.encode([predicted, expected])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return float(similarity)
    except ImportError:
        print("⚠️  sentence-transformers not installed: pip install sentence-transformers")
        return 0.0''')

        if "custom" in metrics:
            metric_code.append('''def custom_metric(example, pred, trace=None):
    """Custom metric - implement your own logic."""
    # TODO: Implement your custom metric
    predicted = pred.output
    expected = example.output

    # Example: Check if key terms are present
    score = 0.0
    # Add your scoring logic here

    return score''')

        return "\n\n".join(metric_code)

    def _generate_evaluation_calls(self, metrics: list[str]) -> str:
        """Generate evaluation call code."""
        calls = []

        for metric in metrics:
            metric_func = f"{metric}_metric"
            calls.append(f"""    print(f"Evaluating with {metric}...")
    evaluator = Evaluate(
        devset=testset,
        metric={metric_func},
        num_threads=1,
        display_progress=True
    )
    results["{metric}"] = evaluator(module)
    print()""")

        return "\n\n".join(calls)
