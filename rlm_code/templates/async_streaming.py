"""
DSPy Async and Streaming Templates

This module provides templates for async/await and streaming support:
- asyncify: Convert synchronous programs to async
- streamify: Stream outputs incrementally
- Usage tracking, caching, and logging

Each template includes complete working code with examples.
"""

from dataclasses import dataclass


@dataclass
class AsyncStreamingInfo:
    """Information about async/streaming features."""

    name: str
    display_name: str
    description: str
    best_for: str
    requires: list
    difficulty: str
    keywords: list


class AsyncStreamingTemplates:
    """Registry of async/streaming templates."""

    def __init__(self):
        self.features = {
            "asyncify": self._asyncify_info(),
            "streamify": self._streamify_info(),
            "usage_tracking": self._usage_tracking_info(),
            "caching": self._caching_info(),
            "logging": self._logging_info(),
        }

    def list_all(self) -> list[AsyncStreamingInfo]:
        """List all available async/streaming features."""
        return list(self.features.values())

    def get_feature_code(self, name: str, use_case: str = "general") -> str | None:
        """Get complete code for a feature."""
        generators = {
            "asyncify": self._generate_asyncify,
            "streamify": self._generate_streamify,
            "usage_tracking": self._generate_usage_tracking,
            "caching": self._generate_caching,
            "logging": self._generate_logging,
        }

        generator = generators.get(name)
        return generator(use_case) if generator else None

    def search(self, query: str) -> list[AsyncStreamingInfo]:
        """Search features by keywords."""
        query_lower = query.lower()
        matches = []

        for feature in self.features.values():
            if (
                any(kw in query_lower for kw in feature.keywords)
                or query_lower in feature.best_for.lower()
            ):
                matches.append(feature)

        return matches

    # Feature Info Methods

    def _asyncify_info(self) -> AsyncStreamingInfo:
        return AsyncStreamingInfo(
            name="asyncify",
            display_name="Asyncify - Async/Await Support",
            description="Convert synchronous DSPy programs to async for parallel execution",
            best_for="Parallel execution, concurrent requests, async applications",
            requires=["asyncio", "asyncer"],
            difficulty="intermediate",
            keywords=["async", "await", "parallel", "concurrent", "asyncio"],
        )

    def _streamify_info(self) -> AsyncStreamingInfo:
        return AsyncStreamingInfo(
            name="streamify",
            display_name="Streamify - Streaming Output",
            description="Stream outputs incrementally instead of waiting for complete response",
            best_for="Real-time responses, user experience, long-running tasks",
            requires=["streaming support in LM"],
            difficulty="intermediate",
            keywords=["streaming", "stream", "incremental", "real-time", "progressive"],
        )

    def _usage_tracking_info(self) -> AsyncStreamingInfo:
        return AsyncStreamingInfo(
            name="usage_tracking",
            display_name="Usage Tracking",
            description="Track token usage, costs, and API calls",
            best_for="Cost monitoring, usage analytics, budget management",
            requires=["dspy.utils.usage_tracker"],
            difficulty="beginner",
            keywords=["usage", "tracking", "tokens", "cost", "analytics"],
        )

    def _caching_info(self) -> AsyncStreamingInfo:
        return AsyncStreamingInfo(
            name="caching",
            display_name="Caching",
            description="Cache LM responses to reduce costs and latency",
            best_for="Cost optimization, faster responses, repeated queries",
            requires=["dspy.utils.caching"],
            difficulty="beginner",
            keywords=["cache", "caching", "optimization", "cost", "performance"],
        )

    def _logging_info(self) -> AsyncStreamingInfo:
        return AsyncStreamingInfo(
            name="logging",
            display_name="Logging",
            description="Configure logging for debugging and monitoring",
            best_for="Debugging, monitoring, production logging",
            requires=["logging module"],
            difficulty="beginner",
            keywords=["logging", "debug", "monitor", "logs", "diagnostics"],
        )

    # Template Generation Methods

    def _generate_asyncify(self, use_case: str = "general") -> str:
        """Generate asyncify template."""
        return '''"""
Asyncify - Async/Await Support for DSPy Programs

Convert synchronous DSPy programs to async for parallel execution.

Generated by RLM Code - Async/Streaming Template
"""

import dspy
import asyncio
from dspy.utils.asyncify import asyncify

# ============================================================================
# 1. CREATE YOUR PROGRAM
# ============================================================================

class QASignature(dspy.Signature):
    """Answer questions."""
    question = dspy.InputField(desc="User's question")
    answer = dspy.OutputField(desc="Answer")


class QAProgram(dspy.Module):
    """Question answering program."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(QASignature)

    def forward(self, question: str):
        return self.predictor(question=question)


# ============================================================================
# 2. CONVERT TO ASYNC
# ============================================================================

# Create program
program = QAProgram()

# Convert to async
async_program = asyncify(program)

# ============================================================================
# 3. USE ASYNC PROGRAM
# ============================================================================

async def process_single_question():
    """Process a single question asynchronously."""
    result = await async_program(question="What is DSPy?")
    print(f"Answer: {result.answer}")
    return result


async def process_multiple_questions():
    """Process multiple questions in parallel."""
    questions = [
        "What is machine learning?",
        "What is deep learning?",
        "What is neural networks?",
    ]

    # Process all questions concurrently
    tasks = [async_program(question=q) for q in questions]
    results = await asyncio.gather(*tasks)

    for question, result in zip(questions, results):
        print(f"Q: {question}")
        print(f"A: {result.answer}\\n")

    return results


# ============================================================================
# 4. PARALLEL EXECUTION EXAMPLE
# ============================================================================

async def parallel_processing():
    """Process multiple programs in parallel."""

    # Create multiple programs
    qa_program = asyncify(QAProgram())
    classifier = asyncify(YourClassifierProgram())
    summarizer = asyncify(YourSummarizerProgram())

    # Run all in parallel
    qa_result, classification, summary = await asyncio.gather(
        qa_program(question="What is AI?"),
        classifier(text="Your text here"),
        summarizer(text="Long text to summarize")
    )

    return qa_result, classification, summary


# ============================================================================
# 5. CONFIGURE ASYNC SETTINGS
# ============================================================================

# Set max concurrent workers
dspy.settings.async_max_workers = 10  # Default is usually 5

# ============================================================================
# 6. BENEFITS OF ASYNCIFY
# ============================================================================

"""
Benefits:
- Parallel execution of multiple programs
- Better resource utilization
- Faster processing for batch operations
- Compatible with async web frameworks (FastAPI, etc.)
- Non-blocking execution
"""

# ============================================================================
# 7. MAIN
# ============================================================================

async def main():
    """Main async function."""
    print("Processing questions asynchronously...\\n")

    # Single question
    await process_single_question()

    print("\\n" + "="*70)
    print("Processing multiple questions in parallel...\\n")

    # Multiple questions
    await process_multiple_questions()


if __name__ == "__main__":
    # Run async main
    asyncio.run(main())
'''

    def _generate_streamify(self, use_case: str = "general") -> str:
        """Generate streamify template."""
        return '''"""
Streamify - Streaming Output Support

Stream outputs incrementally instead of waiting for complete response.

Generated by RLM Code - Async/Streaming Template
"""

import dspy
import asyncio
from dspy.streaming import streamify
from dspy.streaming.messages import StatusMessageProvider

# ============================================================================
# 1. CREATE YOUR PROGRAM
# ============================================================================

class QASignature(dspy.Signature):
    """Answer questions."""
    question = dspy.InputField(desc="User's question")
    answer = dspy.OutputField(desc="Answer")


class QAProgram(dspy.Module):
    """Question answering program."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(QASignature)

    def forward(self, question: str):
        return self.predictor(question=question)


# ============================================================================
# 2. CONVERT TO STREAMING
# ============================================================================

# Create program
program = QAProgram()

# Convert to streaming
streaming_program = streamify(
    program,
    include_final_prediction_in_output_stream=True,
    async_streaming=True
)

# ============================================================================
# 3. USE STREAMING PROGRAM
# ============================================================================

async def stream_response():
    """Stream response incrementally."""
    question = "What is machine learning?"

    print(f"Q: {question}\\n")
    print("A: ", end="", flush=True)

    async for chunk in streaming_program(question=question):
        if hasattr(chunk, 'answer'):
            # Stream the answer field
            print(chunk.answer, end="", flush=True)
        elif isinstance(chunk, str):
            # Stream text directly
            print(chunk, end="", flush=True)

    print("\\n")  # New line after streaming


# ============================================================================
# 4. CUSTOM STATUS MESSAGES
# ============================================================================

class CustomStatusProvider(StatusMessageProvider):
    """Custom status message provider."""

    def get_status_message(self, module_name: str, status: str) -> str:
        """Generate custom status messages."""
        messages = {
            "starting": f"ðŸš€ Starting {module_name}...",
            "processing": f"âš™ï¸  Processing with {module_name}...",
            "complete": f"âœ“ {module_name} complete",
        }
        return messages.get(status, f"{module_name}: {status}")


# Use custom status provider
streaming_with_status = streamify(
    program,
    status_message_provider=CustomStatusProvider(),
    include_final_prediction_in_output_stream=True
)


# ============================================================================
# 5. STREAM SPECIFIC FIELDS
# ============================================================================

from dspy.streaming.streaming_listener import StreamListener

# Create listener for specific field
answer_listener = StreamListener(
    target_module="QAProgram",
    target_field="answer"
)

streaming_with_listener = streamify(
    program,
    stream_listeners=[answer_listener],
    include_final_prediction_in_output_stream=True
)


# ============================================================================
# 6. BENEFITS OF STREAMIFY
# ============================================================================

"""
Benefits:
- Real-time user feedback
- Better user experience
- Progressive output display
- Lower perceived latency
- Works with streaming-capable models
"""

# ============================================================================
# 7. MAIN
# ============================================================================

async def main():
    """Main async function."""
    print("="*70)
    print("Streaming Response Example")
    print("="*70 + "\\n")

    await stream_response()


if __name__ == "__main__":
    asyncio.run(main())
'''

    def _generate_usage_tracking(self, use_case: str = "general") -> str:
        """Generate usage tracking template."""
        return '''"""
Usage Tracking - Monitor Token Usage and Costs

Track token usage, API calls, and costs for your DSPy programs.

Generated by RLM Code - Async/Streaming Template
"""

import dspy
from dspy.utils.usage_tracker import UsageTracker

# ============================================================================
# 1. ENABLE USAGE TRACKING
# ============================================================================

# Usage tracking is enabled by default in DSPy
# Configure DSPy
dspy.configure(
    lm=dspy.LM(model="openai/gpt-4o")
)

# ============================================================================
# 2. TRACK USAGE FOR A PROGRAM
# ============================================================================

class QASignature(dspy.Signature):
    """Answer questions."""
    question = dspy.InputField(desc="User's question")
    answer = dspy.OutputField(desc="Answer")


class QAProgram(dspy.Module):
    """Question answering program."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(QASignature)

    def forward(self, question: str):
        return self.predictor(question=question)


# Create program
program = QAProgram()

# ============================================================================
# 3. GET USAGE STATISTICS
# ============================================================================

def get_usage_stats():
    """Get current usage statistics."""
    tracker = UsageTracker()

    # Get total usage
    stats = tracker.get_stats()

    print("Usage Statistics:")
    print("="*70)
    print(f"Total tokens: {stats.get('total_tokens', 0)}")
    print(f"Total cost: ${stats.get('total_cost', 0):.4f}")
    print(f"API calls: {stats.get('api_calls', 0)}")
    print(f"Cache hits: {stats.get('cache_hits', 0)}")
    print(f"Cache misses: {stats.get('cache_misses', 0)}")

    return stats


# ============================================================================
# 4. TRACK USAGE FOR SPECIFIC CALLS
# ============================================================================

def track_single_call():
    """Track usage for a single program call."""
    # Reset tracker
    tracker = UsageTracker()
    tracker.reset()

    # Run program
    result = program(question="What is DSPy?")

    # Get usage for this call
    stats = tracker.get_stats()

    print(f"Tokens used: {stats.get('total_tokens', 0)}")
    print(f"Cost: ${stats.get('total_cost', 0):.4f}")

    return result, stats


# ============================================================================
# 5. TRACK USAGE OVER TIME
# ============================================================================

def track_batch_processing():
    """Track usage for batch processing."""
    questions = [
        "What is machine learning?",
        "What is deep learning?",
        "What is neural networks?",
    ]

    tracker = UsageTracker()
    tracker.reset()

    # Process all questions
    for question in questions:
        program(question=question)

    # Get total usage
    stats = tracker.get_stats()

    print(f"\\nBatch Processing Stats:")
    print(f"  Total tokens: {stats.get('total_tokens', 0)}")
    print(f"  Total cost: ${stats.get('total_cost', 0):.4f}")
    print(f"  Average per question: {stats.get('total_tokens', 0) / len(questions):.0f} tokens")

    return stats


# ============================================================================
# 6. EXPORT USAGE DATA
# ============================================================================

def export_usage_data():
    """Export usage data for analysis."""
    import json
    from pathlib import Path

    tracker = UsageTracker()
    stats = tracker.get_stats()

    # Export to JSON
    output_file = Path("usage_stats.json")
    with open(output_file, 'w') as f:
        json.dump(stats, f, indent=2)

    print(f"âœ“ Exported usage data to {output_file}")

    return stats


# ============================================================================
# 7. BENEFITS OF USAGE TRACKING
# ============================================================================

"""
Benefits:
- Monitor costs in real-time
- Optimize token usage
- Budget management
- Performance analysis
- Cost per request tracking
"""

# ============================================================================
# 8. MAIN
# ============================================================================

def main():
    """Main function."""
    print("="*70)
    print("Usage Tracking Example")
    print("="*70 + "\\n")

    # Track single call
    print("1. Single Call Tracking:")
    print("-" * 70)
    track_single_call()

    print("\\n2. Batch Processing Tracking:")
    print("-" * 70)
    track_batch_processing()

    print("\\n3. Export Usage Data:")
    print("-" * 70)
    export_usage_data()

    print("\\n4. Current Total Stats:")
    print("-" * 70)
    get_usage_stats()


if __name__ == "__main__":
    main()
'''

    def _generate_caching(self, use_case: str = "general") -> str:
        """Generate caching template."""
        return '''"""
Caching - Cache LM Responses

Cache LM responses to reduce costs and improve latency.

Generated by RLM Code - Async/Streaming Template
"""

import dspy
from dspy.utils.caching import Cache

# ============================================================================
# 1. ENABLE CACHING
# ============================================================================

# Caching is enabled by default in DSPy
# Configure cache directory
import os
os.environ['DSPY_CACHEDIR'] = './.dspy_cache'

# Configure DSPy
dspy.configure(
    lm=dspy.LM(model="openai/gpt-4o")
)

# ============================================================================
# 2. CACHE CONFIGURATION
# ============================================================================

# Set cache directory
cache_dir = "./.dspy_cache"

# Cache settings
cache_settings = {
    "enabled": True,
    "max_size_mb": 1000,  # Maximum cache size
    "ttl_seconds": 3600,  # Time to live (1 hour)
}

# ============================================================================
# 3. PROGRAM WITH CACHING
# ============================================================================

class QASignature(dspy.Signature):
    """Answer questions."""
    question = dspy.InputField(desc="User's question")
    answer = dspy.OutputField(desc="Answer")


class QAProgram(dspy.Module):
    """Question answering program with caching."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(QASignature)

    def forward(self, question: str):
        # Caching happens automatically
        return self.predictor(question=question)


# ============================================================================
# 4. CACHE STATISTICS
# ============================================================================

def get_cache_stats():
    """Get cache statistics."""
    from pathlib import Path

    cache_path = Path(cache_dir)
    if not cache_path.exists():
        print("Cache directory does not exist")
        return

    cache_files = list(cache_path.glob("*.cache"))

    print("Cache Statistics:")
    print("="*70)
    print(f"Cache directory: {cache_dir}")
    print(f"Cache files: {len(cache_files)}")

    total_size = sum(f.stat().st_size for f in cache_files)
    print(f"Total cache size: {total_size / 1024 / 1024:.2f} MB")

    return {
        "files": len(cache_files),
        "size_mb": total_size / 1024 / 1024
    }


# ============================================================================
# 5. CLEAR CACHE
# ============================================================================

def clear_cache():
    """Clear the cache."""
    from pathlib import Path
    import shutil

    cache_path = Path(cache_dir)
    if cache_path.exists():
        shutil.rmtree(cache_path)
        cache_path.mkdir()
        print(f"âœ“ Cleared cache at {cache_dir}")
    else:
        print(f"Cache directory {cache_dir} does not exist")


# ============================================================================
# 6. CACHE WITH ROLLOUT ID
# ============================================================================

def use_cache_with_rollout():
    """Use cache with rollout ID for fresh responses."""
    program = QAProgram()

    # First call - will be cached
    result1 = program(question="What is DSPy?")

    # Second call - will use cache (same result)
    result2 = program(question="What is DSPy?")

    # Third call with rollout_id - bypasses cache, gets fresh response
    result3 = program(
        question="What is DSPy?",
        config={"rollout_id": "fresh_1", "temperature": 1.0}
    )

    print(f"Result 1 (cached): {result1.answer}")
    print(f"Result 2 (from cache): {result2.answer}")
    print(f"Result 3 (fresh): {result3.answer}")


# ============================================================================
# 7. BENEFITS OF CACHING
# ============================================================================

"""
Benefits:
- Reduced API costs (repeated queries use cache)
- Faster responses (cache hits are instant)
- Consistent results for same inputs
- Can bypass cache with rollout_id when needed
- Automatic cache management
"""

# ============================================================================
# 8. MAIN
# ============================================================================

def main():
    """Main function."""
    print("="*70)
    print("Caching Example")
    print("="*70 + "\\n")

    program = QAProgram()

    # First call (will be cached)
    print("1. First call (will be cached):")
    result1 = program(question="What is machine learning?")
    print(f"   Answer: {result1.answer}\\n")

    # Second call (will use cache)
    print("2. Second call (uses cache):")
    result2 = program(question="What is machine learning?")
    print(f"   Answer: {result2.answer}\\n")

    # Cache stats
    print("3. Cache Statistics:")
    get_cache_stats()


if __name__ == "__main__":
    main()
'''

    def _generate_logging(self, use_case: str = "general") -> str:
        """Generate logging template."""
        return '''"""
Logging - Configure Logging for DSPy Programs

Set up logging for debugging and monitoring.

Generated by RLM Code - Async/Streaming Template
"""

import dspy
import logging
from dspy.utils.logging_utils import setup_logging

# ============================================================================
# 1. BASIC LOGGING SETUP
# ============================================================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dspy.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# ============================================================================
# 2. DSPy LOGGING CONFIGURATION
# ============================================================================

# Set DSPy log level
dspy.settings.log_level = logging.INFO

# Or use DSPy's logging utils
setup_logging(level=logging.INFO)


# ============================================================================
# 3. PROGRAM WITH LOGGING
# ============================================================================

class QASignature(dspy.Signature):
    """Answer questions."""
    question = dspy.InputField(desc="User's question")
    answer = dspy.OutputField(desc="Answer")


class QAProgram(dspy.Module):
    """Question answering program with logging."""

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(QASignature)
        self.logger = logging.getLogger(self.__class__.__name__)

    def forward(self, question: str):
        self.logger.info(f"Processing question: {question}")

        try:
            result = self.predictor(question=question)
            self.logger.info(f"Generated answer: {result.answer[:50]}...")
            return result
        except Exception as e:
            self.logger.error(f"Error processing question: {e}", exc_info=True)
            raise


# ============================================================================
# 4. DETAILED LOGGING
# ============================================================================

def detailed_logging_example():
    """Example with detailed logging."""
    logger = logging.getLogger("DetailedExample")
    logger.setLevel(logging.DEBUG)

    program = QAProgram()

    # Log before call
    logger.debug("About to call QA program")

    result = program(question="What is DSPy?")

    # Log after call
    logger.debug(f"Program returned: {result}")

    return result


# ============================================================================
# 5. LOGGING TO FILE
# ============================================================================

def setup_file_logging():
    """Set up file-based logging."""
    file_handler = logging.FileHandler(
        'dspy_program.log',
        mode='a',
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(
        logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    )

    logger = logging.getLogger()
    logger.addHandler(file_handler)

    return logger


# ============================================================================
# 6. STRUCTURED LOGGING
# ============================================================================

def structured_logging_example():
    """Example with structured logging."""
    import json

    def log_structured(level, message, **kwargs):
        log_entry = {
            "level": level,
            "message": message,
            **kwargs
        }
        print(json.dumps(log_entry))

    # Use structured logging
    log_structured("INFO", "Processing question", question="What is DSPy?")
    log_structured("INFO", "Generated answer", answer="DSPy is a framework...")


# ============================================================================
# 7. BENEFITS OF LOGGING
# ============================================================================

"""
Benefits:
- Debug issues in production
- Monitor program behavior
- Track performance metrics
- Audit trail of operations
- Troubleshooting support
"""

# ============================================================================
# 8. MAIN
# ============================================================================

def main():
    """Main function."""
    print("="*70)
    print("Logging Example")
    print("="*70 + "\\n")

    # Setup logging
    setup_file_logging()

    # Create program
    program = QAProgram()

    # Run with logging
    logger = logging.getLogger(__name__)
    logger.info("Starting QA program")

    result = program(question="What is machine learning?")

    logger.info("QA program completed")
    print(f"\\nAnswer: {result.answer}")


if __name__ == "__main__":
    main()
'''
