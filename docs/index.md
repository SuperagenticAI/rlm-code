<div class="rlm-hero" markdown>

<img src="assets/logo.png" alt="RLM Code" class="rlm-hero-logo">

# ğŸ§ª RLM Code

<p class="rlm-tagline">Research Playground & Evaluation OS for Recursive Language Model Agentic Systems</p>

<span class="rlm-badge rlm-badge--purple">v0.1.5</span>
<span class="rlm-badge rlm-badge--green">Python 3.11+</span>
<span class="rlm-badge rlm-badge--blue">Apache 2.0</span>

</div>

**RLM Code** is the definitive research operating system for building, running, evaluating, comparing, and optimizing LLM-based coding agents. It supports multiple agent paradigms including **Pure RLM**, **CodeAct**, and **Traditional** in a single unified platform with built-in safety, observability, and reproducibility.

---

## ğŸ¯ What RLM Code Solves

The underlying long-context reasoning problem is what **RLM (the method)** addresses.
**RLM Code** addresses the tooling and workflow problem around using that method in practice.

Core product problems it targets:

- **Implementation friction**: provide a runnable RLM environment (`llm_query`, REPL, run loop) without custom scaffolding.
- **Experiment management**: run, replay, compare, and benchmark experiments in one place.
- **Safety controls**: route execution through secure backends and explicit runtime settings.
- **Reproducibility**: store traces, metrics, and benchmark artifacts for repeatable research.
- **Operational visibility**: expose observability, status, and diagnostics for debugging experiments.

In short, RLM Code is a research tooling layer for building and evaluating RLM-style workflows.

---

## âœ¨ Highlights

<div class="rlm-features" markdown>

<div class="rlm-feature-card" markdown>

### ğŸ§  Multi-Paradigm Engine
Run **Pure RLM** (paper-compliant with context-as-variable), **CodeAct** (context-in-tokens), or **Traditional** agent orchestration, all from one TUI.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ”¬ Built-in Research Tab
A dedicated Research tab inside the TUI with **Dashboard**, **Trajectory**, **Benchmarks**, **Replay**, and **Live Events** sub-tabs for real-time experiment tracking.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ† Benchmarks & Leaderboard
**10 preset benchmarks** with 33+ test cases, a multi-metric leaderboard, and side-by-side paradigm comparison.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ” Session Replay
Time-travel through any RLM run step-by-step with **forward/backward navigation**, reward curve visualization, and checkpoint/restore.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ¯ Hot-Swappable Policies
Swap **reward**, **action selection**, **compaction**, and **termination** policies at runtime via the Policy Lab.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ”’ HITL Approval Gates
Risk assessment with **40+ rules**, 6 approval modes, and full audit logging to keep humans in the loop for every critical action.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ“Š Pluggable Observability
**7 sinks** including JSONL, MLflow, OpenTelemetry, LangSmith, LangFuse, and Logfire to trace every step of every run.

</div>

<div class="rlm-feature-card" markdown>

### ğŸ“¦ Sandbox Runtimes
**6 runtimes** including Local, Docker, Apple Container, Modal, E2B, and Daytona for safe, isolated code execution.

</div>

</div>

---

## ğŸš€ Quick Start

<div class="rlm-quickstart" markdown>

**Install and launch**

```bash
uv tool install "rlm-code[tui,llm-all]"
rlm-code
```

**Connect to a model**

```text
/connect anthropic claude-opus-4-6
```

**Run your first benchmark**

```text
/rlm bench preset=dspy_quick
```

**Keep runs bounded**

```text
/rlm run "small scoped task" steps=4 timeout=30 budget=60
/rlm abort all
```

**Compare benchmark output**

```text
/rlm bench compare candidate=latest baseline=previous
```

**Switch to the Research tab**

Press `Ctrl+5` or `F6` to open the **Research** tab to see your run's dashboard, trajectory, reward curves, and live events.

</div>

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    CLI["ğŸš€ rlm-code CLI"]
    CLI --> TUI["ğŸ–¥ï¸ Unified TUI"]
    TUI --> RLM["ğŸ” RLM"]
    TUI --> FILES["ğŸ“ Files"]
    TUI --> DETAILS["ğŸ“‹ Details"]
    TUI --> SHELL["âš¡ Shell"]
    TUI --> RESEARCH["ğŸ”¬ Research"]

    CLI --> CMD["âŒ¨ï¸ 50+ Slash Commands"]

    CMD --> RUNNER["ğŸ§  RLM Runner"]
    RUNNER --> EVENTS["ğŸ“¡ Event Bus (27+ types)"]
    RUNNER --> OBS["ğŸ“Š Observability (7 sinks)"]
    RUNNER --> TRAJ["ğŸ“ˆ Trajectory Logger"]
    RUNNER --> POL["ğŸ¯ Policy Lab"]
    RUNNER --> HITL["ğŸ”’ HITL Approval Gates"]

    RUNNER --> ENV["ğŸŒ Environments"]
    ENV --> PURE["Pure RLM"]
    ENV --> DSPY["DSPy Coding"]
    ENV --> GEN["Generic"]

    RUNNER --> SAND["ğŸ“¦ Sandbox Runtimes"]
    SAND --> LOCAL["Local"]
    SAND --> DOCKER["Docker"]
    SAND --> CLOUD["Modal Â· E2B Â· Daytona"]

    CMD --> BENCH["ğŸ† Benchmarks (10 presets)"]
    CMD --> LB["ğŸ“Š Leaderboard"]
    CMD --> SR["âª Session Replay"]
```

---

## ğŸ“‹ Feature Matrix

| Feature | Module |
|---------|--------|
| ğŸ§  RLM Runner (multi-paradigm) | `rlm_code.rlm.runner` |
| ğŸ§ª Pure RLM Environment | `rlm_code.rlm.pure_rlm_environment` |
| ğŸ“¡ Event System (27+ types) | `rlm_code.rlm.events` |
| ğŸ¯ Policy Lab (16 policies) | `rlm_code.rlm.policies` |
| ğŸ”’ HITL Approval Gates | `rlm_code.rlm.approval` |
| ğŸ“Š Observability (7 sinks) | `rlm_code.rlm.observability` |
| ğŸ† Benchmarks (10 presets) | `rlm_code.rlm.benchmarks` |
| ğŸ“Š Leaderboard | `rlm_code.rlm.leaderboard` |
| âª Session Replay | `rlm_code.rlm.session_replay` |
| ğŸ” Paradigm Comparison | `rlm_code.rlm.comparison` |
| ğŸ“ˆ Trajectory Logging | `rlm_code.rlm.trajectory` |
| ğŸ§¹ Memory Compaction | `rlm_code.rlm.memory_compaction` |
| ğŸ“¦ 6 Sandbox Runtimes | `rlm_code.sandbox.runtimes` |
| ğŸ¤– 12+ LLM Providers | `rlm_code.models` |
| ğŸ”Œ MCP Server | `rlm_code.mcp` |
| ğŸ–¥ï¸ Unified TUI (5 tabs) | `rlm_code.ui.tui_app` |
| âŒ¨ï¸ 50+ Slash Commands | `rlm_code.commands` |
| Code Validation | `rlm_code.validation` |
| ğŸ§© Framework Adapters | `rlm_code.rlm.frameworks` |

---

## ğŸ–¥ï¸ The TUI at a Glance

RLM Code ships a **single unified TUI** with **5 tabs**:

| Tab | Shortcut | Purpose |
|-----|----------|---------|
| ğŸ” **RLM** | `Ctrl+1` / `F2` | Converse with LLMs, run slash commands |
| ğŸ“ **Files** | `Ctrl+2` / `F3` | Browse project files with syntax preview |
| ğŸ“‹ **Details** | `Ctrl+3` / `F4` | Status panel, diff viewer |
| âš¡ **Shell** | `Ctrl+4` / `F5` | Persistent stateful shell |
| ğŸ”¬ **Research** | `Ctrl+5` / `F6` | Dashboard, trajectory, benchmarks, replay, live events |

The **Research tab** has 5 internal sub-tabs for organizing experiment data:

- **Dashboard**: Run metrics, reward sparkline, summary
- **Trajectory**: Step-by-step timeline of actions and rewards
- **Benchmarks**: Leaderboard table from `/rlm bench` runs
- **Replay**: Step-through controls for time-travel debugging
- **Events**: Live event stream from the RLM event bus

!!! tip "ğŸ”¬ Research Tab"
    Press `Ctrl+5` after running `/rlm bench preset=dspy_quick` to see real experiment data populate the Research tab dashboards.

---

## ğŸ“š Documentation Guide

| Section | What You'll Find |
|---------|-----------------|
| [ğŸš€ Getting Started](getting-started/index.md) | Installation, quick start, CLI reference, configuration |
| [ğŸ§  Core Engine](core/index.md) | RLM Runner, environments, events, termination, trajectory |
| [ğŸ¯ Policies & Safety](policies/index.md) | Reward, action, compaction, termination policies + HITL gates |
| [ğŸ–¥ï¸ Terminal UI](tui/index.md) | Tab reference, Research tab, widgets, theme system |
| [ğŸ“Š Benchmarks & Replay](benchmarks/index.md) | Presets, leaderboard, session replay |
| [ğŸ” Observability](observability/index.md) | Sink architecture, MLflow, OTel, LangSmith, LangFuse, Logfire |
| [ğŸ“¦ Platform](sandbox/index.md) | Sandbox runtimes, LLM providers, MCP, framework adapters |
| [ğŸ“– Reference](reference/index.md) | Full API reference |
