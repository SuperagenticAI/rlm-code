"""
Evaluation code generator for DSPy programs.

Generates complete evaluation scripts with metrics, dataset loading, and visualization.
"""


class EvaluationGenerator:
    """Generate evaluation code for DSPy programs."""

    def __init__(self):
        self.metric_templates = self._init_metric_templates()

    def _init_metric_templates(self) -> dict[str, str]:
        """Initialize metric function templates."""
        return {
            "accuracy": '''def accuracy_metric(example, prediction, trace=None):
    """Calculate accuracy: exact match between expected and predicted."""
    return example.{output_field} == prediction.{output_field}''',
            "f1": '''def f1_metric(example, prediction, trace=None):
    """Calculate F1 score for classification tasks."""
    from sklearn.metrics import f1_score

    # Convert to binary or multi-class as needed
    y_true = [example.{output_field}]
    y_pred = [prediction.{output_field}]

    return f1_score(y_true, y_pred, average='weighted')''',
            "semantic_similarity": '''def semantic_similarity_metric(example, prediction, trace=None):
    """Calculate semantic similarity between texts."""
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity

    model = SentenceTransformer('all-MiniLM-L6-v2')

    emb1 = model.encode([example.{output_field}])
    emb2 = model.encode([prediction.{output_field}])

    similarity = cosine_similarity(emb1, emb2)[0][0]
    return float(similarity)''',
            "contains": '''def contains_metric(example, prediction, trace=None):
    """Check if prediction contains expected answer."""
    expected = example.{output_field}.lower()
    predicted = prediction.{output_field}.lower()
    return expected in predicted''',
            "custom": '''def custom_metric(example, prediction, trace=None):
    """Custom metric - modify this for your specific needs."""
    # TODO: Implement your custom evaluation logic
    # Return a float between 0.0 and 1.0

    # Example: Check multiple conditions
    score = 0.0

    # Condition 1: Exact match (50% weight)
    if example.{output_field} == prediction.{output_field}:
        score += 0.5

    # Condition 2: Length similarity (25% weight)
    len_ratio = min(len(prediction.{output_field}), len(example.{output_field})) / max(len(prediction.{output_field}), len(example.{output_field}))
    score += 0.25 * len_ratio

    # Condition 3: Your custom logic (25% weight)
    # Add your logic here
    score += 0.25

    return score''',
        }

    def generate_evaluation_script(
        self,
        module_name: str,
        signature_name: str,
        input_fields: list[str],
        output_fields: list[str],
        metrics: list[str] = None,
        dataset_path: str = "data/test.json",
    ) -> str:
        """
        Generate complete evaluation script.

        Args:
            module_name: Name of the module to evaluate
            signature_name: Name of the signature
            input_fields: List of input field names
            output_fields: List of output field names
            metrics: List of metric names to include
            dataset_path: Path to test dataset

        Returns:
            Complete evaluation script as string
        """
        if metrics is None:
            metrics = ["accuracy"]

        # Use first output field for metrics
        output_field = output_fields[0] if output_fields else "output"

        # Generate metric functions
        metric_functions = []
        for metric in metrics:
            template = self.metric_templates.get(metric, self.metric_templates["custom"])
            metric_func = template.format(output_field=output_field)
            metric_functions.append(metric_func)

        # Generate dataset loader
        dataset_loader = self._generate_dataset_loader(input_fields, output_fields, dataset_path)

        # Generate evaluation code
        evaluation_code = self._generate_evaluation_code(
            module_name, signature_name, metrics, input_fields, output_fields
        )

        # Assemble complete script
        script = f'''"""
Evaluation Script for {module_name}

This script evaluates the performance of your DSPy module on a test dataset.
Generated by RLM Code.
"""

import dspy
from dspy.evaluate import Evaluate
import json
from pathlib import Path

# ============================================================================
# 1. METRIC FUNCTIONS
# ============================================================================

{chr(10).join(metric_functions)}


# ============================================================================
# 2. DATASET LOADER
# ============================================================================

{dataset_loader}


# ============================================================================
# 3. EVALUATION SETUP
# ============================================================================

{evaluation_code}


# ============================================================================
# 4. MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    main()
'''

        return script

    def _generate_dataset_loader(
        self, input_fields: list[str], output_fields: list[str], dataset_path: str
    ) -> str:
        """Generate dataset loading code."""

        fields_str = ", ".join([f'"{f}"' for f in input_fields + output_fields])

        return f'''def load_test_dataset(path: str = "{dataset_path}"):
    """Load test dataset from JSON file."""

    dataset_file = Path(path)

    if not dataset_file.exists():
        print(f"âš ï¸  Dataset not found: {{path}}")
        print("\\nCreating example dataset...")

        # Create example dataset
        examples = [
            {{{", ".join([f'"{f}": "example {f} 1"' for f in input_fields + output_fields])}}},
            {{{", ".join([f'"{f}": "example {f} 2"' for f in input_fields + output_fields])}}},
            {{{", ".join([f'"{f}": "example {f} 3"' for f in input_fields + output_fields])}}},
        ]

        # Save example dataset
        dataset_file.parent.mkdir(parents=True, exist_ok=True)
        with open(dataset_file, 'w') as f:
            json.dump(examples, f, indent=2)

        print(f"âœ“ Created example dataset: {{path}}")
        print("  Please replace with your actual test data\\n")

    # Load dataset
    with open(dataset_file) as f:
        data = json.load(f)

    # Convert to DSPy examples
    examples = []
    for item in data:
        example = dspy.Example(**item).with_inputs({fields_str})
        examples.append(example)

    print(f"âœ“ Loaded {{len(examples)}} examples from {{path}}")
    return examples'''

    def _generate_evaluation_code(
        self,
        module_name: str,
        signature_name: str,
        metrics: list[str],
        input_fields: list[str],
        output_fields: list[str],
    ) -> str:
        """Generate evaluation execution code."""

        metric_names = ", ".join([f'"{m}"' for m in metrics])

        return f'''def configure_dspy():
    """Configure DSPy with your language model."""
    # Use the same configuration as your main program (DSPy 3.0+)
    lm = dspy.LM(model='ollama/gpt-oss:20b')
    dspy.configure(lm=lm)
    print("âœ“ Configured DSPy")


def run_evaluation(module, testset, metric_func, metric_name="metric"):
    """Run evaluation with specified metric."""

    print(f"\\n{"=" * 60}")
    print(f"Evaluating with {{metric_name}}...")
    print(f"{"=" * 60}\\n")

    evaluator = Evaluate(
        devset=testset,
        metric=metric_func,
        num_threads=4,
        display_progress=True,
        display_table=5,  # Show first 5 results
    )

    score = evaluator(module)

    print(f"\\nâœ“ {{metric_name}}: {{score:.2f}}%\\n")

    return score


def main():
    """Main evaluation execution."""

    print("\\n" + "="*60)
    print(f"Evaluating {module_name}")
    print("="*60 + "\\n")

    # Step 1: Configure DSPy
    configure_dspy()
    print()

    # Step 2: Load test dataset
    testset = load_test_dataset()
    print()

    # Step 3: Import and create module
    print("Creating module...")
    # TODO: Import your module here
    # from your_module import {module_name}
    # module = {module_name}()

    # For now, we'll show you how to do it:
    print("âš ï¸  Please uncomment the import statement above")
    print("   and import your {module_name} class\\n")

    # Uncomment when you have your module imported:
    # module = {module_name}()
    # print("âœ“ Module created\\n")

    # Step 4: Run evaluation with each metric
    # results = {{}}
    # for metric_name in [{metric_names}]:
    #     if metric_name == "accuracy":
    #         score = run_evaluation(module, testset, accuracy_metric, "Accuracy")
    #     elif metric_name == "f1":
    #         score = run_evaluation(module, testset, f1_metric, "F1 Score")
    #     elif metric_name == "semantic_similarity":
    #         score = run_evaluation(module, testset, semantic_similarity_metric, "Semantic Similarity")
    #     elif metric_name == "contains":
    #         score = run_evaluation(module, testset, contains_metric, "Contains Match")
    #     else:
    #         score = run_evaluation(module, testset, custom_metric, "Custom Metric")
    #
    #     results[metric_name] = score

    # Step 5: Summary
    # print("\\n" + "="*60)
    # print("EVALUATION SUMMARY")
    # print("="*60 + "\\n")
    #
    # for metric_name, score in results.items():
    #     print(f"  {{metric_name:20s}}: {{score:6.2f}}%")
    #
    # print("\\n" + "="*60 + "\\n")

    print("\\nðŸ’¡ Next Steps:")
    print("  1. Replace example dataset with your actual test data")
    print("  2. Uncomment the module import and evaluation code")
    print("  3. Run: python evaluation.py")
    print("  4. Use results to optimize your module\\n")'''


def generate_evaluation_for_program(program_code: str, metrics: list[str] = None) -> str:
    """
    Generate evaluation script for a DSPy program.

    Args:
        program_code: The DSPy program code
        metrics: List of metrics to include

    Returns:
        Complete evaluation script
    """
    # Parse program to extract module and signature names
    # This is a simple implementation - could be enhanced with AST parsing

    module_name = "YourModule"
    signature_name = "YourSignature"
    input_fields = ["input"]
    output_fields = ["output"]

    # Try to extract from code
    if "class" in program_code and "Module" in program_code:
        lines = program_code.split("\n")
        for line in lines:
            if "class" in line and "Module" in line:
                parts = line.split()
                if len(parts) >= 2:
                    module_name = parts[1].split("(")[0]
            elif "class" in line and "Signature" in line:
                parts = line.split()
                if len(parts) >= 2:
                    signature_name = parts[1].split("(")[0]
            elif "InputField" in line:
                field_name = line.split("=")[0].strip()
                if field_name and field_name not in input_fields:
                    input_fields = [field_name]
            elif "OutputField" in line:
                field_name = line.split("=")[0].strip()
                if field_name and field_name not in output_fields:
                    output_fields = [field_name]

    generator = EvaluationGenerator()
    return generator.generate_evaluation_script(
        module_name=module_name,
        signature_name=signature_name,
        input_fields=input_fields,
        output_fields=output_fields,
        metrics=metrics or ["accuracy"],
    )
