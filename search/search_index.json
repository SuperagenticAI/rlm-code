{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":"<p>RLM Code is the definitive research operating system for building, running, evaluating, comparing, and optimizing LLM-based coding agents. It supports multiple agent paradigms including Pure RLM, CodeAct, and Traditional in a single unified platform with built-in safety, observability, and reproducibility.</p>"},{"location":"#rlm-code","title":"\ud83e\uddea RLM Code","text":"<p>Research Playground &amp; Evaluation OS for Recursive Language Model Agentic Systems</p> <p>v0.1.5 Python 3.11+ Apache 2.0</p>"},{"location":"#what-rlm-code-solves","title":"\ud83c\udfaf What RLM Code Solves","text":"<p>The underlying long-context reasoning problem is what RLM (the method) addresses. RLM Code addresses the tooling and workflow problem around using that method in practice.</p> <p>Core product problems it targets:</p> <ul> <li>Implementation friction: provide a runnable RLM environment (<code>llm_query</code>, REPL, run loop) without custom scaffolding.</li> <li>Experiment management: run, replay, compare, and benchmark experiments in one place.</li> <li>Safety controls: route execution through secure backends and explicit runtime settings.</li> <li>Reproducibility: store traces, metrics, and benchmark artifacts for repeatable research.</li> <li>Operational visibility: expose observability, status, and diagnostics for debugging experiments.</li> </ul> <p>In short, RLM Code is a research tooling layer for building and evaluating RLM-style workflows.</p>"},{"location":"#highlights","title":"\u2728 Highlights","text":""},{"location":"#multi-paradigm-engine","title":"\ud83e\udde0 Multi-Paradigm Engine","text":"<p>Run Pure RLM (paper-compliant with context-as-variable), CodeAct (context-in-tokens), or Traditional agent orchestration, all from one TUI.</p>"},{"location":"#built-in-research-tab","title":"\ud83d\udd2c Built-in Research Tab","text":"<p>A dedicated Research tab inside the TUI with Dashboard, Trajectory, Benchmarks, Replay, and Live Events sub-tabs for real-time experiment tracking.</p>"},{"location":"#benchmarks-leaderboard","title":"\ud83c\udfc6 Benchmarks &amp; Leaderboard","text":"<p>10 preset benchmarks with 33+ test cases, a multi-metric leaderboard, and side-by-side paradigm comparison.</p>"},{"location":"#session-replay","title":"\ud83d\udd01 Session Replay","text":"<p>Time-travel through any RLM run step-by-step with forward/backward navigation, reward curve visualization, and checkpoint/restore.</p>"},{"location":"#hot-swappable-policies","title":"\ud83c\udfaf Hot-Swappable Policies","text":"<p>Swap reward, action selection, compaction, and termination policies at runtime via the Policy Lab.</p>"},{"location":"#hitl-approval-gates","title":"\ud83d\udd12 HITL Approval Gates","text":"<p>Risk assessment with 40+ rules, 6 approval modes, and full audit logging to keep humans in the loop for every critical action.</p>"},{"location":"#pluggable-observability","title":"\ud83d\udcca Pluggable Observability","text":"<p>7 sinks including JSONL, MLflow, OpenTelemetry, LangSmith, LangFuse, and Logfire to trace every step of every run.</p>"},{"location":"#sandbox-runtimes","title":"\ud83d\udce6 Sandbox Runtimes","text":"<p>6 runtimes including Local, Docker, Apple Container, Modal, E2B, and Daytona for safe, isolated code execution.</p>"},{"location":"#rlm-research-lab","title":"\ud83d\uddbc\ufe0f RLM Research Lab","text":""},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Install and launch</p> <pre><code>uv tool install \"rlm-code[tui,llm-all]\"\nrlm-code\n</code></pre> <p>Connect to a model</p> <pre><code>/connect anthropic claude-opus-4-6\n</code></pre> <p>Run your first benchmark</p> <pre><code>/rlm bench preset=dspy_quick\n</code></pre> <p>Keep runs bounded</p> <pre><code>/rlm run \"small scoped task\" steps=4 timeout=30 budget=60\n/rlm abort all\n</code></pre> <p>Compare benchmark output</p> <pre><code>/rlm bench compare candidate=latest baseline=previous\n</code></pre> <p>Switch to the Research tab</p> <p>Press <code>Ctrl+5</code> or <code>F6</code> to open the Research tab to see your run's dashboard, trajectory, reward curves, and live events.</p>"},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    CLI[\"\ud83d\ude80 rlm-code CLI\"]\n    CLI --&gt; TUI[\"\ud83d\udda5\ufe0f Unified TUI\"]\n    TUI --&gt; RLM[\"\ud83d\udd01 RLM\"]\n    TUI --&gt; FILES[\"\ud83d\udcc1 Files\"]\n    TUI --&gt; DETAILS[\"\ud83d\udccb Details\"]\n    TUI --&gt; SHELL[\"\u26a1 Shell\"]\n    TUI --&gt; RESEARCH[\"\ud83d\udd2c Research\"]\n\n    CLI --&gt; CMD[\"\u2328\ufe0f 50+ Slash Commands\"]\n\n    CMD --&gt; RUNNER[\"\ud83e\udde0 RLM Runner\"]\n    RUNNER --&gt; EVENTS[\"\ud83d\udce1 Event Bus (27+ types)\"]\n    RUNNER --&gt; OBS[\"\ud83d\udcca Observability (7 sinks)\"]\n    RUNNER --&gt; TRAJ[\"\ud83d\udcc8 Trajectory Logger\"]\n    RUNNER --&gt; POL[\"\ud83c\udfaf Policy Lab\"]\n    RUNNER --&gt; HITL[\"\ud83d\udd12 HITL Approval Gates\"]\n\n    RUNNER --&gt; ENV[\"\ud83c\udf0d Environments\"]\n    ENV --&gt; PURE[\"Pure RLM\"]\n    ENV --&gt; DSPY[\"DSPy Coding\"]\n    ENV --&gt; GEN[\"Generic\"]\n\n    RUNNER --&gt; SAND[\"\ud83d\udce6 Sandbox Runtimes\"]\n    SAND --&gt; LOCAL[\"Local\"]\n    SAND --&gt; DOCKER[\"Docker\"]\n    SAND --&gt; CLOUD[\"Modal \u00b7 E2B \u00b7 Daytona\"]\n\n    CMD --&gt; BENCH[\"\ud83c\udfc6 Benchmarks (10 presets)\"]\n    CMD --&gt; LB[\"\ud83d\udcca Leaderboard\"]\n    CMD --&gt; SR[\"\u23ea Session Replay\"]</code></pre>"},{"location":"#feature-matrix","title":"\ud83d\udccb Feature Matrix","text":"Feature Module \ud83e\udde0 RLM Runner (multi-paradigm) <code>rlm_code.rlm.runner</code> \ud83e\uddea Pure RLM Environment <code>rlm_code.rlm.pure_rlm_environment</code> \ud83d\udce1 Event System (27+ types) <code>rlm_code.rlm.events</code> \ud83c\udfaf Policy Lab (16 policies) <code>rlm_code.rlm.policies</code> \ud83d\udd12 HITL Approval Gates <code>rlm_code.rlm.approval</code> \ud83d\udcca Observability (7 sinks) <code>rlm_code.rlm.observability</code> \ud83c\udfc6 Benchmarks (10 presets) <code>rlm_code.rlm.benchmarks</code> \ud83d\udcca Leaderboard <code>rlm_code.rlm.leaderboard</code> \u23ea Session Replay <code>rlm_code.rlm.session_replay</code> \ud83d\udd01 Paradigm Comparison <code>rlm_code.rlm.comparison</code> \ud83d\udcc8 Trajectory Logging <code>rlm_code.rlm.trajectory</code> \ud83e\uddf9 Memory Compaction <code>rlm_code.rlm.memory_compaction</code> \ud83d\udce6 6 Sandbox Runtimes <code>rlm_code.sandbox.runtimes</code> \ud83e\udd16 12+ LLM Providers <code>rlm_code.models</code> \ud83d\udd0c MCP Server <code>rlm_code.mcp</code> \ud83d\udda5\ufe0f Unified TUI (5 tabs) <code>rlm_code.ui.tui_app</code> \u2328\ufe0f 50+ Slash Commands <code>rlm_code.commands</code> Code Validation <code>rlm_code.validation</code> \ud83e\udde9 Framework Adapters <code>rlm_code.rlm.frameworks</code>"},{"location":"#the-tui-at-a-glance","title":"\ud83d\udda5\ufe0f The TUI at a Glance","text":"<p>RLM Code ships a single unified TUI with 5 tabs:</p> Tab Shortcut Purpose \ud83d\udd01 RLM <code>Ctrl+1</code> / <code>F2</code> Converse with LLMs, run slash commands \ud83d\udcc1 Files <code>Ctrl+2</code> / <code>F3</code> Browse project files with syntax preview \ud83d\udccb Details <code>Ctrl+3</code> / <code>F4</code> Status panel, diff viewer \u26a1 Shell <code>Ctrl+4</code> / <code>F5</code> Persistent stateful shell \ud83d\udd2c Research <code>Ctrl+5</code> / <code>F6</code> Dashboard, trajectory, benchmarks, replay, live events <p>The Research tab has 5 internal sub-tabs for organizing experiment data:</p> <ul> <li>Dashboard: Run metrics, reward sparkline, summary</li> <li>Trajectory: Step-by-step timeline of actions and rewards</li> <li>Benchmarks: Leaderboard table from <code>/rlm bench</code> runs</li> <li>Replay: Step-through controls for time-travel debugging</li> <li>Events: Live event stream from the RLM event bus</li> </ul> <p>\ud83d\udd2c Research Tab</p> <p>Press <code>Ctrl+5</code> after running <code>/rlm bench preset=dspy_quick</code> to see real experiment data populate the Research tab dashboards.</p>"},{"location":"#documentation-guide","title":"\ud83d\udcda Documentation Guide","text":"Section What You'll Find \ud83d\ude80 Getting Started Installation, quick start, CLI reference, configuration \ud83e\udde0 Core Engine RLM Runner, environments, events, termination, trajectory \ud83c\udfaf Policies &amp; Safety Reward, action, compaction, termination policies + HITL gates \ud83d\udda5\ufe0f Terminal UI Tab reference, Research tab, widgets, theme system \ud83d\udcca Benchmarks &amp; Replay Presets, leaderboard, session replay \ud83d\udd0d Observability Sink architecture, MLflow, OTel, LangSmith, LangFuse, Logfire \ud83d\udce6 Platform Sandbox runtimes, LLM providers, MCP, framework adapters \ud83d\udcd6 Reference Full API reference"},{"location":"benchmarks/","title":"Benchmarks &amp; Leaderboard","text":"<p>RLM Code includes a complete benchmarking and evaluation framework designed for research reproducibility and systematic performance tracking. The system provides 10 preset benchmark suites covering 33+ test cases, a multi-metric leaderboard for ranking and comparison, and session replay with full time-travel debugging.</p>"},{"location":"benchmarks/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    Presets[\"Preset Benchmarks\\n(10 suites, 33+ cases)\"] --&gt; Runner[\"RLMRunner\\n.run_benchmark()\"]\n    YAML[\"Custom YAML Packs\"] --&gt; Runner\n    Runner --&gt; Results[\"Benchmark Results\\n(JSON)\"]\n    Runner --&gt; JSONL[\"Step Traces\\n(JSONL)\"]\n    Results --&gt; LB[\"Leaderboard\\n(ranking, statistics)\"]\n    JSONL --&gt; Replay[\"Session Replay\\n(time-travel debugging)\"]\n    LB --&gt; Export[\"Export\\n(JSON / CSV / Markdown / Rich)\"]\n    Replay --&gt; Compare[\"Session Comparison\\n(diff two runs)\"]</code></pre>"},{"location":"benchmarks/#key-components","title":"Key Components","text":""},{"location":"benchmarks/#preset-benchmarks","title":"Preset Benchmarks","text":"<p>10 built-in benchmark suites that cover the full spectrum of RLM capabilities:</p> Category Presets Total Cases Focus DSPy <code>dspy_quick</code>, <code>dspy_extended</code> 8 DSPy coding loop: signatures, modules, tests Generic <code>generic_smoke</code> 2 Basic Python execution and error recovery Pure RLM <code>pure_rlm_smoke</code>, <code>pure_rlm_context</code> 7 Paper-compliant mode, context-as-variable Advanced <code>deep_recursion</code>, <code>paradigm_comparison</code> 6 Depth &gt; 1 recursion, cross-paradigm comparison Paper-Compatible <code>oolong_style</code>, <code>browsecomp_style</code>, <code>token_efficiency</code> 10 OOLONG, BrowseComp-Plus, token efficiency <p>See Preset Benchmarks for full details on every suite and case.</p>"},{"location":"benchmarks/#multi-metric-leaderboard","title":"Multi-Metric Leaderboard","text":"<p>The leaderboard aggregates results from all benchmark runs and ranks them across 7 metrics:</p> Metric Direction Description <code>REWARD</code> Higher is better Average cumulative reward <code>COMPLETION_RATE</code> Higher is better Percentage of completed runs <code>STEPS</code> Lower is better Average steps to completion <code>TOKENS</code> Lower is better Total tokens consumed <code>COST</code> Lower is better Estimated cost in USD <code>DURATION</code> Lower is better Execution time in seconds <code>EFFICIENCY</code> Higher is better Reward per 1000 tokens <p>See Leaderboard for filtering, statistics, aggregation, and export.</p>"},{"location":"benchmarks/#session-replay","title":"Session Replay","text":"<p>Every RLM run can be recorded and replayed step by step:</p> <ul> <li>SessionRecorder captures actions, observations, rewards, memory, and variables at each step</li> <li>SessionReplayer provides forward/backward navigation (<code>step_forward</code>, <code>step_backward</code>, <code>goto_step</code>)</li> <li>SessionStore persists snapshots and checkpoints to disk</li> <li>SessionComparison diffs two sessions to find the divergence point</li> </ul> <p>See Session Replay for the full API and time-travel debugging workflows.</p>"},{"location":"benchmarks/#quick-start","title":"Quick Start","text":""},{"location":"benchmarks/#run-a-preset-benchmark","title":"Run a Preset Benchmark","text":"<pre><code>rlm-code bench preset=dspy_quick\n</code></pre>"},{"location":"benchmarks/#view-the-leaderboard","title":"View the Leaderboard","text":"<pre><code>rlm-code leaderboard --metric reward --limit 10\n</code></pre>"},{"location":"benchmarks/#replay-a-session","title":"Replay a Session","text":"<pre><code>from rlm_code.rlm.session_replay import load_session\n\nreplayer = load_session(\".rlm_code/rlm/observability/steps/abc12345.jsonl\")\nfor step in replayer.iterate_steps():\n    print(f\"Step {step.step}: {step.action_type} -&gt; reward={step.reward}\")\n</code></pre>"},{"location":"benchmarks/#custom-benchmarks","title":"Custom Benchmarks","text":"<p>You can extend the built-in presets by loading custom YAML, JSON, or JSONL benchmark packs:</p> <pre><code># my_benchmarks.yaml\npresets:\n  my_suite:\n    description: \"Custom evaluation suite\"\n    cases:\n      - id: test_1\n        task: \"Write a function that reverses a string\"\n        environment: generic\n        max_steps: 3\n        exec_timeout: 30\n      - id: test_2\n        task: \"Build a REST API client\"\n        environment: generic\n        max_steps: 5\n        exec_timeout: 60\n</code></pre> <pre><code>rlm-code bench preset=my_suite --pack my_benchmarks.yaml\n</code></pre> <p>See Preset Benchmarks for all supported pack formats including Google ADK eval sets and generic datasets.</p>"},{"location":"benchmarks/#whats-next","title":"What's Next","text":"Page Description Preset Benchmarks All 10 presets in detail, custom pack loading, YAML format Leaderboard Ranking, filtering, statistics, trend analysis, export Session Replay Recording, replaying, time-travel debugging, session comparison"},{"location":"benchmarks/leaderboard/","title":"Leaderboard","text":"<p>The leaderboard aggregates results from benchmark runs and individual RLM runs, providing multi-metric ranking, filtering, statistical analysis, trend tracking, and export to multiple formats.</p> <p>Module: <code>rlm_code.rlm.leaderboard</code></p>"},{"location":"benchmarks/leaderboard/#core-classes","title":"Core Classes","text":""},{"location":"benchmarks/leaderboard/#leaderboard_1","title":"<code>Leaderboard</code>","text":"<p>The central manager that loads results, applies rankings, and exports data.</p> <pre><code>from rlm_code.rlm.leaderboard import Leaderboard\n\nlb = Leaderboard(workdir=Path(\".rlm_code\"), auto_load=True)\n</code></pre> Parameter Type Default Description <code>workdir</code> <code>Path | None</code> <code>Path.cwd() / \".rlm_code\"</code> Working directory containing results <code>auto_load</code> <code>bool</code> <code>True</code> Automatically load all results on construction"},{"location":"benchmarks/leaderboard/#leaderboardentry","title":"<code>LeaderboardEntry</code>","text":"<p>A single entry in the leaderboard representing one benchmark run or individual run.</p> <pre><code>@dataclass\nclass LeaderboardEntry:\n    # Identification\n    entry_id: str              # Short ID (first 16 chars of benchmark_id)\n    benchmark_id: str          # Full benchmark identifier\n    run_id: str | None = None  # Individual run ID (if from runs.jsonl)\n\n    # Metadata\n    environment: str = \"\"      # Environment name (dspy, generic, pure_rlm)\n    model: str = \"\"            # Model identifier\n    preset: str = \"\"           # Benchmark preset name\n    timestamp: str = \"\"        # ISO timestamp\n    description: str = \"\"      # Human-readable description\n\n    # Core metrics\n    avg_reward: float = 0.0           # Average reward across cases\n    completion_rate: float = 0.0      # Fraction of completed cases (0.0 - 1.0)\n    total_cases: int = 0              # Number of cases in the benchmark\n    completed_cases: int = 0          # Number that completed\n    avg_steps: float = 0.0            # Average steps per case\n\n    # Token metrics\n    total_tokens: int = 0             # Total tokens consumed\n    prompt_tokens: int = 0            # Prompt/input tokens\n    completion_tokens: int = 0        # Completion/output tokens\n\n    # Cost and time\n    estimated_cost: float = 0.0       # Estimated cost in USD\n    duration_seconds: float = 0.0     # Total execution time\n\n    # Computed metrics (auto-calculated in __post_init__)\n    efficiency: float = 0.0           # reward per 1000 tokens\n    tokens_per_step: float = 0.0      # tokens / avg_steps\n\n    # Raw data\n    source_path: str = \"\"             # Path to the source file\n    tags: list[str] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"benchmarks/leaderboard/#computed-metrics","title":"Computed Metrics","text":"<p>Two metrics are automatically calculated in <code>__post_init__</code>:</p> <pre><code>def __post_init__(self) -&gt; None:\n    if self.total_tokens &gt; 0:\n        self.efficiency = (self.avg_reward * 1000) / self.total_tokens\n    if self.avg_steps &gt; 0:\n        self.tokens_per_step = self.total_tokens / self.avg_steps\n</code></pre>"},{"location":"benchmarks/leaderboard/#factory-methods","title":"Factory Methods","text":"Method Source Description <code>from_benchmark_json(data, source_path)</code> Benchmark JSON file Creates entry from full benchmark result data <code>from_run_jsonl(data, source_path)</code> <code>runs.jsonl</code> line Creates entry from a single run record"},{"location":"benchmarks/leaderboard/#rankingmetric","title":"<code>RankingMetric</code>","text":"<p>Enum of 7 available metrics for ranking:</p> <pre><code>class RankingMetric(Enum):\n    REWARD = \"reward\"                    # Average reward (higher is better)\n    COMPLETION_RATE = \"completion_rate\"  # % completed runs (higher is better)\n    STEPS = \"steps\"                      # Average steps (lower is better)\n    TOKENS = \"tokens\"                    # Total tokens used (lower is better)\n    COST = \"cost\"                        # Estimated cost (lower is better)\n    DURATION = \"duration\"                # Execution time (lower is better)\n    EFFICIENCY = \"efficiency\"            # Reward per token (higher is better)\n</code></pre> <p>Each metric has a default sort direction:</p> Metric Higher is Better? Default Sort <code>REWARD</code> Yes Descending <code>COMPLETION_RATE</code> Yes Descending <code>STEPS</code> No Ascending <code>TOKENS</code> No Ascending <code>COST</code> No Ascending <code>DURATION</code> No Ascending <code>EFFICIENCY</code> Yes Descending"},{"location":"benchmarks/leaderboard/#leaderboardfilter","title":"<code>LeaderboardFilter</code>","text":"<p>Filters for narrowing down leaderboard queries.</p> <pre><code>@dataclass\nclass LeaderboardFilter:\n    environments: list[str] | None = None    # Filter by environment names\n    models: list[str] | None = None          # Filter by model identifiers\n    presets: list[str] | None = None         # Filter by preset names\n    tags: list[str] | None = None            # Filter by tags (any match)\n    min_reward: float | None = None          # Minimum average reward\n    max_reward: float | None = None          # Maximum average reward\n    min_completion_rate: float | None = None  # Minimum completion rate\n    date_from: datetime | None = None        # Earliest timestamp\n    date_to: datetime | None = None          # Latest timestamp\n    min_cases: int | None = None             # Minimum number of cases\n</code></pre> <p>The <code>matches(entry)</code> method checks all non-<code>None</code> filter fields against the entry. All conditions must pass (AND logic). For <code>tags</code>, any tag match suffices (OR within tags).</p>"},{"location":"benchmarks/leaderboard/#example","title":"Example","text":"<pre><code>from datetime import datetime, timezone\nfrom rlm_code.rlm.leaderboard import LeaderboardFilter\n\nfilter = LeaderboardFilter(\n    environments=[\"dspy\", \"pure_rlm\"],\n    min_reward=0.5,\n    min_completion_rate=0.8,\n    date_from=datetime(2025, 1, 1, tzinfo=timezone.utc),\n)\n</code></pre>"},{"location":"benchmarks/leaderboard/#ranking","title":"Ranking","text":""},{"location":"benchmarks/leaderboard/#leaderboardrank","title":"<code>Leaderboard.rank()</code>","text":"<p>The primary ranking method.</p> <pre><code>def rank(\n    self,\n    metric: RankingMetric = RankingMetric.REWARD,\n    order: SortOrder | None = None,\n    limit: int | None = None,\n    filter: LeaderboardFilter | None = None,\n) -&gt; RankingResult:\n</code></pre> Parameter Type Default Description <code>metric</code> <code>RankingMetric</code> <code>REWARD</code> Metric to rank by <code>order</code> <code>SortOrder | None</code> Auto (based on metric) <code>ASCENDING</code> or <code>DESCENDING</code> <code>limit</code> <code>int | None</code> <code>None</code> (all) Maximum entries to return <code>filter</code> <code>LeaderboardFilter | None</code> <code>None</code> Filter to apply <p>Returns a <code>RankingResult</code> containing the ranked entries and statistics.</p>"},{"location":"benchmarks/leaderboard/#rankingresult","title":"<code>RankingResult</code>","text":"<pre><code>@dataclass\nclass RankingResult:\n    entries: list[LeaderboardEntry]   # Ranked entries\n    metric: RankingMetric             # The metric used for ranking\n    order: SortOrder                  # The sort order applied\n    total_count: int                  # Total entries (before filter)\n    filtered_count: int               # Entries after filter\n\n    # Statistics (auto-computed in __post_init__)\n    mean: float = 0.0\n    median: float = 0.0\n    std_dev: float = 0.0\n    min_value: float = 0.0\n    max_value: float = 0.0\n</code></pre>"},{"location":"benchmarks/leaderboard/#example_1","title":"Example","text":"<pre><code>from rlm_code.rlm.leaderboard import Leaderboard, RankingMetric, LeaderboardFilter\n\nlb = Leaderboard(workdir=Path(\".rlm_code\"))\nresult = lb.rank(\n    metric=RankingMetric.EFFICIENCY,\n    limit=10,\n    filter=LeaderboardFilter(environments=[\"pure_rlm\"]),\n)\n\nprint(f\"Showing {len(result.entries)}/{result.filtered_count} entries\")\nprint(f\"Mean efficiency: {result.mean:.4f}\")\nprint(f\"Median: {result.median:.4f}, Std Dev: {result.std_dev:.4f}\")\n\nfor rank, entry in enumerate(result.entries, 1):\n    print(f\"#{rank} {entry.entry_id}: efficiency={entry.efficiency:.4f}, \"\n          f\"reward={entry.avg_reward:.3f}, tokens={entry.total_tokens:,}\")\n</code></pre>"},{"location":"benchmarks/leaderboard/#statistics","title":"Statistics","text":""},{"location":"benchmarks/leaderboard/#get_statistics","title":"<code>get_statistics()</code>","text":"<p>Compute statistical summary for any metric.</p> <pre><code>stats = lb.get_statistics(\n    metric=RankingMetric.REWARD,\n    filter=LeaderboardFilter(environments=[\"dspy\"]),\n)\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"count\": 15,\n    \"mean\": 0.7234,\n    \"median\": 0.7500,\n    \"std_dev\": 0.1523,\n    \"min\": 0.3000,\n    \"max\": 1.0000,\n    \"sum\": 10.8510,\n}\n</code></pre>"},{"location":"benchmarks/leaderboard/#trend-analysis","title":"Trend Analysis","text":""},{"location":"benchmarks/leaderboard/#compute_trend","title":"<code>compute_trend()</code>","text":"<p>Compute a moving-average trend over time for any metric.</p> <pre><code>from rlm_code.rlm.leaderboard import compute_trend, RankingMetric\n\ntrend = compute_trend(\n    entries=lb.entries,\n    metric=RankingMetric.REWARD,\n    window=5,  # 5-entry moving average\n)\n\nfor point in trend:\n    print(f\"{point['timestamp']}: value={point['value']:.3f}, \"\n          f\"moving_avg={point['moving_avg']:.3f}\")\n</code></pre> <p>Returns a list of dicts:</p> <pre><code>[\n    {\n        \"timestamp\": \"2025-05-15T10:00:00+00:00\",\n        \"entry_id\": \"abc12345\",\n        \"value\": 0.75,\n        \"moving_avg\": 0.75,\n    },\n    ...\n]\n</code></pre> <p>The entries are sorted by timestamp, and the moving average window slides from the start.</p>"},{"location":"benchmarks/leaderboard/#aggregation","title":"Aggregation","text":""},{"location":"benchmarks/leaderboard/#aggregate_by_field","title":"<code>aggregate_by_field()</code>","text":"<p>Group entries by any field and compute per-group statistics.</p> <pre><code>from rlm_code.rlm.leaderboard import aggregate_by_field, RankingMetric\n\n# Aggregate by environment\nby_env = aggregate_by_field(\n    entries=lb.entries,\n    field=\"environment\",\n    metric=RankingMetric.REWARD,\n)\n\nfor env, stats in by_env.items():\n    print(f\"{env}: count={stats['count']}, mean={stats['mean']:.3f}, \"\n          f\"median={stats['median']:.3f}\")\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"dspy\": {\"count\": 8, \"mean\": 0.72, \"median\": 0.75, \"min\": 0.3, \"max\": 1.0},\n    \"pure_rlm\": {\"count\": 7, \"mean\": 0.68, \"median\": 0.70, \"min\": 0.2, \"max\": 0.95},\n}\n</code></pre> <p>Useful field values: <code>\"environment\"</code>, <code>\"model\"</code>, <code>\"preset\"</code>.</p>"},{"location":"benchmarks/leaderboard/#comparison","title":"Comparison","text":""},{"location":"benchmarks/leaderboard/#leaderboardcompare","title":"<code>Leaderboard.compare()</code>","text":"<p>Compare specific entries across multiple metrics side by side.</p> <pre><code>comparison = lb.compare(\n    entry_ids=[\"abc12345\", \"def67890\"],\n    metrics=[RankingMetric.REWARD, RankingMetric.TOKENS, RankingMetric.EFFICIENCY],\n)\n\nfor entry_id, data in comparison.items():\n    print(f\"{entry_id}:\")\n    for metric, value in data[\"metrics\"].items():\n        print(f\"  {metric}: {value}\")\n</code></pre>"},{"location":"benchmarks/leaderboard/#export","title":"Export","text":""},{"location":"benchmarks/leaderboard/#json","title":"JSON","text":"<pre><code>lb.to_json(\n    output_path=\"leaderboard.json\",\n    metric=RankingMetric.REWARD,\n    limit=20,\n)\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"exported_at\": \"2025-05-15T10:30:00+00:00\",\n  \"metric\": \"reward\",\n  \"order\": \"desc\",\n  \"total_entries\": 50,\n  \"filtered_entries\": 50,\n  \"statistics\": {\n    \"mean\": 0.72,\n    \"median\": 0.75,\n    \"std_dev\": 0.15,\n    \"min\": 0.30,\n    \"max\": 1.00\n  },\n  \"entries\": [...]\n}\n</code></pre>"},{"location":"benchmarks/leaderboard/#csv","title":"CSV","text":"<pre><code>lb.to_csv(\n    output_path=\"leaderboard.csv\",\n    metric=RankingMetric.REWARD,\n    limit=20,\n)\n</code></pre> <p>Columns: <code>rank</code>, <code>entry_id</code>, <code>environment</code>, <code>model</code>, <code>preset</code>, <code>avg_reward</code>, <code>completion_rate</code>, <code>avg_steps</code>, <code>total_tokens</code>, <code>efficiency</code>, <code>timestamp</code>.</p>"},{"location":"benchmarks/leaderboard/#markdown","title":"Markdown","text":"<pre><code>md = lb.to_markdown(\n    metric=RankingMetric.REWARD,\n    limit=10,\n    title=\"RLM Leaderboard\",\n)\nprint(md)\n</code></pre> <p>Produces a full Markdown document with a table and statistics section:</p> <pre><code># RLM Leaderboard\n\n**Ranked by**: reward | **Entries**: 50/50\n\n| Rank | ID | Environment | Reward | Completion | Steps | Tokens | Efficiency |\n|------|-----|-------------|--------|------------|-------|--------|------------|\n| 1 | abc12345 | dspy | 0.950 | 100% | 3.0 | 1,200 | 0.792 |\n| 2 | def67890 | pure_rlm | 0.900 | 100% | 4.0 | 1,500 | 0.600 |\n...\n\n## Statistics\n\n- **Mean**: 0.7234\n- **Median**: 0.7500\n- **Std Dev**: 0.1523\n- **Range**: 0.3000 - 1.0000\n</code></pre>"},{"location":"benchmarks/leaderboard/#rich-table-terminal","title":"Rich Table (Terminal)","text":"<pre><code>table = lb.format_rich_table(\n    metric=RankingMetric.REWARD,\n    limit=10,\n    title=\"RLM Leaderboard\",\n)\n\nfrom rich.console import Console\nConsole().print(table)\n</code></pre> <p>The Rich table features:</p> <ul> <li>Color-coded reward values (green &gt;= 0.7, yellow &gt;= 0.4, red &lt; 0.4)</li> <li>Color-coded completion rates (green &gt;= 80%, yellow &gt;= 50%, red &lt; 50%)</li> <li>Caption showing entry count and mean value</li> <li>Right-aligned numeric columns</li> </ul>"},{"location":"benchmarks/leaderboard/#cli-usage","title":"CLI Usage","text":"<pre><code># Default: show top 10 by reward\nrlm-code leaderboard\n\n# Rank by efficiency, show top 20\nrlm-code leaderboard --metric efficiency --limit 20\n\n# Filter by environment\nrlm-code leaderboard --metric reward --environment dspy\n\n# Filter by model\nrlm-code leaderboard --metric tokens --model gpt-4o\n\n# Export to JSON\nrlm-code leaderboard --metric reward --format json --output-path results.json\n\n# Export to CSV\nrlm-code leaderboard --metric reward --format csv --output-path results.csv\n\n# Export to Markdown\nrlm-code leaderboard --metric reward --format markdown --output-path results.md\n</code></pre>"},{"location":"benchmarks/leaderboard/#data-loading","title":"Data Loading","text":"<p>The <code>Leaderboard</code> automatically loads data from two sources:</p>"},{"location":"benchmarks/leaderboard/#benchmark-json-files","title":"Benchmark JSON Files","text":"<p>Located in <code>.rlm_code/rlm/benchmarks/*.json</code>. Each file contains a full benchmark result with case-level detail. The leaderboard uses <code>LeaderboardEntry.from_benchmark_json()</code> to parse these.</p>"},{"location":"benchmarks/leaderboard/#runsjsonl","title":"runs.jsonl","text":"<p>Located in <code>.rlm_code/observability/runs.jsonl</code>. Each line contains a single run result from the <code>LocalJSONLSink</code>. The leaderboard uses <code>LeaderboardEntry.from_run_jsonl()</code> to parse these. Runs that already appear in a benchmark file (matched by <code>run_id</code>) are skipped to avoid duplicates.</p> <pre><code>lb = Leaderboard(workdir=Path(\".rlm_code\"))\nprint(f\"Loaded {len(lb.entries)} entries\")\n\n# Manual reload\ncount = lb.load_all()\nprint(f\"Loaded {count} new entries\")\n\n# Load from specific paths\nlb.load_benchmarks(benchmarks_dir=Path(\"custom/benchmarks\"))\nlb.load_runs(runs_file=Path(\"custom/runs.jsonl\"))\n</code></pre>"},{"location":"benchmarks/leaderboard/#utility-methods","title":"Utility Methods","text":"Method Returns Description <code>add_entry(entry)</code> <code>None</code> Manually add an entry <code>remove_entry(entry_id)</code> <code>bool</code> Remove by ID <code>get_entry(entry_id)</code> <code>LeaderboardEntry | None</code> Retrieve by ID <code>get_unique_values(field)</code> <code>list[str]</code> Get unique values for a field (useful for filter suggestions) <pre><code># Get all unique environments for building a filter dropdown\nenvironments = lb.get_unique_values(\"environment\")\n# [\"dspy\", \"generic\", \"pure_rlm\"]\n\nmodels = lb.get_unique_values(\"model\")\n# [\"gpt-4o\", \"claude-3-opus\", \"claude-3.5-sonnet\"]\n</code></pre>"},{"location":"benchmarks/presets/","title":"Preset Benchmarks","text":"<p>RLM Code ships with 10 preset benchmark suites containing 33+ test cases. These cover DSPy coding loops, generic execution, Pure RLM paper-compliant mode, deep recursion, paradigm comparison, and paper-compatible evaluation tasks.</p> <p>Module: <code>rlm_code.rlm.benchmarks</code></p>"},{"location":"benchmarks/presets/#rlmbenchmarkcase","title":"RLMBenchmarkCase","text":"<p>Every benchmark case is represented by the <code>RLMBenchmarkCase</code> frozen dataclass:</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass RLMBenchmarkCase:\n    \"\"\"One benchmark case runnable by RLMRunner.run_benchmark.\"\"\"\n\n    case_id: str          # Unique identifier within the preset\n    description: str      # Human-readable description\n    task: str             # The task prompt sent to the agent\n    environment: str = \"dspy\"     # Target environment\n    max_steps: int = 4            # Maximum iterations allowed\n    exec_timeout: int = 30        # Per-step execution timeout (seconds)\n</code></pre> Field Type Default Description <code>case_id</code> <code>str</code> (required) Unique ID within the preset (e.g., <code>sig_essay</code>) <code>description</code> <code>str</code> (required) Short human-readable label <code>task</code> <code>str</code> (required) Full task prompt for the agent <code>environment</code> <code>str</code> <code>\"dspy\"</code> The RLM environment to use <code>max_steps</code> <code>int</code> <code>4</code> Upper bound on iterations <code>exec_timeout</code> <code>int</code> <code>30</code> Timeout in seconds for each code execution"},{"location":"benchmarks/presets/#all-10-presets","title":"All 10 Presets","text":""},{"location":"benchmarks/presets/#1-dspy_quick-fast-dspy-smoke-test-3-cases","title":"1. <code>dspy_quick</code> -- Fast DSPy Smoke Test (3 cases)","text":"<p>Quick validation of the DSPy coding loop.</p> Case ID Description Max Steps Timeout <code>sig_essay</code> Build a DSPy signature for essay scoring 4 35s <code>module_outline</code> Build a DSPy module scaffold with <code>forward()</code> 4 35s <code>tests_min</code> Add minimal pytest coverage for the signature/module 5 45s <pre><code>rlm-code bench preset=dspy_quick\n</code></pre>"},{"location":"benchmarks/presets/#2-dspy_extended-broader-dspy-sweep-5-cases","title":"2. <code>dspy_extended</code> -- Broader DSPy Sweep (5 cases)","text":"<p>Comprehensive DSPy coding loop evaluation including refactoring and verification.</p> Case ID Description Max Steps Timeout <code>sig_essay</code> Build signature with rubric outputs 4 35s <code>module_reasoning</code> Build module producing score and rationale 5 45s <code>refactor_patch</code> Patch existing code for clarity, keep API stable 5 45s <code>verifier_pass</code> Run tests and iterate until verifier feedback improves 6 50s <code>final_summary</code> Summarize changes and remaining work 3 30s <pre><code>rlm-code bench preset=dspy_extended\n</code></pre>"},{"location":"benchmarks/presets/#3-generic_smoke-generic-sanity-checks-2-cases","title":"3. <code>generic_smoke</code> -- Generic Sanity Checks (2 cases)","text":"<p>Basic Python execution and error recovery in the generic environment.</p> Case ID Description Max Steps Timeout <code>hello_py</code> Write and run a tiny Python program that prints hello 2 20s <code>error_recovery</code> Run code with an intentional error, then recover 3 20s <pre><code>rlm-code bench preset=generic_smoke\n</code></pre>"},{"location":"benchmarks/presets/#4-pure_rlm_smoke-pure-rlm-paper-compliant-smoke-test-3-cases","title":"4. <code>pure_rlm_smoke</code> -- Pure RLM Paper-Compliant Smoke Test (3 cases)","text":"<p>Tests the Pure RLM mode where context is accessed as a variable through code.</p> Case ID Description Max Steps Timeout <code>context_exploration</code> Explore context structure via code (length, words, preview) 3 30s <code>context_analysis</code> Analyze context using <code>llm_query()</code> 4 45s <code>final_var_usage</code> Store findings in a variable and return with <code>FINAL_VAR()</code> 3 30s <pre><code>rlm-code bench preset=pure_rlm_smoke\n</code></pre>"},{"location":"benchmarks/presets/#5-pure_rlm_context-pure-rlm-context-as-variable-tests-4-cases","title":"5. <code>pure_rlm_context</code> -- Pure RLM Context-as-Variable Tests (4 cases)","text":"<p>Advanced context manipulation patterns from the RLM paper.</p> Case ID Description Max Steps Timeout <code>chunked_analysis</code> Chunk context and use <code>llm_query_batched()</code> in parallel 5 60s <code>iterative_refinement</code> Multi-iteration progressive understanding 6 60s <code>variable_accumulation</code> Accumulate findings in REPL variables, verify with <code>SHOW_VARS()</code> 5 45s <code>recursive_decomposition</code> Map-reduce pattern from the RLM paper using <code>llm_query()</code> 6 60s <pre><code>rlm-code bench preset=pure_rlm_context\n</code></pre>"},{"location":"benchmarks/presets/#6-deep_recursion-deep-recursion-tests-3-cases","title":"6. <code>deep_recursion</code> -- Deep Recursion Tests (3 cases)","text":"<p>Key Differentiator</p> <p>These tests exercise recursion depth &gt; 1, which exceeds the limitation of the original RLM paper (depth=1 only). This is a key differentiator of RLM Code.</p> Case ID Description Max Steps Timeout <code>nested_analysis_depth2</code> Nested recursive analysis with 3 specialist agents (depth=2) 8 90s <code>hierarchical_decomposition</code> Hierarchical task decomposition with sub-specialists 10 120s <code>parallel_recursive_batch</code> Parallel recursive calls using <code>delegate_batch</code> 8 120s <pre><code>rlm-code bench preset=deep_recursion\n</code></pre>"},{"location":"benchmarks/presets/#7-paradigm_comparison-side-by-side-paradigm-comparison-3-cases","title":"7. <code>paradigm_comparison</code> -- Side-by-Side Paradigm Comparison (3 cases)","text":"<p>Tasks designed to be run across Pure RLM, CodeAct, and Traditional paradigms.</p> Case ID Description Max Steps Timeout <code>document_summary</code> Document summarization across paradigms 5 60s <code>information_extraction</code> Extract dates, names, monetary values 5 60s <code>multi_hop_reasoning</code> Multi-hop reasoning combining multiple context sections 6 90s <pre><code>rlm-code bench preset=paradigm_comparison\n</code></pre>"},{"location":"benchmarks/presets/#8-oolong_style-oolong-style-long-context-4-cases","title":"8. <code>oolong_style</code> -- OOLONG-Style Long Context (4 cases)","text":"<p>Paper-Compatible</p> <p>Based on the OOLONG benchmark from the RLM paper evaluation suite. Tests long-context handling with programmatic search.</p> Case ID Description Max Steps Timeout <code>oolong_passage_retrieval</code> Retrieve specific passage from ~50K token document 6 90s <code>oolong_needle_in_haystack</code> Find hidden needle fact without loading full document 5 60s <code>oolong_multi_doc_qa</code> Answer question requiring info from 2+ documents 7 120s <code>oolong_summarize_long</code> Hierarchical summarization of 50K+ char document 8 180s <pre><code>rlm-code bench preset=oolong_style\n</code></pre>"},{"location":"benchmarks/presets/#9-browsecomp_style-browsecomp-plus-style-3-cases","title":"9. <code>browsecomp_style</code> -- BrowseComp-Plus Style (3 cases)","text":"<p>Web reasoning benchmarks adapted for structured data analysis.</p> Case ID Description Max Steps Timeout <code>browsecomp_fact_verification</code> Verify claim from structured JSON/CSV data 5 60s <code>browsecomp_entity_resolution</code> Resolve entity aliases across sources 6 90s <code>browsecomp_temporal_reasoning</code> Temporal reasoning over event timelines 6 90s <pre><code>rlm-code bench preset=browsecomp_style\n</code></pre>"},{"location":"benchmarks/presets/#10-token_efficiency-token-efficiency-comparison-3-cases","title":"10. <code>token_efficiency</code> -- Token Efficiency Comparison (3 cases)","text":"<p>RLM's Key Advantage</p> <p>These benchmarks demonstrate the token efficiency gains of the RLM approach -- metadata-only context loading vs full document ingestion.</p> Case ID Description Max Steps Timeout <code>efficiency_100k_context</code> Process 100K char context, report token metrics 6 120s <code>efficiency_incremental_context</code> Incremental context loading vs upfront 7 120s <code>efficiency_recursive_delegation</code> Recursive delegation token tracking per level 8 150s <pre><code>rlm-code bench preset=token_efficiency\n</code></pre>"},{"location":"benchmarks/presets/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"benchmarks/presets/#cli","title":"CLI","text":"<pre><code># Run a built-in preset\nrlm-code bench preset=dspy_quick\n\n# Run with a custom YAML pack\nrlm-code bench preset=my_suite --pack benchmarks/my_benchmarks.yaml\n\n# Run with multiple packs\nrlm-code bench preset=combined --pack pack1.yaml --pack pack2.json\n</code></pre>"},{"location":"benchmarks/presets/#programmatic","title":"Programmatic","text":"<pre><code>from rlm_code.rlm.benchmarks import get_benchmark_cases, list_benchmark_presets\n\n# List all available presets\nfor preset in list_benchmark_presets():\n    print(f\"{preset['preset']}: {preset['cases']} cases - {preset['description']}\")\n\n# Get cases for a specific preset\ncases = get_benchmark_cases(\"dspy_quick\")\nfor case in cases:\n    print(f\"  {case.case_id}: {case.description} (max_steps={case.max_steps})\")\n</code></pre>"},{"location":"benchmarks/presets/#custom-yaml-pack-loading","title":"Custom YAML Pack Loading","text":"<p>The <code>load_benchmark_packs()</code> function supports 5 different file formats:</p>"},{"location":"benchmarks/presets/#format-1-explicit-preset-mapping","title":"Format 1: Explicit Preset Mapping","text":"<pre><code>presets:\n  my_suite:\n    description: \"My custom benchmark suite\"\n    cases:\n      - id: case_1\n        description: \"First test case\"\n        task: \"Write a hello world program\"\n        environment: generic\n        max_steps: 3\n        exec_timeout: 30\n      - id: case_2\n        description: \"Second test case\"\n        task: \"Build a data pipeline\"\n        environment: dspy\n        max_steps: 5\n        exec_timeout: 60\n</code></pre>"},{"location":"benchmarks/presets/#format-2-top-level-preset-mapping-no-presets-wrapper","title":"Format 2: Top-Level Preset Mapping (no <code>presets:</code> wrapper)","text":"<pre><code>my_suite:\n  description: \"Suite without wrapper\"\n  cases:\n    - id: test_1\n      task: \"Do something\"\n</code></pre>"},{"location":"benchmarks/presets/#format-3-pydantic-style-dataset-cases-key","title":"Format 3: Pydantic-Style Dataset (<code>cases</code> key)","text":"<pre><code>{\n  \"name\": \"my_dataset\",\n  \"description\": \"A test dataset\",\n  \"cases\": [\n    {\"id\": \"q1\", \"task\": \"What is 2+2?\", \"environment\": \"generic\"},\n    {\"id\": \"q2\", \"question\": \"Explain recursion\", \"environment\": \"generic\"}\n  ]\n}\n</code></pre> <p>Flexible Task Field</p> <p>The loader checks multiple field names for the task prompt: <code>task</code>, <code>prompt</code>, <code>question</code>, <code>query</code>, <code>instruction</code>, <code>input</code>. It also searches inside an <code>inputs</code> dict if present.</p>"},{"location":"benchmarks/presets/#format-4-google-adk-eval-set-eval_cases-key","title":"Format 4: Google ADK Eval Set (<code>eval_cases</code> key)","text":"<pre><code>{\n  \"name\": \"adk_eval\",\n  \"eval_cases\": [\n    {\n      \"eval_id\": \"e1\",\n      \"conversation\": [\n        {\"user_content\": {\"parts\": [{\"text\": \"Help me write a function\"}]}}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"benchmarks/presets/#format-5-generic-record-list-jsonl-json-array","title":"Format 5: Generic Record List (JSONL, JSON array)","text":"<pre><code>{\"id\": \"r1\", \"prompt\": \"Write a sort function\", \"environment\": \"generic\"}\n{\"id\": \"r2\", \"prompt\": \"Build a REST API\", \"environment\": \"generic\"}\n</code></pre>"},{"location":"benchmarks/presets/#supported-file-extensions","title":"Supported File Extensions","text":"Extension Parser <code>.yaml</code>, <code>.yml</code> YAML (<code>yaml.safe_load</code>) <code>.json</code> JSON (<code>json.loads</code>) <code>.jsonl</code> JSONL (line-by-line <code>json.loads</code>)"},{"location":"benchmarks/presets/#api-reference","title":"API Reference","text":""},{"location":"benchmarks/presets/#list_benchmark_presets","title":"<code>list_benchmark_presets()</code>","text":"<pre><code>def list_benchmark_presets(\n    extra_presets: dict[str, list[RLMBenchmarkCase]] | None = None,\n    *,\n    extra_descriptions: dict[str, str] | None = None,\n    extra_sources: dict[str, str] | None = None,\n) -&gt; list[dict[str, str | int]]:\n</code></pre> <p>Returns a list of dicts with keys <code>preset</code>, <code>cases</code> (count), <code>description</code>, and optionally <code>source</code>.</p>"},{"location":"benchmarks/presets/#get_benchmark_cases","title":"<code>get_benchmark_cases()</code>","text":"<pre><code>def get_benchmark_cases(\n    preset: str,\n    *,\n    extra_presets: dict[str, list[RLMBenchmarkCase]] | None = None,\n) -&gt; list[RLMBenchmarkCase]:\n</code></pre> <p>Returns the list of <code>RLMBenchmarkCase</code> objects for a named preset. Raises <code>ValueError</code> for unknown preset names.</p>"},{"location":"benchmarks/presets/#load_benchmark_packs","title":"<code>load_benchmark_packs()</code>","text":"<pre><code>def load_benchmark_packs(\n    paths: list[str | Path] | None,\n    *,\n    workdir: Path | None = None,\n) -&gt; tuple[\n    dict[str, list[RLMBenchmarkCase]],   # presets\n    dict[str, str],                       # descriptions\n    dict[str, str],                       # sources (file paths)\n]:\n</code></pre> <p>Loads one or more pack files and returns merged presets, descriptions, and source file paths.</p>"},{"location":"benchmarks/presets/#creating-custom-benchmarks","title":"Creating Custom Benchmarks","text":""},{"location":"benchmarks/presets/#step-1-define-your-cases","title":"Step 1: Define Your Cases","text":"<p>Create a YAML file with your benchmark cases:</p> <pre><code>presets:\n  code_review:\n    description: \"Code review benchmark suite (5 cases)\"\n    cases:\n      - id: review_syntax\n        description: \"Find syntax errors in Python code\"\n        task: \"Review the following code for syntax errors and fix them...\"\n        environment: generic\n        max_steps: 4\n        exec_timeout: 30\n\n      - id: review_logic\n        description: \"Find logic bugs\"\n        task: \"Review the following code for logic bugs...\"\n        environment: generic\n        max_steps: 5\n        exec_timeout: 45\n\n      - id: review_perf\n        description: \"Identify performance issues\"\n        task: \"Review the following code for performance issues...\"\n        environment: generic\n        max_steps: 5\n        exec_timeout: 45\n\n      - id: review_security\n        description: \"Find security vulnerabilities\"\n        task: \"Review the following code for security vulnerabilities...\"\n        environment: generic\n        max_steps: 6\n        exec_timeout: 60\n\n      - id: review_refactor\n        description: \"Suggest refactoring improvements\"\n        task: \"Suggest refactoring improvements for the following code...\"\n        environment: generic\n        max_steps: 5\n        exec_timeout: 45\n</code></pre>"},{"location":"benchmarks/presets/#step-2-run-your-benchmark","title":"Step 2: Run Your Benchmark","text":"<pre><code>rlm-code bench preset=code_review --pack code_review_bench.yaml\n</code></pre>"},{"location":"benchmarks/presets/#step-3-view-results","title":"Step 3: View Results","text":"<p>Results are saved as JSON in <code>.rlm_code/rlm/benchmarks/</code> and automatically appear in the leaderboard:</p> <pre><code>rlm-code leaderboard --metric reward\n</code></pre>"},{"location":"benchmarks/presets/#merging-with-built-in-presets","title":"Merging with Built-In Presets","text":"<p>Custom packs merge with built-in presets. If a custom preset name collides with a built-in name, the custom version overrides it. This lets you refine or replace built-in suites:</p> <pre><code>from rlm_code.rlm.benchmarks import get_benchmark_cases, load_benchmark_packs\n\n# Load custom packs\nextra_presets, extra_descriptions, extra_sources = load_benchmark_packs(\n    [\"my_benchmarks.yaml\"],\n    workdir=Path.cwd(),\n)\n\n# Get cases (custom overrides built-in if name matches)\ncases = get_benchmark_cases(\"dspy_quick\", extra_presets=extra_presets)\n</code></pre>"},{"location":"benchmarks/session-replay/","title":"Session Replay","text":"<p>Session replay provides full state recording and time-travel debugging for RLM runs. Every action, observation, reward, memory update, and variable change is captured in a structured event stream that can be replayed step by step, navigated forward and backward, checkpointed, and compared across sessions.</p> <p>Module: <code>rlm_code.rlm.session_replay</code></p>"},{"location":"benchmarks/session-replay/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    Runner[\"RLMRunner\"] --&gt;|events| Recorder[\"SessionRecorder\"]\n    Recorder --&gt; Events[\"SessionEvent stream\"]\n    Recorder --&gt; Snapshot[\"SessionSnapshot\"]\n    Snapshot --&gt; Store[\"SessionStore\\n(disk persistence)\"]\n    Snapshot --&gt; Replayer[\"SessionReplayer\"]\n    Replayer --&gt; Nav[\"Navigation\\nstep_forward/backward/goto\"]\n    Snapshot --&gt; Compare[\"SessionComparison\\n(diff two sessions)\"]\n    Events --&gt;|JSONL file| Loader[\"load_session()\"]\n    Loader --&gt; Replayer</code></pre>"},{"location":"benchmarks/session-replay/#core-types","title":"Core Types","text":""},{"location":"benchmarks/session-replay/#sessioneventtype","title":"SessionEventType","text":"<p>Enum of all possible event types in a session:</p> <pre><code>class SessionEventType(Enum):\n    # Lifecycle\n    SESSION_START = \"session_start\"\n    SESSION_END = \"session_end\"\n\n    # Execution\n    STEP_START = \"step_start\"\n    STEP_ACTION = \"step_action\"\n    STEP_RESULT = \"step_result\"\n    STEP_END = \"step_end\"\n\n    # State changes\n    STATE_SNAPSHOT = \"state_snapshot\"\n    MEMORY_UPDATE = \"memory_update\"\n    VARIABLE_UPDATE = \"variable_update\"\n\n    # LLM interactions\n    LLM_REQUEST = \"llm_request\"\n    LLM_RESPONSE = \"llm_response\"\n\n    # Child/recursive\n    CHILD_SPAWN = \"child_spawn\"\n    CHILD_RESULT = \"child_result\"\n\n    # Termination\n    FINAL_DETECTED = \"final_detected\"\n    CHECKPOINT = \"checkpoint\"\n\n    # Errors\n    ERROR = \"error\"\n</code></pre> Category Events Description Lifecycle <code>SESSION_START</code>, <code>SESSION_END</code> Session boundaries Execution <code>STEP_START</code>, <code>STEP_ACTION</code>, <code>STEP_RESULT</code>, <code>STEP_END</code> Per-step execution flow State <code>STATE_SNAPSHOT</code>, <code>MEMORY_UPDATE</code>, <code>VARIABLE_UPDATE</code> State mutations LLM <code>LLM_REQUEST</code>, <code>LLM_RESPONSE</code> LLM call tracking Recursive <code>CHILD_SPAWN</code>, <code>CHILD_RESULT</code> Child agent lifecycle Termination <code>FINAL_DETECTED</code>, <code>CHECKPOINT</code> Completion and checkpointing Errors <code>ERROR</code> Error events"},{"location":"benchmarks/session-replay/#sessionevent","title":"SessionEvent","text":"<p>A single event in the session event stream.</p> <pre><code>@dataclass\nclass SessionEvent:\n    event_type: SessionEventType     # Type of event\n    timestamp: str                   # ISO 8601 UTC timestamp\n    step: int                        # Current step number\n    data: dict[str, Any]             # Event-specific payload\n\n    # Optional fields\n    run_id: str = \"\"                 # Run identifier\n    depth: int = 0                   # Recursion depth\n    parent_id: str | None = None     # Parent agent ID\n    duration_ms: float | None = None # Duration in milliseconds\n</code></pre> <p>Events support serialization via <code>to_dict()</code> and <code>from_dict()</code> for JSONL persistence.</p>"},{"location":"benchmarks/session-replay/#stepstate","title":"StepState","text":"<p>Captures the complete state at a single step -- the fundamental unit of replay navigation.</p> <pre><code>@dataclass\nclass StepState:\n    step: int                        # Step number\n    timestamp: str                   # ISO timestamp\n\n    # Action state\n    action_type: str = \"\"            # e.g., \"run_python\", \"submit\"\n    action_code: str = \"\"            # Code that was executed\n    action_rationale: str = \"\"       # LLM's reasoning for the action\n\n    # Result state\n    success: bool = False            # Whether the step succeeded\n    output: str = \"\"                 # stdout / output text\n    error: str = \"\"                  # stderr / error text\n    reward: float = 0.0              # Step reward\n    cumulative_reward: float = 0.0   # Running total\n\n    # Execution metrics\n    duration_ms: float = 0.0         # Step duration\n    tokens_used: int = 0             # Tokens consumed\n\n    # Memory state\n    memory_notes: list[str]          # Memory notes at this point\n\n    # Variables\n    variables: dict[str, Any]        # REPL variables at this point\n\n    # Raw data\n    raw_action: dict[str, Any]       # Full action dict\n    raw_observation: dict[str, Any]  # Full observation dict\n</code></pre> <p>Point-in-Time Capture</p> <p>The <code>variables</code> and <code>memory_notes</code> fields capture the complete state at that step, not just the delta. This enables true time-travel debugging -- you can jump to any step and see the full variable and memory state as it existed at that moment.</p>"},{"location":"benchmarks/session-replay/#sessionsnapshot","title":"SessionSnapshot","text":"<p>A complete point-in-time capture of an entire session.</p> <pre><code>@dataclass\nclass SessionSnapshot:\n    # Identification\n    snapshot_id: str          # Unique snapshot hash\n    session_id: str           # Session identifier\n    run_id: str               # Run identifier\n    created_at: str           # ISO timestamp\n\n    # Position\n    step: int                 # Current step position\n    total_steps: int          # Total number of steps\n\n    # Task info\n    task: str                 # Task description\n    environment: str          # Environment name\n    model: str = \"\"           # Model identifier\n\n    # Completion state\n    completed: bool = False   # Whether the run completed\n    final_answer: str = \"\"    # The final answer text\n\n    # Metrics\n    total_reward: float = 0.0      # Cumulative reward\n    total_tokens: int = 0          # Total tokens used\n    duration_seconds: float = 0.0  # Total duration\n\n    # Step history\n    steps: list[StepState]         # All step states\n\n    # Memory and variables\n    memory_notes: list[str]        # Final memory state\n    variables: dict[str, Any]      # Final variable state\n\n    # Metadata\n    metadata: dict[str, Any]       # Arbitrary metadata\n</code></pre>"},{"location":"benchmarks/session-replay/#key-methods","title":"Key Methods","text":"Method Returns Description <code>get_step(n)</code> <code>StepState | None</code> Get state at step <code>n</code> <code>get_reward_curve()</code> <code>list[dict]</code> Get <code>[{step, reward, cumulative_reward}, ...]</code> <code>to_dict()</code> <code>dict</code> Serialize to dictionary <code>from_dict(data)</code> <code>SessionSnapshot</code> Deserialize from dictionary"},{"location":"benchmarks/session-replay/#sessionrecorder","title":"SessionRecorder","text":"<p>Records session events during live execution. Create one at the start of a run and call its methods as the run progresses.</p>"},{"location":"benchmarks/session-replay/#construction","title":"Construction","text":"<pre><code>from rlm_code.rlm.session_replay import SessionRecorder\n\nrecorder = SessionRecorder(\n    session_id=\"session_abc123\",\n    run_id=\"run_xyz789\",\n    task=\"Build a DSPy signature for essay scoring\",\n    environment=\"dspy\",\n    model=\"gpt-4o\",\n    output_path=Path(\".rlm_code/sessions/session_abc123.jsonl\"),\n)\n</code></pre> <p>If <code>output_path</code> is provided, events are written to JSONL as they occur (streaming persistence).</p>"},{"location":"benchmarks/session-replay/#recording-api","title":"Recording API","text":""},{"location":"benchmarks/session-replay/#step-lifecycle","title":"Step Lifecycle","text":"<pre><code># Start of step\nrecorder.record_step_start(step=1)\n\n# Record the action\nrecorder.record_action(\n    action={\"action\": \"run_python\", \"code\": \"import dspy\"},\n    rationale=\"Setting up DSPy imports\",\n)\n\n# Record the result\nrecorder.record_result(\n    observation={\"success\": True, \"output\": \"OK\"},\n    reward=0.5,\n    success=True,\n    duration_ms=150.0,\n    tokens_used=500,\n)\n\n# End of step (captures full StepState)\nrecorder.record_step_end(\n    action={\"action\": \"run_python\", \"code\": \"import dspy\"},\n    observation={\"success\": True, \"output\": \"OK\"},\n    reward=0.5,\n    success=True,\n    duration_ms=150.0,\n    tokens_used=500,\n)\n</code></pre>"},{"location":"benchmarks/session-replay/#state-updates","title":"State Updates","text":"<pre><code># Memory update\nrecorder.record_memory_update([\"DSPy imported\", \"Working on signature\"])\n\n# Variable update\nrecorder.record_variable_update(\"result\", {\"score\": 0.95})\n</code></pre>"},{"location":"benchmarks/session-replay/#llm-interactions","title":"LLM Interactions","text":"<pre><code># LLM request\nrecorder.record_llm_request(\n    prompt=\"Create a DSPy signature...\",\n    model=\"gpt-4o\",\n)\n\n# LLM response\nrecorder.record_llm_response(\n    response=\"class EssaySignature(dspy.Signature):...\",\n    tokens_in=200,\n    tokens_out=150,\n    duration_ms=800.0,\n)\n</code></pre>"},{"location":"benchmarks/session-replay/#recursive-agent-events","title":"Recursive Agent Events","text":"<pre><code># Child spawn\nrecorder.record_child_spawn(\n    child_id=\"child_001\",\n    task=\"Analyze section 1\",\n    depth=1,\n)\n\n# Child result\nrecorder.record_child_result(\n    child_id=\"child_001\",\n    success=True,\n    result=\"Section 1 contains financial data...\",\n)\n</code></pre>"},{"location":"benchmarks/session-replay/#completion-and-errors","title":"Completion and Errors","text":"<pre><code># Final answer detected\nrecorder.record_final(answer=\"The essay score is 4/5\", completed=True)\n\n# Error\nrecorder.record_error(error=\"Timeout exceeded\", recoverable=True)\n</code></pre>"},{"location":"benchmarks/session-replay/#checkpointing","title":"Checkpointing","text":"<pre><code># Create a checkpoint at the current step\nsnapshot = recorder.create_checkpoint(name=\"before_refactor\")\n</code></pre>"},{"location":"benchmarks/session-replay/#ending-the-session","title":"Ending the Session","text":"<pre><code># End session and get final snapshot\nfinal_snapshot = recorder.end_session()\n</code></pre>"},{"location":"benchmarks/session-replay/#sessionreplayer","title":"SessionReplayer","text":"<p>Replays a recorded session with step-by-step navigation.</p>"},{"location":"benchmarks/session-replay/#loading-a-session","title":"Loading a Session","text":"From a snapshot file (JSON)From a JSONL trajectoryFrom a snapshot objectUsing the convenience function <pre><code>from rlm_code.rlm.session_replay import SessionReplayer\n\nreplayer = SessionReplayer.from_file(Path(\"session_snapshot.json\"))\n</code></pre> <pre><code>replayer = SessionReplayer.from_jsonl(\n    Path(\".rlm_code/rlm/observability/steps/abc12345.jsonl\")\n)\n</code></pre> <pre><code>replayer = SessionReplayer(snapshot=my_snapshot)\n</code></pre> <pre><code>from rlm_code.rlm.session_replay import load_session\n\n# Automatically detects format from extension\nreplayer = load_session(\".rlm_code/sessions/session_abc.jsonl\")\nreplayer = load_session(\"snapshots/session_abc.json\")\n</code></pre>"},{"location":"benchmarks/session-replay/#navigation","title":"Navigation","text":"<pre><code># Properties\nreplayer.current_step    # Current position (int)\nreplayer.total_steps     # Total steps available (int)\nreplayer.at_start        # True if at step 0\nreplayer.at_end          # True if past last step\n\n# Forward navigation\nstate = replayer.step_forward()   # Move forward one step\nif state:\n    print(f\"Step {state.step}: {state.action_type} -&gt; reward={state.reward}\")\n\n# Backward navigation\nstate = replayer.step_backward()  # Move backward one step\n\n# Jump to specific step\nstate = replayer.goto_step(5)     # Jump directly to step 5\n\n# Jump to boundaries\nreplayer.goto_start()             # Jump to step 0\nreplayer.goto_end()               # Jump past the last step\n\n# Get state at current position\nstate = replayer.get_current_state()\n</code></pre>"},{"location":"benchmarks/session-replay/#iteration","title":"Iteration","text":"<pre><code># Iterate through all remaining steps\nfor step in replayer.iterate_steps():\n    print(f\"Step {step.step}: action={step.action_type}, \"\n          f\"success={step.success}, reward={step.reward}\")\n    print(f\"  Code: {step.action_code[:80]}\")\n    print(f\"  Output: {step.output[:80]}\")\n    print(f\"  Variables: {list(step.variables.keys())}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#search","title":"Search","text":"<pre><code># Find first error step\nerror_step = replayer.find_step(lambda s: bool(s.error))\nif error_step:\n    print(f\"First error at step {error_step.step}: {error_step.error}\")\n\n# Find all errors\nerrors = replayer.find_errors()\nprint(f\"Found {len(errors)} error steps\")\n\n# Find all successes\nsuccesses = replayer.find_successes()\nprint(f\"Found {len(successes)} successful steps\")\n\n# Custom search from current position\nhigh_reward = replayer.find_step(lambda s: s.reward &gt; 0.8, from_current=True)\n</code></pre>"},{"location":"benchmarks/session-replay/#summary","title":"Summary","text":"<pre><code>summary = replayer.get_summary()\nprint(summary)\n# {\n#     \"session_id\": \"session_abc123\",\n#     \"run_id\": \"run_xyz789\",\n#     \"task\": \"Build a DSPy signature...\",\n#     \"environment\": \"dspy\",\n#     \"model\": \"gpt-4o\",\n#     \"completed\": True,\n#     \"total_steps\": 4,\n#     \"total_reward\": 2.0,\n#     \"total_tokens\": 3500,\n#     \"duration_seconds\": 45.2,\n#     \"success_rate\": 0.75,\n#     \"error_count\": 1,\n# }\n</code></pre>"},{"location":"benchmarks/session-replay/#sessionstore","title":"SessionStore","text":"<p>Persistent storage for session snapshots and checkpoints.</p>"},{"location":"benchmarks/session-replay/#construction_1","title":"Construction","text":"<pre><code>from rlm_code.rlm.session_replay import SessionStore\n\nstore = SessionStore(base_dir=Path(\"~/.rlm_code/sessions\"))\n</code></pre> <p>Default base directory: <code>~/.rlm_code/sessions</code></p>"},{"location":"benchmarks/session-replay/#directory-layout","title":"Directory Layout","text":"<pre><code>~/.rlm_code/sessions/\n    snapshots/\n        &lt;session_id&gt;_&lt;snapshot_id&gt;.json\n    checkpoints/\n        &lt;session_id&gt;_&lt;checkpoint_name&gt;.json\n</code></pre>"},{"location":"benchmarks/session-replay/#saving-and-loading","title":"Saving and Loading","text":"<pre><code># Save a snapshot\npath = store.save_snapshot(snapshot)\nprint(f\"Saved to: {path}\")\n\n# Load a snapshot by ID\nsnapshot = store.load_snapshot(\"abc123def456\")\n\n# Save a checkpoint\npath = store.save_checkpoint(snapshot, name=\"before_refactor\")\n\n# Load a checkpoint\nsnapshot = store.load_checkpoint(\n    session_id=\"session_abc123\",\n    name=\"before_refactor\",\n)\n</code></pre>"},{"location":"benchmarks/session-replay/#listing","title":"Listing","text":"<pre><code># List all saved sessions\nsessions = store.list_sessions()\nfor s in sessions:\n    print(f\"{s['session_id']}: {s['task'][:50]} \"\n          f\"({s['total_steps']} steps, completed={s['completed']})\")\n\n# List checkpoints (optionally filtered by session)\ncheckpoints = store.list_checkpoints(session_id=\"session_abc123\")\nfor cp in checkpoints:\n    print(f\"  Checkpoint: {cp['checkpoint_name']} at step {cp['step']}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#cleanup","title":"Cleanup","text":"<pre><code># Delete all snapshots for a session\ncount = store.delete_session(\"session_abc123\")\nprint(f\"Deleted {count} snapshots\")\n\n# Delete a specific checkpoint\ndeleted = store.delete_checkpoint(\"session_abc123\", \"before_refactor\")\n\n# Clean up sessions older than 30 days\ncount = store.cleanup_old(days=30)\nprint(f\"Cleaned up {count} old files\")\n</code></pre>"},{"location":"benchmarks/session-replay/#sessioncomparison","title":"SessionComparison","text":"<p>Compare two sessions to find performance differences and the point of divergence.</p> <pre><code>from rlm_code.rlm.session_replay import compare_sessions\n\ncomparison = compare_sessions(snapshot_a, snapshot_b)\n</code></pre>"},{"location":"benchmarks/session-replay/#comparison-fields","title":"Comparison Fields","text":"<pre><code>@dataclass\nclass SessionComparison:\n    session_a_id: str\n    session_b_id: str\n\n    # Completion\n    a_completed: bool\n    b_completed: bool\n\n    # Metrics\n    a_steps: int\n    b_steps: int\n    a_reward: float\n    b_reward: float\n    a_tokens: int\n    b_tokens: int\n\n    # Deltas (b - a)\n    step_delta: int          # Positive = B took more steps\n    reward_delta: float      # Positive = B got higher reward\n    token_delta: int         # Positive = B used more tokens\n\n    # Efficiency\n    a_efficiency: float      # reward / tokens * 1000\n    b_efficiency: float\n    efficiency_delta: float  # Positive = B is more efficient\n\n    # Divergence point\n    first_divergence_step: int | None = None\n    divergence_reason: str = \"\"\n</code></pre>"},{"location":"benchmarks/session-replay/#divergence-detection","title":"Divergence Detection","text":"<p>The comparison function iterates through the steps of both sessions and identifies the first point where they differ:</p> <ol> <li>Different action type: e.g., one session runs code while the other submits</li> <li>Different code: Both run code but with different content</li> <li>Different success: Same code produces different outcomes</li> </ol> <pre><code>comparison = compare_sessions(snapshot_a, snapshot_b)\n\nif comparison.first_divergence_step is not None:\n    print(f\"Sessions diverge at step {comparison.first_divergence_step}\")\n    print(f\"Reason: {comparison.divergence_reason}\")\nelse:\n    print(\"Sessions followed the same execution path\")\n\nprint(f\"Reward delta: {comparison.reward_delta:+.3f}\")\nprint(f\"Token delta: {comparison.token_delta:+,}\")\nprint(f\"Efficiency delta: {comparison.efficiency_delta:+.4f}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#example-output","title":"Example Output","text":"<pre><code>Sessions diverge at step 2\nReason: Different code\nReward delta: +0.500\nToken delta: -200\nEfficiency delta: +0.1200\n</code></pre>"},{"location":"benchmarks/session-replay/#jsonl-compatibility","title":"JSONL Compatibility","text":"<p>The session replay system is fully compatible with the trajectory JSONL files produced by the <code>LocalJSONLSink</code>. The <code>from_jsonl()</code> loader handles both formats:</p>"},{"location":"benchmarks/session-replay/#native-session-events","title":"Native Session Events","text":"<pre><code>{\"event_type\": \"session_start\", \"timestamp\": \"...\", \"step\": 0, \"data\": {\"task\": \"...\"}}\n{\"event_type\": \"step_start\", \"timestamp\": \"...\", \"step\": 1, \"data\": {}}\n{\"event_type\": \"step_action\", \"timestamp\": \"...\", \"step\": 1, \"data\": {\"action\": {...}}}\n{\"event_type\": \"step_result\", \"timestamp\": \"...\", \"step\": 1, \"data\": {\"observation\": {...}}}\n{\"event_type\": \"step_end\", \"timestamp\": \"...\", \"step\": 1, \"data\": {...}}\n</code></pre>"},{"location":"benchmarks/session-replay/#legacy-trajectory-events","title":"Legacy Trajectory Events","text":"<pre><code>{\"event_type\": \"run_start\", \"timestamp\": \"...\", \"data\": {\"task\": \"...\"}}\n{\"event_type\": \"iteration_start\", \"iteration\": 1, \"data\": {}}\n{\"event_type\": \"iteration_code\", \"iteration\": 1, \"data\": {\"action\": {...}}}\n{\"event_type\": \"iteration_output\", \"iteration\": 1, \"data\": {\"observation\": {...}}}\n{\"event_type\": \"iteration_end\", \"iteration\": 1, \"data\": {...}}\n</code></pre>"},{"location":"benchmarks/session-replay/#legacy-step-format","title":"Legacy Step Format","text":"<pre><code>{\"type\": \"step\", \"step\": 1, \"action\": {...}, \"observation\": {...}, \"reward\": 0.5}\n{\"type\": \"final\", \"steps\": 3, \"completed\": true, \"total_reward\": 1.5}\n</code></pre> <p>All three formats are automatically detected and converted to <code>SessionEvent</code> objects, then assembled into a <code>SessionSnapshot</code> for replay.</p>"},{"location":"benchmarks/session-replay/#time-travel-debugging-workflow","title":"Time-Travel Debugging Workflow","text":"<p>Here is a complete workflow for investigating a failed or underperforming run:</p>"},{"location":"benchmarks/session-replay/#step-1-load-the-session","title":"Step 1: Load the Session","text":"<pre><code>from rlm_code.rlm.session_replay import load_session\n\nreplayer = load_session(\".rlm_code/rlm/observability/steps/abc12345.jsonl\")\nsummary = replayer.get_summary()\nprint(f\"Run: {summary['run_id']}\")\nprint(f\"Completed: {summary['completed']}, Reward: {summary['total_reward']}\")\nprint(f\"Steps: {summary['total_steps']}, Errors: {summary['error_count']}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#step-2-find-the-first-error","title":"Step 2: Find the First Error","text":"<pre><code>errors = replayer.find_errors()\nif errors:\n    first_error = errors[0]\n    print(f\"\\nFirst error at step {first_error.step}:\")\n    print(f\"  Action: {first_error.action_type}\")\n    print(f\"  Code:\\n    {first_error.action_code[:200]}\")\n    print(f\"  Error:\\n    {first_error.error[:200]}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#step-3-inspect-state-before-the-error","title":"Step 3: Inspect State Before the Error","text":"<pre><code># Jump to the step before the error\nprev_step = replayer.goto_step(first_error.step - 1)\nif prev_step:\n    print(f\"\\nState before error (step {prev_step.step}):\")\n    print(f\"  Success: {prev_step.success}\")\n    print(f\"  Variables: {list(prev_step.variables.keys())}\")\n    print(f\"  Memory: {prev_step.memory_notes}\")\n    print(f\"  Cumulative reward: {prev_step.cumulative_reward}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#step-4-walk-through-remaining-steps","title":"Step 4: Walk Through Remaining Steps","text":"<pre><code>print(\"\\nRemaining steps:\")\nfor step in replayer.iterate_steps():\n    status = \"OK\" if step.success else \"FAIL\"\n    print(f\"  Step {step.step} [{status}]: {step.action_type} \"\n          f\"(reward={step.reward:+.2f}, cum={step.cumulative_reward:.2f})\")\n</code></pre>"},{"location":"benchmarks/session-replay/#step-5-examine-the-reward-curve","title":"Step 5: Examine the Reward Curve","text":"<pre><code>curve = replayer.snapshot.get_reward_curve()\nprint(\"\\nReward curve:\")\nfor point in curve:\n    bar = \"#\" * int(point[\"cumulative_reward\"] * 20)\n    print(f\"  Step {point['step']}: {point['reward']:+.2f} \"\n          f\"(cum: {point['cumulative_reward']:.2f}) {bar}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#step-6-compare-with-a-successful-run","title":"Step 6: Compare with a Successful Run","text":"<pre><code>from rlm_code.rlm.session_replay import load_session, compare_sessions\n\nreplayer_good = load_session(\"good_run.jsonl\")\nreplayer_bad = load_session(\"bad_run.jsonl\")\n\ncomparison = compare_sessions(replayer_good.snapshot, replayer_bad.snapshot)\nprint(f\"\\nDivergence at step {comparison.first_divergence_step}: \"\n      f\"{comparison.divergence_reason}\")\nprint(f\"Good run: {comparison.a_steps} steps, reward={comparison.a_reward:.2f}\")\nprint(f\"Bad run:  {comparison.b_steps} steps, reward={comparison.b_reward:.2f}\")\n</code></pre>"},{"location":"benchmarks/session-replay/#convenience-functions","title":"Convenience Functions","text":""},{"location":"benchmarks/session-replay/#load_session","title":"<code>load_session()</code>","text":"<pre><code>def load_session(path: Path | str) -&gt; SessionReplayer:\n    \"\"\"Load a session for replay. Auto-detects format from file extension.\"\"\"\n</code></pre> <ul> <li><code>.jsonl</code> files are loaded via <code>SessionReplayer.from_jsonl()</code></li> <li>All other extensions are loaded via <code>SessionReplayer.from_file()</code> (JSON snapshot)</li> </ul>"},{"location":"benchmarks/session-replay/#create_recorder","title":"<code>create_recorder()</code>","text":"<pre><code>def create_recorder(\n    task: str,\n    environment: str,\n    run_id: str | None = None,\n    output_dir: Path | None = None,\n) -&gt; SessionRecorder:\n    \"\"\"Create a new session recorder with auto-generated IDs.\"\"\"\n</code></pre> <p>If <code>run_id</code> is not provided, one is generated as <code>run_&lt;hex8&gt;</code>. The session ID is always auto-generated as <code>session_&lt;hex8&gt;</code>. If <code>output_dir</code> is provided, events are streamed to a JSONL file inside that directory.</p> <pre><code>from rlm_code.rlm.session_replay import create_recorder\n\nrecorder = create_recorder(\n    task=\"Analyze financial data\",\n    environment=\"pure_rlm\",\n    output_dir=Path(\".rlm_code/sessions\"),\n)\n# recorder.session_id = \"session_a1b2c3d4\"\n# recorder.output_path = \".rlm_code/sessions/session_a1b2c3d4.jsonl\"\n</code></pre>"},{"location":"core/","title":"Core Engine","text":"<p>The Core Engine is the heart of RLM Code. It implements the Recursive Language Model paradigm from the 2025 research paper, providing a complete runtime for context-as-variable reasoning, iterative code execution, reward-driven optimization, and multi-paradigm orchestration.</p>"},{"location":"core/#architecture-overview","title":"Architecture Overview","text":"<p>The Core Engine follows a context -&gt; action proposal -&gt; sandbox execution -&gt; observation -&gt; reward -&gt; memory update loop. Unlike traditional coding agents that load full context into the LLM's token window, RLM Code stores context as REPL variables and exposes only metadata (type, length, preview) to the LLM. The LLM then accesses the data programmatically through code execution.</p> <pre><code>                    +------------------+\n                    |    RLMRunner      |\n                    | (Orchestrator)    |\n                    +--------+---------+\n                             |\n          +------------------+------------------+\n          |                  |                  |\n  +-------v------+  +-------v------+  +--------v-------+\n  | Pure RLM Env |  | DSPy Env     |  | Generic Env    |\n  | (Paper-exact)|  | (DSPy-aware) |  | (General use)  |\n  +--------------+  +--------------+  +----------------+\n          |                  |                  |\n          +------------------+------------------+\n                             |\n                    +--------v---------+\n                    |  Event Bus        |\n                    |  (27+ event types)|\n                    +------------------+\n</code></pre>"},{"location":"core/#subsystems","title":"Subsystems","text":"<p>The Core Engine is composed of several tightly integrated subsystems:</p> Subsystem Module Purpose Runner <code>rlm_code.rlm.runner</code> Multi-paradigm orchestrator with trajectory persistence Environments <code>rlm_code.rlm.environments</code>, <code>rlm_code.rlm.pure_rlm_environment</code> Execution environments with reward profiles Execution Patterns <code>rlm_code.rlm.runner</code>, <code>rlm_code.harness.runner</code> How to run pure recursive mode vs harness vs direct baseline Event System <code>rlm_code.rlm.events</code> Pub-sub event bus for observability and UI Termination <code>rlm_code.rlm.termination</code> FINAL/FINAL_VAR termination patterns Memory Compaction <code>rlm_code.rlm.memory_compaction</code> Context window management via summarization REPL Types <code>rlm_code.rlm.repl_types</code> Foundation types for context-as-variable paradigm Trajectory <code>rlm_code.rlm.trajectory</code> JSONL trajectory logging and visualization Paradigm Comparison <code>rlm_code.rlm.comparison</code> Side-by-side paradigm benchmarking"},{"location":"core/#key-concepts","title":"Key Concepts","text":""},{"location":"core/#context-as-variable","title":"Context-as-Variable","text":"<p>The central innovation of RLM: instead of injecting full context into the LLM prompt (consuming tokens), the context is stored as a Python variable in the REPL namespace. The LLM receives only lightweight metadata -- the variable name, type, character count, and a short preview -- and accesses the data through code.</p>"},{"location":"core/#reward-driven-optimization","title":"Reward-Driven Optimization","text":"<p>Every action produces a scalar reward in the range <code>[-1.0, 1.0]</code>. The <code>RLMRewardProfile</code> provides 25+ configurable knobs for tuning reward signals across different action types, including code execution success/failure, DSPy pattern matching, verifier suites, and warning penalties.</p>"},{"location":"core/#recursive-llm-calls","title":"Recursive LLM Calls","text":"<p>Within the REPL, the LLM can call <code>llm_query()</code> for single sub-LLM queries and <code>llm_query_batched()</code> for concurrent parallel queries. This enables recursive decomposition of complex tasks -- the hallmark of the RLM paradigm.</p>"},{"location":"core/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>The runner publishes 27+ event types through the <code>RLMEventBus</code>, enabling real-time UI updates, observability sinks, and execution tracing without coupling the core engine to any specific consumer.</p>"},{"location":"core/#quick-start","title":"Quick Start","text":"<pre><code>from rlm_code.rlm.runner import RLMRunner\n\nrunner = RLMRunner(\n    llm_connector=my_connector,\n    execution_engine=my_engine,\n    workdir=Path(\"/my/project\"),\n)\n\nresult = runner.run_task(\n    task=\"Analyze the codebase and find all TODO comments\",\n    environment=\"pure_rlm\",\n    max_steps=5,\n)\n\nprint(f\"Completed: {result.completed}\")\nprint(f\"Answer: {result.final_response}\")\nprint(f\"Steps: {result.steps}, Reward: {result.total_reward}\")\n</code></pre>"},{"location":"core/#module-map","title":"Module Map","text":"<pre><code>rlm_code/rlm/\n  __init__.py\n  runner.py              # RLMRunner orchestrator\n  environments.py        # RLMEnvironment protocol, Generic, DSPy environments\n  pure_rlm_environment.py # Paper-compliant Pure RLM environment\n  events.py              # Event bus and event types\n  termination.py         # FINAL/FINAL_VAR patterns\n  memory_compaction.py   # Context window management\n  repl_types.py          # REPLVariable, REPLHistory, REPLEntry, REPLResult\n  trajectory.py          # JSONL logging and visualization\n  comparison.py          # Paradigm comparison framework\n  benchmarks.py          # Benchmark case definitions\n  config_schema.py       # Configuration schema\n  observability.py       # Observability hooks\n  context_store.py       # Lazy file context loading\n  visualizer.py          # Run visualization\n</code></pre>"},{"location":"core/comparison/","title":"Paradigm Comparison","text":"<p>Module</p> <p><code>rlm_code.rlm.comparison</code></p> <p>The paradigm comparison module enables side-by-side empirical comparison of different RLM approaches on the same task. It directly addresses the debate around whether RLM provides real benefits over simpler approaches by measuring token usage, cost, execution time, and accuracy.</p> <p>For concept-level guidance on when to use each execution style, see Execution Patterns.</p>"},{"location":"core/comparison/#overview","title":"Overview","text":"<p>Three paradigms are compared:</p> Paradigm How Context is Handled Token Profile Pure RLM Context stored as REPL variable; LLM sees only metadata Low context tokens, moderate total tokens CodeAct Context included directly in the token window High context tokens, variable total tokens Traditional Context written to file, accessed via tools Medium context tokens (partial reads) <p>The comparison runs each paradigm on the same task and context, collecting detailed metrics for head-to-head analysis.</p>"},{"location":"core/comparison/#classes","title":"Classes","text":""},{"location":"core/comparison/#paradigm","title":"<code>Paradigm</code>","text":"<p>Enumeration of RLM paradigms for comparison.</p> <pre><code>class Paradigm(Enum):\n    PURE_RLM = \"pure_rlm\"\n    CODEACT = \"codeact\"\n    TRADITIONAL = \"traditional\"\n</code></pre> Value Description Environment Used <code>PURE_RLM</code> Paper-compliant context-as-variable <code>pure_rlm</code> <code>CODEACT</code> Context in token window <code>generic</code> <code>TRADITIONAL</code> Tool-based file access <code>dspy</code>"},{"location":"core/comparison/#paradigmresult","title":"<code>ParadigmResult</code>","text":"<p>Result from running a task under a specific paradigm.</p> <pre><code>@dataclass\nclass ParadigmResult:\n    paradigm: Paradigm\n    success: bool\n    answer: str\n\n    # Token metrics\n    context_tokens: int = 0\n    total_tokens: int = 0\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n\n    # Cost metrics\n    estimated_cost: float = 0.0\n\n    # Time metrics\n    duration_seconds: float = 0.0\n    iterations: int = 0\n\n    # Quality metrics (if ground truth available)\n    accuracy: float | None = None\n    f1_score: float | None = None\n\n    # LLM call breakdown\n    root_llm_calls: int = 0\n    sub_llm_calls: int = 0\n\n    # Event trace\n    events: list[dict[str, Any]] = field(default_factory=list)\n\n    # Error info\n    error: str | None = None\n</code></pre>"},{"location":"core/comparison/#field-reference","title":"Field Reference","text":"Field Type Description <code>paradigm</code> <code>Paradigm</code> Which paradigm was used <code>success</code> <code>bool</code> Whether the task completed successfully <code>answer</code> <code>str</code> The final answer produced <code>context_tokens</code> <code>int</code> Tokens consumed by context (metadata-only for Pure RLM, full for CodeAct) <code>total_tokens</code> <code>int</code> Total tokens used across all LLM calls <code>prompt_tokens</code> <code>int</code> Total prompt (input) tokens <code>completion_tokens</code> <code>int</code> Total completion (output) tokens <code>estimated_cost</code> <code>float</code> Estimated cost in USD <code>duration_seconds</code> <code>float</code> Wall-clock execution time <code>iterations</code> <code>int</code> Number of RLM iterations <code>accuracy</code> <code>float \\| None</code> Accuracy score (0.0-1.0) if ground truth available <code>f1_score</code> <code>float \\| None</code> F1 score if ground truth available <code>root_llm_calls</code> <code>int</code> Number of root-level LLM calls (one per iteration) <code>sub_llm_calls</code> <code>int</code> Number of sub-LLM calls via <code>llm_query()</code> <code>events</code> <code>list[dict]</code> Full event trace from <code>RLMEventCollector</code> <code>error</code> <code>str \\| None</code> Error message if the paradigm failed"},{"location":"core/comparison/#to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to dictionary (answer truncated to 500 characters).</p>"},{"location":"core/comparison/#comparisonresult","title":"<code>ComparisonResult</code>","text":"<p>Aggregated result of comparing multiple paradigms on the same task.</p> <pre><code>@dataclass\nclass ComparisonResult:\n    comparison_id: str\n    task: str\n    context_length: int\n\n    # Results by paradigm\n    results: dict[Paradigm, ParadigmResult] = field(default_factory=dict)\n\n    # Timing\n    started_at: str = field(...)\n    finished_at: str = \"\"\n    total_duration_seconds: float = 0.0\n\n    # Ground truth (if available)\n    ground_truth: str | None = None\n</code></pre>"},{"location":"core/comparison/#methods","title":"Methods","text":""},{"location":"core/comparison/#add_resultresult","title":"<code>add_result(result)</code>","text":"<p>Add a <code>ParadigmResult</code> to the comparison.</p>"},{"location":"core/comparison/#get_winnermetrictotal_tokens-paradigm-none","title":"<code>get_winner(metric=\"total_tokens\") -&gt; Paradigm | None</code>","text":"<p>Get the winning paradigm for a given metric.</p> <pre><code>winner = comparison.get_winner(\"total_tokens\")     # Lower is better\nwinner = comparison.get_winner(\"estimated_cost\")    # Lower is better\nwinner = comparison.get_winner(\"duration_seconds\")  # Lower is better\nwinner = comparison.get_winner(\"accuracy\")          # Higher is better\n</code></pre> <p>Winner Selection</p> <p>Only paradigms with <code>success=True</code> are considered. For <code>accuracy</code>, higher is better; for all other metrics, lower is better.</p>"},{"location":"core/comparison/#get_summary-dictstr-any","title":"<code>get_summary() -&gt; dict[str, Any]</code>","text":"<p>Get a structured comparison summary with metrics grouped by paradigm and winners identified.</p> <pre><code>summary = comparison.get_summary()\n# {\n#     \"comparison_id\": \"abc12345\",\n#     \"task\": \"Analyze sentiment...\",\n#     \"context_length\": 45230,\n#     \"paradigms_tested\": [\"pure_rlm\", \"codeact\", \"traditional\"],\n#     \"total_duration_seconds\": 45.2,\n#     \"total_tokens_by_paradigm\": {\"pure_rlm\": 5200, \"codeact\": 12400, \"traditional\": 8100},\n#     \"total_tokens_winner\": \"pure_rlm\",\n#     \"estimated_cost_by_paradigm\": {...},\n#     \"estimated_cost_winner\": \"pure_rlm\",\n#     ...\n# }\n</code></pre>"},{"location":"core/comparison/#format_table-str","title":"<code>format_table() -&gt; str</code>","text":"<p>Format the comparison as an ASCII table for terminal display.</p> <pre><code>print(comparison.format_table())\n</code></pre> <p>Example output:</p> <pre><code>======================================================================\nPARADIGM COMPARISON: Analyze sentiment of customer reviews\n======================================================================\n\nMetric          pure_rlm        codeact         traditional\n----------------------------------------------------------------------\nContext Tokens  200             11,308          5,654\nTotal Tokens    5,200           12,400          8,100\nEst. Cost       $0.0260         $0.0620         $0.0405\nDuration        12.30s          8.50s           15.20s\nIterations      3               2               4\nRoot LLM Calls  3               2               4\nSub LLM Calls   5               0               0\nAccuracy        85.0%           82.0%           78.0%\nSuccess         True            True            True\n----------------------------------------------------------------------\n\nWINNERS:\n  Lowest Tokens: pure_rlm\n  Lowest Cost: pure_rlm\n  Fastest: codeact\n\n======================================================================\n</code></pre>"},{"location":"core/comparison/#paradigmcomparator","title":"<code>ParadigmComparator</code>","text":"<p>The orchestrator that runs side-by-side paradigm comparisons.</p> <pre><code>class ParadigmComparator:\n    def __init__(\n        self,\n        runner: Any,           # RLMRunner instance\n        event_bus: RLMEventBus | None = None,\n    ):\n</code></pre> Parameter Type Default Description <code>runner</code> <code>RLMRunner</code> required The RLM runner to execute tasks <code>event_bus</code> <code>RLMEventBus \\| None</code> Auto-created Event bus for comparison events"},{"location":"core/comparison/#compare","title":"<code>compare()</code>","text":"<p>Run a comparison across paradigms.</p> <pre><code>def compare(\n    self,\n    task: str,\n    context: str,\n    paradigms: list[Paradigm] | None = None,\n    ground_truth: str | None = None,\n    max_steps: int = 5,\n    exec_timeout: int = 60,\n) -&gt; ComparisonResult:\n</code></pre> Parameter Type Default Description <code>task</code> <code>str</code> required The task to perform <code>context</code> <code>str</code> required The context to analyze <code>paradigms</code> <code>list[Paradigm] \\| None</code> All three Paradigms to test <code>ground_truth</code> <code>str \\| None</code> <code>None</code> Expected answer for accuracy calculation <code>max_steps</code> <code>int</code> <code>5</code> Maximum steps per paradigm <code>exec_timeout</code> <code>int</code> <code>60</code> Timeout per execution in seconds <p>Execution flow:</p> <ol> <li>Emit <code>COMPARISON_START</code> event</li> <li>For each paradigm:<ul> <li>Emit <code>COMPARISON_PARADIGM_START</code> event</li> <li>Subscribe an <code>RLMEventCollector</code> to capture events</li> <li>Run the task using the appropriate paradigm strategy</li> <li>Calculate accuracy against ground truth (if provided)</li> <li>Build <code>ParadigmResult</code> with all metrics</li> <li>Emit <code>COMPARISON_PARADIGM_END</code> event</li> </ul> </li> <li>Emit <code>COMPARISON_END</code> event with summary</li> <li>Return <code>ComparisonResult</code></li> </ol>"},{"location":"core/comparison/#paradigm-strategies","title":"Paradigm Strategies","text":"<p>Pure RLM (<code>_run_pure_rlm</code>):</p> <ul> <li>Initializes <code>PureRLMEnvironment</code> and loads context as variable</li> <li>Context tokens = ~200 (metadata only)</li> <li>Runs task in <code>pure_rlm</code> environment</li> </ul> <p>CodeAct (<code>_run_codeact</code>):</p> <ul> <li>Embeds the full context directly in the task prompt</li> <li>Context tokens = <code>len(context) / 4</code> (full context)</li> <li>Runs task in <code>generic</code> environment</li> </ul> <p>Traditional (<code>_run_traditional</code>):</p> <ul> <li>Writes context to a temporary file</li> <li>Task instructs LLM to use <code>read_file</code> and <code>search_code</code> tools</li> <li>Context tokens = estimated at half of full context (partial reads)</li> <li>Runs task in <code>dspy</code> environment</li> <li>Cleans up temporary file after completion</li> </ul>"},{"location":"core/comparison/#accuracy-calculation","title":"Accuracy Calculation","text":"<p>When <code>ground_truth</code> is provided, accuracy is calculated using Jaccard similarity:</p> <pre><code>answer_tokens = set(answer.lower().split())\ntruth_tokens = set(ground_truth.lower().split())\naccuracy = len(answer_tokens &amp; truth_tokens) / len(answer_tokens | truth_tokens)\n</code></pre>"},{"location":"core/comparison/#cost-estimation","title":"Cost Estimation","text":"<p>Costs are estimated based on token count and model pricing:</p> Model Cost per 1K tokens <code>gpt-4o</code> $0.005 <code>gpt-4</code> $0.030 <code>claude-3-opus</code> $0.015 <code>claude-3-sonnet</code> $0.003 Default $0.005"},{"location":"core/comparison/#create_comparison_report","title":"<code>create_comparison_report()</code>","text":"<p>Generate a detailed human-readable comparison report.</p> <pre><code>from rlm_code.rlm.comparison import create_comparison_report\n\nreport = create_comparison_report(comparison)\nprint(report)\n</code></pre> <p>The report includes:</p> <ol> <li>Header with comparison ID, task, context length, and duration</li> <li>Metrics table (same as <code>format_table()</code>)</li> <li>Analysis section with:<ul> <li>Token savings percentage between Pure RLM and CodeAct</li> <li>Context token reduction percentage</li> <li>Cost savings analysis</li> <li>Speed comparison</li> </ul> </li> <li>Verdict section with a conclusion about which paradigm is best for this scenario</li> </ol> <p>Example verdict:</p> <pre><code>VERDICT:\n----------------------------------------\nPure RLM wins on both tokens and cost, validating the paper's claims\nthat context-as-variable reduces token usage.\n</code></pre>"},{"location":"core/comparison/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code>from rlm_code.rlm.comparison import ParadigmComparator, Paradigm\nfrom rlm_code.rlm.runner import RLMRunner\n\n# Set up runner\nrunner = RLMRunner(\n    llm_connector=my_connector,\n    execution_engine=my_engine,\n    workdir=my_project_dir,\n)\n\n# Create comparator\ncomparator = ParadigmComparator(runner=runner)\n\n# Load a large context\nwith open(\"large_document.txt\") as f:\n    context = f.read()\n\n# Run comparison\nresult = comparator.compare(\n    task=\"Summarize the key findings in this document\",\n    context=context,\n    paradigms=[Paradigm.PURE_RLM, Paradigm.CODEACT],\n    ground_truth=\"The document discusses three main findings: ...\",\n    max_steps=5,\n    exec_timeout=120,\n)\n\n# Display results\nprint(result.format_table())\n\n# Get winner\ntoken_winner = result.get_winner(\"total_tokens\")\ncost_winner = result.get_winner(\"estimated_cost\")\nprint(f\"Token winner: {token_winner.value}\")\nprint(f\"Cost winner: {cost_winner.value}\")\n\n# Detailed analysis\nfrom rlm_code.rlm.comparison import create_comparison_report\nprint(create_comparison_report(result))\n\n# Access individual paradigm results\npure_rlm = result.results[Paradigm.PURE_RLM]\ncodeact = result.results[Paradigm.CODEACT]\nprint(f\"Pure RLM: {pure_rlm.total_tokens} tokens, ${pure_rlm.estimated_cost:.4f}\")\nprint(f\"CodeAct:  {codeact.total_tokens} tokens, ${codeact.estimated_cost:.4f}\")\nprint(f\"Token savings: {1 - pure_rlm.total_tokens / codeact.total_tokens:.1%}\")\n</code></pre>"},{"location":"core/comparison/#event-integration","title":"Event Integration","text":"<p>The comparator emits events at each stage through the event bus:</p> Event Payload <code>COMPARISON_START</code> <code>paradigms</code>, <code>task</code>, <code>context_length</code> <code>COMPARISON_PARADIGM_START</code> <code>paradigm</code> name <code>COMPARISON_PARADIGM_END</code> Full <code>ParadigmResult.to_dict()</code> <code>COMPARISON_END</code> <code>summary</code>, <code>duration_ms</code> <p>Subscribe to these events for real-time progress updates:</p> <pre><code>from rlm_code.rlm.events import RLMEventBus, RLMEventType\n\nbus = RLMEventBus()\n\ndef on_paradigm_end(event):\n    data = event.event_data.metadata\n    print(f\"  {data['paradigm']}: {data['total_tokens']} tokens, \"\n          f\"{'SUCCESS' if data['success'] else 'FAILED'}\")\n\nbus.subscribe_to_type(RLMEventType.COMPARISON_PARADIGM_END, on_paradigm_end)\n\ncomparator = ParadigmComparator(runner=runner, event_bus=bus)\nresult = comparator.compare(task=task, context=context)\n</code></pre>"},{"location":"core/environments/","title":"Environments","text":"<p>Modules</p> <p><code>rlm_code.rlm.environments</code> and <code>rlm_code.rlm.pure_rlm_environment</code></p> <p>Environments define how the RLM runner interacts with the LLM and executes actions. Each environment provides its own system prompt, planner prompt construction, action execution logic, reward computation, and health checks.</p>"},{"location":"core/environments/#environment-protocol","title":"Environment Protocol","text":"<p>All environments implement the <code>RLMEnvironment</code> protocol:</p> <pre><code>class RLMEnvironment(Protocol):\n    name: str\n\n    def system_prompt(self) -&gt; str: ...\n\n    def planner_prompt(\n        self,\n        task: str,\n        memory: list[str],\n        trajectory: list[dict[str, Any]],\n        step_index: int,\n    ) -&gt; str: ...\n\n    def execute_action(\n        self,\n        action: dict[str, Any],\n        execution_engine: Any,\n        exec_timeout: int,\n        llm_connector: Any | None = None,\n    ) -&gt; EnvironmentActionResult: ...\n\n    def doctor_checks(self) -&gt; list[EnvironmentDoctorCheck]: ...\n</code></pre>"},{"location":"core/environments/#purerlmenvironment","title":"PureRLMEnvironment","text":"<p>The paper-compliant RLM environment implementing exact semantics from \"Recursive Language Models\" (2025).</p> <pre><code>from rlm_code.rlm.pure_rlm_environment import PureRLMEnvironment, PureRLMConfig\n</code></pre>"},{"location":"core/environments/#key-innovations","title":"Key Innovations","text":"<ol> <li>Context stored as REPL variable -- not in the token window</li> <li>LLM receives only metadata -- variable name, type, length, preview</li> <li><code>llm_query()</code> -- enables recursive LLM calls from within code</li> <li><code>llm_query_batched()</code> -- concurrent parallel LLM queries</li> <li><code>SHOW_VARS()</code> -- namespace introspection</li> <li><code>FINAL()</code> / <code>FINAL_VAR()</code> -- clean termination patterns</li> </ol>"},{"location":"core/environments/#constructor","title":"Constructor","text":"<pre><code>class PureRLMEnvironment:\n    def __init__(\n        self,\n        workdir: Path | None = None,\n        reward_profile: RLMRewardProfile | dict[str, Any] | None = None,\n        config: PureRLMConfig | None = None,\n    ):\n</code></pre>"},{"location":"core/environments/#purerlmconfig","title":"PureRLMConfig","text":"<pre><code>@dataclass\nclass PureRLMConfig:\n    max_llm_calls: int = 50          # Maximum llm_query() calls per run\n    max_output_chars: int = 20000    # Maximum stdout characters returned\n    preview_length: int = 500        # Character preview length for variables\n    max_workers: int = 8             # Thread pool size for batched queries\n    sub_model: str | None = None     # Override model for sub-LLM calls\n    sub_provider: str | None = None  # Override provider for sub-LLM calls\n</code></pre> Parameter Default Description <code>max_llm_calls</code> <code>50</code> Hard limit on <code>llm_query()</code> + <code>llm_query_batched()</code> calls. Raises <code>RuntimeError</code> when exceeded. <code>max_output_chars</code> <code>20000</code> Truncation limit for stdout in observation <code>preview_length</code> <code>500</code> Number of characters included in variable preview metadata <code>max_workers</code> <code>8</code> <code>ThreadPoolExecutor</code> worker count for <code>llm_query_batched()</code> <code>sub_model</code> <code>None</code> Model name for sub-LLM calls (falls back to root model) <code>sub_provider</code> <code>None</code> Provider for sub-LLM calls"},{"location":"core/environments/#context-initialization","title":"Context Initialization","text":"<pre><code>env = PureRLMEnvironment(config=PureRLMConfig(preview_length=1000))\nenv.initialize_context(\n    context=large_document_text,\n    description=\"Legal contract to analyze\",\n    additional_vars={\"reference_data\": lookup_table},\n)\n</code></pre> <p>After initialization, the REPL namespace contains:</p> Name Type Description <code>context</code> Varies The primary input context <code>FINAL</code> Function Direct completion: <code>FINAL(answer)</code> <code>FINAL_VAR</code> Function Variable-based completion: <code>FINAL_VAR(\"result\")</code> <code>SHOW_VARS</code> Function List all user-defined variables <code>llm_query</code> Function Single recursive LLM call (set after <code>set_llm_connector()</code>) <code>llm_query_batched</code> Function Concurrent batch LLM calls (set after <code>set_llm_connector()</code>) <p>Plus all entries from the safe builtins whitelist.</p>"},{"location":"core/environments/#repl-functions","title":"REPL Functions","text":""},{"location":"core/environments/#llm_queryprompt-modelnone-str","title":"<code>llm_query(prompt, model=None) -&gt; str</code>","text":"<p>Query the LLM from within code execution. Each call consumes one unit from the <code>max_llm_calls</code> quota.</p> <pre><code># Inside REPL code\nsummary = llm_query(\"Summarize this section:\\n\" + context[0:5000])\nprint(summary)\n</code></pre> <p>Call Quota</p> <p>When the call count exceeds <code>max_llm_calls</code>, a <code>RuntimeError</code> is raised with the message: <code>\"Exceeded maximum LLM calls (50). Use llm_query_batched for efficiency.\"</code></p>"},{"location":"core/environments/#llm_query_batchedprompts-modelnone-liststr","title":"<code>llm_query_batched(prompts, model=None) -&gt; list[str]</code>","text":"<p>Concurrent batch LLM queries. Significantly faster than sequential <code>llm_query()</code> calls for multiple prompts. Consumes <code>len(prompts)</code> units from the quota.</p> <pre><code># Inside REPL code\nchunks = [context[i:i+10000] for i in range(0, len(context), 10000)]\nprompts = [f\"Summarize:\\n{chunk}\" for chunk in chunks]\nsummaries = llm_query_batched(prompts)\n</code></pre>"},{"location":"core/environments/#show_vars-str","title":"<code>SHOW_VARS() -&gt; str</code>","text":"<p>List all user-defined variables in the REPL namespace, filtering out builtins and internal functions.</p> <pre><code># Inside REPL code\nSHOW_VARS()\n# Output:\n# Available variables:\n#   context: str\n#   summaries: list\n#   result: dict\n</code></pre>"},{"location":"core/environments/#safe-builtins-whitelist","title":"Safe Builtins Whitelist","text":"<p>The REPL namespace includes a curated set of safe builtins (approximately 50+ functions):</p> Category Functions Core types <code>True</code>, <code>False</code>, <code>None</code> Type constructors <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>list</code>, <code>dict</code>, <code>tuple</code>, <code>set</code>, <code>frozenset</code>, <code>bytes</code>, <code>bytearray</code> Iterables <code>range</code>, <code>enumerate</code>, <code>zip</code>, <code>map</code>, <code>filter</code>, <code>sorted</code>, <code>reversed</code>, <code>iter</code>, <code>next</code> Math/comparison <code>len</code>, <code>min</code>, <code>max</code>, <code>sum</code>, <code>abs</code>, <code>round</code>, <code>pow</code>, <code>divmod</code> String/char <code>chr</code>, <code>ord</code>, <code>repr</code>, <code>ascii</code>, <code>format</code> Type checking <code>type</code>, <code>isinstance</code>, <code>issubclass</code>, <code>hasattr</code>, <code>getattr</code>, <code>setattr</code>, <code>delattr</code>, <code>callable</code> Collections <code>all</code>, <code>any</code>, <code>slice</code> IO <code>print</code>, <code>open</code> Import <code>__import__</code> (required for standard library access) Exceptions <code>Exception</code>, <code>ValueError</code>, <code>TypeError</code>, <code>KeyError</code>, <code>IndexError</code>, <code>AttributeError</code>, <code>RuntimeError</code>, <code>StopIteration</code> <p>Blocked Functions</p> <p>The following are intentionally excluded: <code>eval</code>, <code>exec</code>, <code>compile</code>, <code>input</code>, <code>globals</code>, <code>locals</code>.</p>"},{"location":"core/environments/#execution-flow","title":"Execution Flow","text":"<pre><code>initialize_context(data) --&gt; set_llm_connector(connector) --&gt; execute_action(action)\n                                                                    |\n                                                            +-------v-------+\n                                                            | Extract code  |\n                                                            | from action   |\n                                                            +-------+-------+\n                                                                    |\n                                                            +-------v-------+\n                                                            | exec(code,    |\n                                                            |   namespace)  |\n                                                            +-------+-------+\n                                                                    |\n                                                        +-----------+-----------+\n                                                        |                       |\n                                                 FinalOutput?              Normal result\n                                                        |                       |\n                                                 +------v------+        +-------v-------+\n                                                 | Resolve     |        | Compute reward|\n                                                 | FINAL/      |        | Update history|\n                                                 | FINAL_VAR   |        | Return result |\n                                                 +-------------+        +---------------+\n</code></pre>"},{"location":"core/environments/#reward-computation","title":"Reward Computation","text":"<p>The Pure RLM environment computes rewards as:</p> <pre><code>reward = run_python_base                      # 0.1\nif success:\n    reward += run_python_success_bonus        # +0.7\nelse:\n    reward -= run_python_failure_penalty      # -0.3\nif stderr:\n    reward -= run_python_stderr_penalty       # -0.1\nif llm_calls_made:\n    reward += 0.1 * min(len(llm_calls), 5)   # Bonus for using RLM paradigm\n</code></pre>"},{"location":"core/environments/#genericrlmenvironment","title":"GenericRLMEnvironment","text":"<p>A general-purpose environment supporting <code>run_python</code> and <code>final</code> actions.</p> <pre><code>from rlm_code.rlm.environments import GenericRLMEnvironment\n</code></pre>"},{"location":"core/environments/#constructor_1","title":"Constructor","text":"<pre><code>class GenericRLMEnvironment:\n    name = \"generic\"\n\n    def __init__(\n        self,\n        workdir: Path | None = None,\n        reward_profile: RLMRewardProfile | dict[str, Any] | None = None,\n    ):\n</code></pre>"},{"location":"core/environments/#supported-actions","title":"Supported Actions","text":"Action Description <code>run_python</code> Execute Python code in the sandbox <code>final</code> Signal task completion with <code>final_response</code>"},{"location":"core/environments/#system-prompt","title":"System Prompt","text":"<pre><code>You are an RLM planner.\nReturn ONLY valid JSON with keys: action, code, rationale, done, final_response.\nValid action values: \"run_python\", \"final\".\nNo markdown. JSON only.\n</code></pre>"},{"location":"core/environments/#doctor-checks","title":"Doctor Checks","text":"Check Description <code>workdir_exists</code> Working directory exists <code>workdir_writable</code> Write access to working directory <code>python_runtime</code> Python executable path"},{"location":"core/environments/#dspycodingrlmenvironment","title":"DSPyCodingRLMEnvironment","text":"<p>Extends <code>GenericRLMEnvironment</code> with DSPy-specific features, file operations, code search, test execution, and DSPy-aware scoring.</p> <pre><code>from rlm_code.rlm.environments import DSPyCodingRLMEnvironment\n</code></pre>"},{"location":"core/environments/#supported-actions_1","title":"Supported Actions","text":"Action Description <code>run_python</code> Execute Python code (with DSPy pattern bonus) <code>write_file</code> Write content to a file with verifier suite <code>patch_file</code> Search-and-replace or full content patch <code>read_file</code> Read file content with line range support <code>search_code</code> Regex-based code search across files <code>list_tree</code> Directory tree listing <code>run_tests</code> Execute pytest commands <code>analyze_code</code> / <code>analyze_dspy</code> Score code quality with DSPy heuristics <code>llm_query</code> Single sub-LLM query <code>llm_query_batched</code> Batch sub-LLM queries <code>delegate</code> / <code>delegate_batch</code> Recursive subtask delegation <code>final</code> Signal task completion"},{"location":"core/environments/#dspy-pattern-matching-bonus","title":"DSPy Pattern Matching Bonus","text":"<p>When <code>run_python</code> is executed, the code is scanned for DSPy patterns:</p> Pattern Regex Bonus DSPy import <code>\\bimport\\s+dspy\\b</code> <code>+0.03</code> Signature class <code>\\bdspy\\.Signature\\b</code> <code>+0.03</code> InputField <code>\\bdspy\\.InputField\\b</code> <code>+0.03</code> OutputField <code>\\bdspy\\.OutputField\\b</code> <code>+0.03</code> Module class <code>\\bdspy\\.Module\\b</code> <code>+0.03</code> <code>forward()</code> method <code>\\bdef\\s+forward\\s*\\(</code> <code>+0.03</code> <p>Total bonus is capped at <code>dspy_pattern_bonus_cap</code> (default: <code>0.2</code>).</p>"},{"location":"core/environments/#dspy-source-scoring","title":"DSPy Source Scoring","text":"<p>Files written with <code>write_file</code> or <code>patch_file</code> are scored on a 0-100 scale:</p> Pattern Score Contribution Base score <code>35.0</code> <code>import dspy</code> <code>+10.0</code> <code>class X(dspy.Signature):</code> <code>+15.0</code> <code>dspy.InputField(</code> <code>+10.0</code> <code>dspy.OutputField(</code> <code>+10.0</code> <code>class X(dspy.Module):</code> <code>+10.0</code> <code>def forward(</code> <code>+10.0</code> <code>dspy.settings.configure</code> <code>-8.0</code> (deprecated pattern) <code>dspy.OpenAI(</code> / <code>dspy.Anthropic(</code> <code>-8.0</code> (hardcoded provider) <code>TODO</code> <code>-4.0</code>"},{"location":"core/environments/#verifier-suite","title":"Verifier Suite","text":"<p>After <code>write_file</code> and <code>patch_file</code>, a three-stage verifier runs:</p> <ol> <li>Compile check -- <code>python -m compileall</code> for syntax validation</li> <li>Targeted pytest -- runs matching <code>test_&lt;stem&gt;.py</code> if it exists</li> <li>Code validation -- DSPy-oriented validator from the execution engine</li> </ol>"},{"location":"core/environments/#verifier-reward-calculation","title":"Verifier Reward Calculation","text":"<pre><code>reward = verifier_base + (dspy_score / 100.0) * verifier_score_weight\nif compile_ok:    reward += verifier_compile_bonus\nelse:             reward -= verifier_compile_penalty\nif pytest_ran:\n    if pytest_ok: reward += verifier_pytest_bonus\n    else:         reward -= verifier_pytest_penalty\nif validation_ok: reward += verifier_validation_bonus\nelse:             reward -= verifier_validation_penalty\nreward -= min(warning_penalty_cap, num_warnings * warning_penalty_per_warning)\n</code></pre>"},{"location":"core/environments/#doctor-checks-dspy-specific","title":"Doctor Checks (DSPy-specific)","text":"<p>All generic checks plus:</p> Check Description <code>pytest_cli</code> pytest available on PATH <code>test_discovery</code> Test files found in <code>tests/</code> <code>dspy_import</code> <code>import dspy</code> succeeds"},{"location":"core/environments/#common-data-classes","title":"Common Data Classes","text":""},{"location":"core/environments/#environmentactionresult","title":"<code>EnvironmentActionResult</code>","text":"<p>Returned by every <code>execute_action()</code> call.</p> <pre><code>@dataclass(slots=True)\nclass EnvironmentActionResult:\n    observation: dict[str, Any]       # Execution output visible to the LLM\n    reward: float                     # Scalar reward in [-1.0, 1.0]\n    done: bool = False                # True if task is complete\n    final_response: str | None = None # Final answer (when done=True)\n    memory_note: str | None = None    # Short note for rolling memory\n</code></pre>"},{"location":"core/environments/#environmentdoctorcheck","title":"<code>EnvironmentDoctorCheck</code>","text":"<p>Health check result from <code>doctor_checks()</code>.</p> <pre><code>@dataclass(slots=True)\nclass EnvironmentDoctorCheck:\n    name: str                          # Check identifier\n    status: str                        # \"pass\" | \"warn\" | \"fail\"\n    detail: str                        # Human-readable description\n    recommendation: str | None = None  # Fix suggestion (if not passing)\n</code></pre>"},{"location":"core/environments/#rlmrewardprofile","title":"RLMRewardProfile","text":"<p>The reward tuning profile with 25+ configurable knobs.</p> <pre><code>@dataclass(slots=True)\nclass RLMRewardProfile:\n    # Global multiplier\n    global_scale: float = 1.0\n\n    # run_python scoring\n    run_python_base: float = 0.1\n    run_python_success_bonus: float = 0.7\n    run_python_failure_penalty: float = 0.3\n    run_python_stderr_penalty: float = 0.1\n\n    # DSPy heuristic adjustments\n    dspy_pattern_match_bonus: float = 0.03\n    dspy_pattern_bonus_cap: float = 0.2\n\n    # Verifier scoring\n    verifier_base: float = 0.15\n    verifier_score_weight: float = 0.5\n    verifier_compile_bonus: float = 0.2\n    verifier_compile_penalty: float = 0.35\n    verifier_pytest_bonus: float = 0.25\n    verifier_pytest_penalty: float = 0.25\n    verifier_validation_bonus: float = 0.15\n    verifier_validation_penalty: float = 0.3\n    verifier_warning_penalty_per_warning: float = 0.03\n    verifier_warning_penalty_cap: float = 0.15\n</code></pre>"},{"location":"core/environments/#reward-knob-reference","title":"Reward Knob Reference","text":"Knob Default Category Description <code>global_scale</code> <code>1.0</code> Global Multiplier applied to all rewards after environment calculation <code>run_python_base</code> <code>0.1</code> Execution Base reward for any code execution <code>run_python_success_bonus</code> <code>0.7</code> Execution Added on successful execution <code>run_python_failure_penalty</code> <code>0.3</code> Execution Subtracted on execution failure <code>run_python_stderr_penalty</code> <code>0.1</code> Execution Subtracted when stderr is non-empty <code>dspy_pattern_match_bonus</code> <code>0.03</code> DSPy Per-pattern bonus for DSPy idioms <code>dspy_pattern_bonus_cap</code> <code>0.2</code> DSPy Maximum total DSPy pattern bonus <code>verifier_base</code> <code>0.15</code> Verifier Base reward for file write/patch <code>verifier_score_weight</code> <code>0.5</code> Verifier Weight of DSPy source score (0-100 mapped to 0-0.5) <code>verifier_compile_bonus</code> <code>0.2</code> Verifier Bonus for passing compile check <code>verifier_compile_penalty</code> <code>0.35</code> Verifier Penalty for failing compile check <code>verifier_pytest_bonus</code> <code>0.25</code> Verifier Bonus for passing targeted tests <code>verifier_pytest_penalty</code> <code>0.25</code> Verifier Penalty for failing targeted tests <code>verifier_validation_bonus</code> <code>0.15</code> Verifier Bonus for passing code validation <code>verifier_validation_penalty</code> <code>0.3</code> Verifier Penalty for failing code validation <code>verifier_warning_penalty_per_warning</code> <code>0.03</code> Verifier Per-warning penalty <code>verifier_warning_penalty_cap</code> <code>0.15</code> Verifier Maximum total warning penalty"},{"location":"core/environments/#methods","title":"Methods","text":"Method Description <code>from_mapping(payload)</code> Class method to build profile from dict with safe fallbacks <code>clamp(value)</code> Static method to clamp reward to <code>[-1.0, 1.0]</code> <code>apply_global_scale(value)</code> Apply global scaling and clamp"},{"location":"core/environments/#custom-profile-example","title":"Custom Profile Example","text":"<pre><code>from rlm_code.rlm.environments import RLMRewardProfile\n\n# Aggressive profile favoring correctness over speed\nprofile = RLMRewardProfile(\n    global_scale=1.2,\n    run_python_success_bonus=0.9,\n    run_python_failure_penalty=0.5,\n    verifier_compile_penalty=0.5,\n    verifier_pytest_bonus=0.4,\n)\n\n# Or from a dictionary (e.g., loaded from config)\nprofile = RLMRewardProfile.from_mapping({\n    \"global_scale\": 1.2,\n    \"run_python_success_bonus\": 0.9,\n})\n</code></pre>"},{"location":"core/events/","title":"Event System","text":"<p>Module</p> <p><code>rlm_code.rlm.events</code></p> <p>The event system provides a fine-grained, in-process pub-sub bus for real-time observability and UI updates during RLM execution. Inspired by Google ADK's event streaming architecture, it enables decoupled consumers to react to every phase of the RLM lifecycle without modifying the core engine.</p>"},{"location":"core/events/#classes","title":"Classes","text":""},{"location":"core/events/#rlmeventtype","title":"<code>RLMEventType</code>","text":"<p>An enumeration of 31 discrete event types, organized by category.</p> <pre><code>class RLMEventType(Enum):\n    # Run lifecycle\n    RUN_START = \"run_start\"\n    RUN_END = \"run_end\"\n    RUN_ERROR = \"run_error\"\n\n    # Iteration events\n    ITERATION_START = \"iteration_start\"\n    ITERATION_END = \"iteration_end\"\n\n    # LLM interaction\n    LLM_CALL_START = \"llm_call_start\"\n    LLM_CALL_END = \"llm_call_end\"\n    LLM_RESPONSE = \"llm_response\"\n\n    # Code execution\n    CODE_FOUND = \"code_found\"\n    CODE_EXEC_START = \"code_exec_start\"\n    CODE_EXEC_END = \"code_exec_end\"\n    CODE_OUTPUT = \"code_output\"\n\n    # Sub-LLM calls (llm_query in REPL)\n    SUB_LLM_START = \"sub_llm_start\"\n    SUB_LLM_END = \"sub_llm_end\"\n    SUB_LLM_BATCH_START = \"sub_llm_batch_start\"\n    SUB_LLM_BATCH_END = \"sub_llm_batch_end\"\n\n    # Recursive/child agent events\n    CHILD_SPAWN = \"child_spawn\"\n    CHILD_START = \"child_start\"\n    CHILD_END = \"child_end\"\n    CHILD_ERROR = \"child_error\"\n\n    # Results and termination\n    FINAL_DETECTED = \"final_detected\"\n    FINAL_ANSWER = \"final_answer\"\n\n    # Memory management\n    MEMORY_COMPACT_START = \"memory_compact_start\"\n    MEMORY_COMPACT_END = \"memory_compact_end\"\n\n    # Context events\n    CONTEXT_LOAD = \"context_load\"\n    CONTEXT_CHUNK = \"context_chunk\"\n\n    # Comparison mode\n    COMPARISON_START = \"comparison_start\"\n    COMPARISON_PARADIGM_START = \"comparison_paradigm_start\"\n    COMPARISON_PARADIGM_END = \"comparison_paradigm_end\"\n    COMPARISON_END = \"comparison_end\"\n\n    # Benchmark events\n    BENCHMARK_START = \"benchmark_start\"\n    BENCHMARK_CASE_START = \"benchmark_case_start\"\n    BENCHMARK_CASE_END = \"benchmark_case_end\"\n    BENCHMARK_END = \"benchmark_end\"\n</code></pre>"},{"location":"core/events/#event-type-groups","title":"Event Type Groups","text":""},{"location":"core/events/#run-lifecycle","title":"Run Lifecycle","text":"Event When Emitted Typical Payload <code>RUN_START</code> Beginning of <code>run_task()</code> <code>run_id</code>, <code>task</code>, <code>environment</code>, <code>depth</code> <code>RUN_END</code> End of <code>run_task()</code> <code>run_id</code>, <code>completed</code>, <code>steps</code>, <code>total_reward</code> <code>RUN_ERROR</code> Unrecoverable error during run <code>run_id</code>, <code>error</code>"},{"location":"core/events/#iteration-events","title":"Iteration Events","text":"Event When Emitted Typical Payload <code>ITERATION_START</code> Before planner prompt generation <code>run_id</code>, <code>iteration</code> <code>ITERATION_END</code> After action execution and reward <code>run_id</code>, <code>iteration</code>, <code>reward</code>"},{"location":"core/events/#llm-calls","title":"LLM Calls","text":"Event When Emitted Typical Payload <code>LLM_CALL_START</code> Before root LLM call <code>run_id</code>, <code>iteration</code>, <code>prompt_length</code> <code>LLM_CALL_END</code> After root LLM response <code>run_id</code>, <code>tokens_used</code>, <code>duration_ms</code> <code>LLM_RESPONSE</code> When LLM response text is available <code>run_id</code>, <code>response_length</code>"},{"location":"core/events/#code-execution","title":"Code Execution","text":"Event When Emitted Typical Payload <code>CODE_FOUND</code> Code block extracted from LLM response <code>code</code> <code>CODE_EXEC_START</code> Before sandbox execution <code>code</code>, <code>timeout</code> <code>CODE_EXEC_END</code> After sandbox execution <code>success</code>, <code>execution_time</code> <code>CODE_OUTPUT</code> When stdout/stderr is available <code>output</code>, <code>stderr</code>"},{"location":"core/events/#sub-llm-calls","title":"Sub-LLM Calls","text":"Event When Emitted Typical Payload <code>SUB_LLM_START</code> Before <code>llm_query()</code> call from code <code>prompt</code>, <code>agent_depth</code> <code>SUB_LLM_END</code> After <code>llm_query()</code> response <code>response</code>, <code>duration_ms</code> <code>SUB_LLM_BATCH_START</code> Before <code>llm_query_batched()</code> <code>batch_size</code>, <code>batch_id</code> <code>SUB_LLM_BATCH_END</code> After all batch queries complete <code>batch_id</code>, <code>results_count</code>"},{"location":"core/events/#child-agent-events","title":"Child Agent Events","text":"Event When Emitted Typical Payload <code>CHILD_SPAWN</code> When delegate action creates child task <code>child_id</code>, <code>task</code>, <code>depth</code> <code>CHILD_START</code> When child begins execution <code>child_id</code>, <code>parent_agent</code> <code>CHILD_END</code> When child completes <code>child_id</code>, <code>success</code>, <code>reward</code> <code>CHILD_ERROR</code> When child fails with exception <code>child_id</code>, <code>error</code>"},{"location":"core/events/#results-and-termination","title":"Results and Termination","text":"Event When Emitted Typical Payload <code>FINAL_DETECTED</code> When FINAL/FINAL_VAR pattern found <code>final_type</code>, <code>content</code> <code>FINAL_ANSWER</code> When final answer is resolved <code>answer</code>"},{"location":"core/events/#memory-management","title":"Memory Management","text":"Event When Emitted Typical Payload <code>MEMORY_COMPACT_START</code> Before memory compaction <code>entry_count</code>, <code>total_chars</code> <code>MEMORY_COMPACT_END</code> After memory compaction <code>compression_ratio</code>, <code>used_llm</code>"},{"location":"core/events/#context-events","title":"Context Events","text":"Event When Emitted Typical Payload <code>CONTEXT_LOAD</code> When context is loaded into environment <code>context_type</code>, <code>length</code> <code>CONTEXT_CHUNK</code> When context is chunked for processing <code>chunk_index</code>, <code>chunk_size</code>"},{"location":"core/events/#comparison-events","title":"Comparison Events","text":"Event When Emitted Typical Payload <code>COMPARISON_START</code> Beginning of paradigm comparison <code>paradigms</code>, <code>task</code> <code>COMPARISON_PARADIGM_START</code> Before testing one paradigm <code>paradigm</code> <code>COMPARISON_PARADIGM_END</code> After testing one paradigm <code>paradigm</code>, <code>metrics</code> <code>COMPARISON_END</code> After all paradigms tested <code>summary</code>, <code>duration_ms</code>"},{"location":"core/events/#benchmark-events","title":"Benchmark Events","text":"Event When Emitted Typical Payload <code>BENCHMARK_START</code> Beginning of benchmark sweep <code>preset</code>, <code>case_count</code> <code>BENCHMARK_CASE_START</code> Before running one benchmark case <code>case_id</code>, <code>task</code> <code>BENCHMARK_CASE_END</code> After running one benchmark case <code>case_id</code>, <code>completed</code>, <code>reward</code> <code>BENCHMARK_END</code> After all cases complete <code>avg_reward</code>, <code>completion_rate</code>"},{"location":"core/events/#rlmeventdata","title":"<code>RLMEventData</code>","text":"<p>Structured event data with ancestry tracking for recursive calls.</p> <pre><code>@dataclass(slots=True)\nclass RLMEventData:\n    # Core identification\n    event_type: RLMEventType\n    run_id: str = \"\"\n    iteration: int = 0\n\n    # Ancestry for recursive calls\n    agent_name: str = \"\"\n    agent_depth: int = 0\n    parent_agent: str | None = None\n    ancestry: list[dict[str, Any]] = field(default_factory=list)\n\n    # Batch tracking\n    batch_id: str | None = None\n    batch_index: int | None = None\n    batch_size: int | None = None\n\n    # Timing\n    start_time: str | None = None\n    end_time: str | None = None\n    duration_ms: float | None = None\n\n    # Content\n    message: str = \"\"\n    code: str | None = None\n    output: str | None = None\n    error: str | None = None\n\n    # Metrics\n    tokens_used: int | None = None\n    cost: float | None = None\n\n    # Additional payload\n    metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre> Field Type Description <code>event_type</code> <code>RLMEventType</code> The event category <code>run_id</code> <code>str</code> Run identifier for correlation <code>iteration</code> <code>int</code> Current iteration number <code>agent_name</code> <code>str</code> Name of the agent emitting the event <code>agent_depth</code> <code>int</code> Recursion depth (0 = root) <code>parent_agent</code> <code>str \\| None</code> Parent agent name for child events <code>ancestry</code> <code>list[dict]</code> Full ancestry chain for deep recursion <code>batch_id</code> <code>str \\| None</code> Batch operation identifier <code>batch_index</code> <code>int \\| None</code> Position within batch <code>batch_size</code> <code>int \\| None</code> Total batch size <code>start_time</code> <code>str \\| None</code> ISO timestamp of event start <code>end_time</code> <code>str \\| None</code> ISO timestamp of event end <code>duration_ms</code> <code>float \\| None</code> Duration in milliseconds <code>message</code> <code>str</code> Human-readable event description <code>code</code> <code>str \\| None</code> Code being executed <code>output</code> <code>str \\| None</code> Execution output <code>error</code> <code>str \\| None</code> Error message <code>tokens_used</code> <code>int \\| None</code> Token count for LLM events <code>cost</code> <code>float \\| None</code> Estimated cost <code>metadata</code> <code>dict[str, Any]</code> Arbitrary additional data"},{"location":"core/events/#serialization","title":"Serialization","text":"<pre><code>event_data = RLMEventData(\n    event_type=RLMEventType.CODE_EXEC_END,\n    run_id=\"run_abc123\",\n    iteration=3,\n    duration_ms=245.7,\n    message=\"Code execution completed\",\n)\n\nd = event_data.to_dict()\n# {\n#     \"event_type\": \"code_exec_end\",\n#     \"run_id\": \"run_abc123\",\n#     \"iteration\": 3,\n#     \"agent_name\": \"\",\n#     \"agent_depth\": 0,\n#     \"message\": \"Code execution completed\",\n#     \"duration_ms\": 245.7,\n# }\n</code></pre>"},{"location":"core/events/#rlmruntimeevent","title":"<code>RLMRuntimeEvent</code>","text":"<p>The envelope type for all events passing through the bus.</p> <pre><code>@dataclass(slots=True)\nclass RLMRuntimeEvent:\n    name: str                             # Event name (string identifier)\n    timestamp: str                        # ISO 8601 UTC timestamp\n    payload: dict[str, Any]               # Arbitrary payload dictionary\n    event_type: RLMEventType | None = None  # Typed event category\n    event_data: RLMEventData | None = None  # Structured event data\n</code></pre> Field Type Description <code>name</code> <code>str</code> Event name (often the <code>event_type.value</code>) <code>timestamp</code> <code>str</code> ISO 8601 UTC timestamp of emission <code>payload</code> <code>dict[str, Any]</code> Backward-compatible payload dictionary <code>event_type</code> <code>RLMEventType \\| None</code> Typed event category (for typed events) <code>event_data</code> <code>RLMEventData \\| None</code> Structured data (for typed events) <p>The <code>to_dict()</code> method produces a serializable dictionary.</p>"},{"location":"core/events/#rlmeventbus","title":"<code>RLMEventBus</code>","text":"<p>In-process pub-sub bus supporting both simple and typed events. Thread-safe via <code>RLock</code>.</p> <pre><code>class RLMEventBus:\n    def __init__(self): ...\n</code></pre>"},{"location":"core/events/#methods","title":"Methods","text":"Method Signature Description <code>subscribe</code> <code>(callback: Callable[[RLMRuntimeEvent], None]) -&gt; None</code> Subscribe to all events <code>subscribe_to_type</code> <code>(event_type: RLMEventType, callback: ...) -&gt; None</code> Subscribe to a specific event type only <code>unsubscribe</code> <code>(callback: ...) -&gt; None</code> Remove a subscriber from all subscriptions <code>emit</code> <code>(name: str, payload: dict \\| None = None) -&gt; None</code> Emit a simple event (backward compatible) <code>emit_typed</code> <code>(event_type: RLMEventType, event_data: RLMEventData \\| None = None, **kwargs) -&gt; RLMRuntimeEvent</code> Emit a typed event with structured data <p>Error Isolation</p> <p>Subscriber exceptions are silently caught in <code>_dispatch()</code> to prevent a faulty subscriber from breaking the event bus or the runner.</p>"},{"location":"core/events/#usage-examples","title":"Usage Examples","text":"<p>Subscribe to all events:</p> <pre><code>from rlm_code.rlm.events import RLMEventBus, RLMRuntimeEvent\n\nbus = RLMEventBus()\n\ndef on_event(event: RLMRuntimeEvent):\n    print(f\"[{event.timestamp}] {event.name}: {event.payload}\")\n\nbus.subscribe(on_event)\n</code></pre> <p>Subscribe to specific event type:</p> <pre><code>from rlm_code.rlm.events import RLMEventBus, RLMEventType\n\nbus = RLMEventBus()\n\ndef on_code_output(event):\n    print(f\"Code output: {event.payload.get('output', '')[:100]}\")\n\nbus.subscribe_to_type(RLMEventType.CODE_OUTPUT, on_code_output)\n</code></pre> <p>Emit a simple event:</p> <pre><code>bus.emit(\"custom_event\", {\"key\": \"value\", \"count\": 42})\n</code></pre> <p>Emit a typed event:</p> <pre><code>from rlm_code.rlm.events import RLMEventType, RLMEventData\n\nevent = bus.emit_typed(\n    RLMEventType.CODE_EXEC_END,\n    RLMEventData(\n        event_type=RLMEventType.CODE_EXEC_END,\n        run_id=\"run_abc\",\n        iteration=2,\n        duration_ms=150.3,\n        message=\"Execution complete\",\n        metadata={\"success\": True},\n    ),\n)\n</code></pre> <p>Unsubscribe:</p> <pre><code>bus.unsubscribe(on_event)\n</code></pre>"},{"location":"core/events/#rlmeventcollector","title":"<code>RLMEventCollector</code>","text":"<p>Collects events for later analysis or comparison. Thread-safe.</p> <pre><code>class RLMEventCollector:\n    def __init__(self): ...\n</code></pre>"},{"location":"core/events/#methods_1","title":"Methods","text":"Method Signature Description <code>collect</code> <code>(event: RLMRuntimeEvent) -&gt; None</code> Add an event to the collection <code>get_events</code> <code>() -&gt; list[RLMRuntimeEvent]</code> Get all collected events (copy) <code>get_events_by_type</code> <code>(event_type: RLMEventType) -&gt; list[RLMRuntimeEvent]</code> Filter events by type <code>clear</code> <code>() -&gt; None</code> Clear all collected events <code>get_summary</code> <code>() -&gt; dict[str, Any]</code> Get summary statistics"},{"location":"core/events/#usage-example","title":"Usage Example","text":"<pre><code>from rlm_code.rlm.events import RLMEventBus, RLMEventCollector, RLMEventType\n\nbus = RLMEventBus()\ncollector = RLMEventCollector()\n\n# Subscribe the collector to all events\nbus.subscribe(collector.collect)\n\n# ... run some RLM tasks ...\n\n# Analyze collected events\nsummary = collector.get_summary()\nprint(f\"Total events: {summary['total_events']}\")\nprint(f\"Event types: {summary['event_types']}\")\nprint(f\"Total duration: {summary['total_duration_ms']}ms\")\nprint(f\"Total tokens: {summary['total_tokens']}\")\n\n# Get specific event types\ncode_events = collector.get_events_by_type(RLMEventType.CODE_EXEC_END)\nfor event in code_events:\n    print(f\"  Code executed in {event.event_data.duration_ms}ms\")\n\n# Clean up\nbus.unsubscribe(collector.collect)\ncollector.clear()\n</code></pre>"},{"location":"core/events/#summary-output","title":"Summary Output","text":"<p>The <code>get_summary()</code> method returns:</p> <pre><code>{\n    \"total_events\": 47,\n    \"event_types\": {\n        \"run_start\": 1,\n        \"iteration_start\": 4,\n        \"llm_call_start\": 4,\n        \"llm_call_end\": 4,\n        \"code_exec_start\": 3,\n        \"code_exec_end\": 3,\n        \"code_output\": 3,\n        \"final_detected\": 1,\n        \"run_end\": 1,\n        # ...\n    },\n    \"total_duration_ms\": 12450.0,\n    \"total_tokens\": 8523,\n}\n</code></pre>"},{"location":"core/events/#dispatch-architecture","title":"Dispatch Architecture","text":"<p>The event bus uses a two-tier dispatch model:</p> <ol> <li>Global subscribers -- receive every event</li> <li>Type-specific subscribers -- receive only events matching their subscribed <code>RLMEventType</code></li> </ol> <p>Both tiers are dispatched for typed events. Simple events (emitted via <code>emit()</code>) only reach global subscribers.</p> <pre><code>emit_typed(CODE_EXEC_END, data)\n    |\n    +---&gt; Global subscribers (all events)\n    |\n    +---&gt; Type subscribers for CODE_EXEC_END only\n</code></pre> <p>Dispatch occurs outside the lock to prevent deadlocks when subscribers emit events of their own.</p>"},{"location":"core/execution-patterns/","title":"Execution Patterns","text":"<p>This page describes the three execution patterns available in RLM Code and how to use each one intentionally.</p> <p>It focuses on behavior and configuration, without opinionated claims.</p>"},{"location":"core/execution-patterns/#why-this-matters","title":"Why This Matters","text":"<p>RLM Code can operate in multiple modes:</p> <ol> <li>Recursive symbolic context processing (pure RLM native path)</li> <li>Tool-delegation coding loop (harness path)</li> <li>Direct model response (single-call baseline)</li> </ol> <p>These modes solve different problems. Comparing them is most useful when you run each one deliberately.</p>"},{"location":"core/execution-patterns/#pattern-1-recursive-symbolic-context-processing","title":"Pattern 1: Recursive Symbolic Context Processing","text":"<p>In this pattern, context is loaded into the REPL as variables and the model works by writing code that:</p> <ul> <li>Inspects variables programmatically</li> <li>Calls <code>llm_query()</code> or <code>llm_query_batched()</code> from inside code</li> <li>Composes intermediate results in variables</li> <li>Terminates with <code>FINAL(...)</code> or <code>FINAL_VAR(...)</code></li> </ul>"},{"location":"core/execution-patterns/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>Large-context analysis</li> <li>Programmatic decomposition/map-reduce style reasoning</li> <li>Experiments where token efficiency and context handling strategy are primary variables</li> </ul>"},{"location":"core/execution-patterns/#recommended-settings","title":"Recommended Settings","text":"<pre><code>/sandbox profile secure\n/sandbox backend docker\n/sandbox strict on\n/sandbox output-mode metadata\n</code></pre> <p>Then run:</p> <pre><code>/rlm run \"Analyze this context with programmatic decomposition\" env=pure_rlm framework=native\n</code></pre> <p>Notes:</p> <ul> <li><code>strict on</code> disables runner-level <code>delegate</code> actions in pure mode, so recursion stays inside REPL code.</li> <li><code>output-mode metadata</code> keeps per-step output compact and stable for long runs.</li> </ul>"},{"location":"core/execution-patterns/#pattern-2-tool-delegation-coding-loop-harness","title":"Pattern 2: Tool-Delegation Coding Loop (Harness)","text":"<p>In this pattern, the model chooses tools (<code>read</code>, <code>grep</code>, <code>edit</code>, <code>bash</code>, MCP tools, etc.) step by step.</p>"},{"location":"core/execution-patterns/#typical-use-cases_1","title":"Typical Use Cases","text":"<ul> <li>Repository editing and test-fix loops</li> <li>Local/BYOK coding assistant workflows</li> <li>MCP-augmented automation</li> </ul>"},{"location":"core/execution-patterns/#commands","title":"Commands","text":"<pre><code>/harness tools\n/harness doctor\n/harness run \"fix failing tests and explain changes\" steps=8 mcp=on\n</code></pre> <p>If a connected model is in local/BYOK mode, TUI chat can auto-route coding prompts to harness.</p> <p>To disable auto-route for controlled experiments:</p> <pre><code>export RLM_TUI_HARNESS_AUTO=0\nrlm-code\n</code></pre>"},{"location":"core/execution-patterns/#pattern-3-direct-model-baseline","title":"Pattern 3: Direct Model Baseline","text":"<p>This is a simple one-shot baseline without recursive REPL execution or tool loop orchestration.</p> <p>Use it for sanity checks and benchmark comparison baselines.</p>"},{"location":"core/execution-patterns/#controlled-comparison-workflow","title":"Controlled Comparison Workflow","text":"<p>Run the same benchmark suite with each mode:</p> <pre><code>/rlm bench preset=paradigm_comparison mode=native\n/rlm bench preset=paradigm_comparison mode=harness\n/rlm bench preset=paradigm_comparison mode=direct-llm\n</code></pre> <p>Then compare:</p> <pre><code>/rlm bench compare candidate=latest baseline=previous\n/rlm bench report candidate=latest baseline=previous format=markdown\n</code></pre>"},{"location":"core/execution-patterns/#mode-selection-checklist","title":"Mode Selection Checklist","text":"<p>Use recursive symbolic context processing when:</p> <ul> <li>You need code-driven context understanding over large or structured inputs.</li> <li>You want recursion written inside code (<code>llm_query</code> in loops/functions).</li> <li>You want strict experimental control over context exposure.</li> </ul> <p>Use harness when:</p> <ul> <li>Your primary goal is practical coding velocity in a repository.</li> <li>You want tool-first workflows (file ops, shell, MCP tools).</li> </ul> <p>Use direct-llm when:</p> <ul> <li>You need a minimal baseline for comparison.</li> </ul>"},{"location":"core/execution-patterns/#common-questions","title":"Common Questions","text":""},{"location":"core/execution-patterns/#is-this-just-another-coding-agent","title":"\"Is this just another coding agent?\"","text":"<p>RLM Code includes both:</p> <ul> <li>A recursive symbolic mode (<code>/rlm ... env=pure_rlm framework=native</code>)</li> <li>A tool-delegation harness mode (<code>/harness ...</code>, or benchmark <code>mode=harness</code>)</li> </ul> <p>Because both exist in one product, comparisons should be done with explicit mode selection.</p>"},{"location":"core/execution-patterns/#if-context-is-hidden-how-does-the-model-know-what-to-do","title":"\"If context is hidden, how does the model know what to do?\"","text":"<p>The model sees metadata (type/length/preview) and can inspect data via code in REPL, then query sub-models with <code>llm_query()</code> / <code>llm_query_batched()</code>.</p>"},{"location":"core/execution-patterns/#how-does-the-run-know-when-it-is-done","title":"\"How does the run know when it is done?\"","text":"<p>Pure recursive runs terminate through <code>FINAL(...)</code> or <code>FINAL_VAR(...)</code> semantics in REPL flow. Runner-level completion can also occur from explicit final actions depending on mode.</p>"},{"location":"core/execution-patterns/#will-recursive-sub-calls-increase-cost","title":"\"Will recursive sub-calls increase cost?\"","text":"<p>Potentially yes. Recursive strategies can reduce prompt bloat but may increase total call count. This is why RLM Code provides side-by-side benchmark modes (<code>native</code>, <code>harness</code>, <code>direct-llm</code>) for measured tradeoff analysis.</p>"},{"location":"core/execution-patterns/#does-this-hurt-caching","title":"\"Does this hurt caching?\"","text":"<p>Caching behavior depends on provider/runtime and prompt evolution. Use repeated benchmark runs and compare usage/cost metrics in reports instead of assuming one universal caching outcome.</p>"},{"location":"core/execution-patterns/#why-enforce-strict-mode-in-some-experiments","title":"\"Why enforce strict mode in some experiments?\"","text":"<p><code>/sandbox strict on</code> disables runner-level delegate actions in pure mode, which helps isolate code-level recursion behavior for cleaner experiments.</p>"},{"location":"core/memory-compaction/","title":"Memory Compaction","text":"<p>Module</p> <p><code>rlm_code.rlm.memory_compaction</code></p> <p>The memory compaction module prevents context window bloat by summarizing interaction history between REPL turns. Based on patterns from the RLM-From-Scratch implementation, it provides both LLM-based and deterministic summarization strategies with configurable triggers.</p>"},{"location":"core/memory-compaction/#overview","title":"Overview","text":"<p>As the RLM iterates through its think-code-observe loop, the interaction history grows with each step. Without compaction, the accumulated history of reasoning, code, and outputs can quickly consume the LLM's context window, leaving insufficient room for the actual task context and new reasoning.</p> <p>Memory compaction solves this by:</p> <ol> <li>Detecting when history has grown too large (by entry count or character count)</li> <li>Summarizing older entries into a compact summary</li> <li>Preserving the most recent entries in full detail</li> <li>Replacing the original history with the compacted version</li> </ol> <pre><code>graph TD\n    A[REPL History: 12 entries] --&gt; B{Should compact?}\n    B --&gt;|entries &gt;= 10| C[Split: old vs recent]\n    C --&gt; D[Summarize old entries]\n    C --&gt; E[Preserve last 2 entries]\n    D --&gt; F[Compacted History]\n    E --&gt; F\n    F --&gt; G[Summary + 2 preserved = 3 entries]</code></pre>"},{"location":"core/memory-compaction/#classes","title":"Classes","text":""},{"location":"core/memory-compaction/#compactionconfig","title":"<code>CompactionConfig</code>","text":"<p>Configuration dataclass controlling when and how compaction occurs.</p> <pre><code>from rlm_code.rlm.memory_compaction import CompactionConfig\n\n# Default configuration\nconfig = CompactionConfig()\n\n# Custom configuration for aggressive compaction\nconfig = CompactionConfig(\n    min_entries_for_compaction=3,\n    max_entries_before_compaction=6,\n    max_chars_before_compaction=4000,\n    summary_max_sentences=2,\n    preserve_last_n_entries=1,\n    include_key_findings=True,\n    use_llm_for_summary=True,\n    fallback_to_deterministic=True,\n)\n</code></pre>"},{"location":"core/memory-compaction/#configuration-fields","title":"Configuration Fields","text":"Field Type Default Description <code>min_entries_for_compaction</code> <code>int</code> <code>5</code> Minimum entries required before compaction can trigger. <code>max_entries_before_compaction</code> <code>int</code> <code>10</code> Force compaction when entry count reaches this threshold. <code>max_chars_before_compaction</code> <code>int</code> <code>8000</code> Force compaction when total character count reaches this threshold. <code>summary_max_sentences</code> <code>int</code> <code>3</code> Maximum sentences in the generated summary. <code>preserve_last_n_entries</code> <code>int</code> <code>2</code> Number of most recent entries to preserve in full. <code>include_key_findings</code> <code>bool</code> <code>True</code> Include extracted key findings in deterministic summaries. <code>use_llm_for_summary</code> <code>bool</code> <code>True</code> Use the LLM to generate summaries (higher quality). <code>fallback_to_deterministic</code> <code>bool</code> <code>True</code> Fall back to deterministic summarization if LLM fails. <p>Tuning Compaction Thresholds</p> <p>For models with smaller context windows (e.g., 8K tokens), lower the <code>max_entries_before_compaction</code> to <code>5</code> and <code>max_chars_before_compaction</code> to <code>3000</code>. For large-context models (128K+), you can increase these significantly or even disable compaction for short tasks.</p>"},{"location":"core/memory-compaction/#compactionresult","title":"<code>CompactionResult</code>","text":"<p>Dataclass holding the result of a compaction operation, including metrics about the compression achieved.</p> <pre><code>from rlm_code.rlm.memory_compaction import CompactionResult\n\n# Returned by MemoryCompactor.compact()\nresult: CompactionResult\nprint(f\"Compressed {result.original_entries} entries to {result.compacted_entries}\")\nprint(f\"Character reduction: {result.original_chars} -&gt; {result.compacted_chars}\")\nprint(f\"Compression ratio: {result.compression_ratio:.1%}\")\nprint(f\"Used LLM: {result.used_llm}\")\n</code></pre>"},{"location":"core/memory-compaction/#fields","title":"Fields","text":"Field Type Description <code>original_entries</code> <code>int</code> Number of entries before compaction. <code>compacted_entries</code> <code>int</code> Number of entries after compaction (1 summary + preserved). <code>original_chars</code> <code>int</code> Total characters before compaction. <code>compacted_chars</code> <code>int</code> Total characters after compaction. <code>summary</code> <code>str</code> The generated summary text. <code>preserved_entries</code> <code>list[REPLEntry]</code> The preserved recent entries. <code>timestamp</code> <code>str</code> ISO 8601 UTC timestamp of when compaction occurred. <code>used_llm</code> <code>bool</code> Whether LLM-based summarization was used."},{"location":"core/memory-compaction/#properties","title":"Properties","text":"Property Type Description <code>compression_ratio</code> <code>float</code> Ratio of characters removed (<code>1.0 - compacted/original</code>). Returns <code>0.0</code> if original was empty. <p>Compression Ratio</p> <p>A compression ratio of <code>0.75</code> means 75% of the original characters were removed. Typical LLM-based compaction achieves 0.6--0.8 compression; deterministic compaction typically achieves 0.4--0.6.</p>"},{"location":"core/memory-compaction/#memorycompactor","title":"<code>MemoryCompactor</code>","text":"<p>The primary compaction engine. Supports LLM-based summarization with deterministic fallback.</p> <pre><code>from rlm_code.rlm.memory_compaction import MemoryCompactor, CompactionConfig\n\n# Default compactor\ncompactor = MemoryCompactor()\n\n# Compactor with custom config and LLM connector\ncompactor = MemoryCompactor(\n    config=CompactionConfig(\n        max_entries_before_compaction=8,\n        preserve_last_n_entries=3,\n    ),\n    llm_connector=my_llm_connector,\n)\n</code></pre>"},{"location":"core/memory-compaction/#constructor","title":"Constructor","text":"Parameter Type Default Description <code>config</code> <code>CompactionConfig \\| None</code> <code>CompactionConfig()</code> Compaction configuration. <code>llm_connector</code> <code>Any</code> <code>None</code> LLM connector for summarization (must implement <code>generate_response(prompt=...)</code>)."},{"location":"core/memory-compaction/#methods","title":"Methods","text":""},{"location":"core/memory-compaction/#set_llm_connectorconnector","title":"<code>set_llm_connector(connector)</code>","text":"<p>Set or update the LLM connector after construction.</p> <pre><code>compactor = MemoryCompactor()\n# Later, when LLM is available:\ncompactor.set_llm_connector(my_llm_connector)\n</code></pre> Parameter Type Description <code>connector</code> <code>Any</code> LLM connector implementing <code>generate_response(prompt=...)</code>."},{"location":"core/memory-compaction/#should_compacthistory","title":"<code>should_compact(history)</code>","text":"<p>Check whether the given history should be compacted based on the configured thresholds.</p> <pre><code>from rlm_code.rlm.repl_types import REPLHistory\n\nhistory = REPLHistory()\nfor i in range(12):\n    history = history.append(reasoning=f\"Step {i}\", code=f\"x = {i}\", output=str(i))\n\ncompactor = MemoryCompactor()\nif compactor.should_compact(history):\n    print(\"History needs compaction!\")\n</code></pre> Parameter Type Description <code>history</code> <code>REPLHistory</code> The REPL history to check. <p>Returns: <code>bool</code> -- <code>True</code> if compaction should be triggered.</p> <p>Trigger logic:</p> <ol> <li>If <code>len(history) &lt; min_entries_for_compaction</code>: return <code>False</code> (never compact tiny histories)</li> <li>If <code>len(history) &gt;= max_entries_before_compaction</code>: return <code>True</code></li> <li>If total character count &gt;= <code>max_chars_before_compaction</code>: return <code>True</code></li> <li>Otherwise: return <code>False</code></li> </ol> <p>Minimum Threshold</p> <p>Compaction never triggers if the history has fewer than <code>min_entries_for_compaction</code> entries (default: 5), regardless of character count. This prevents unnecessary compaction of short histories.</p>"},{"location":"core/memory-compaction/#compacthistory-task-forcefalse","title":"<code>compact(history, task=\"\", force=False)</code>","text":"<p>Perform the compaction, returning a <code>CompactionResult</code> with the summary and preserved entries.</p> <pre><code>result = compactor.compact(history, task=\"Analyze the dataset\")\n\nprint(f\"Summary: {result.summary}\")\nprint(f\"Preserved {len(result.preserved_entries)} recent entries\")\nprint(f\"Compression: {result.compression_ratio:.0%}\")\n</code></pre> Parameter Type Default Description <code>history</code> <code>REPLHistory</code> required The REPL history to compact. <code>task</code> <code>str</code> <code>\"\"</code> The original task description (provides context for summarization). <code>force</code> <code>bool</code> <code>False</code> Force compaction even if thresholds are not met. <p>Returns: <code>CompactionResult</code></p> <p>Strategy selection:</p> <ol> <li>If <code>use_llm_for_summary=True</code> and connector is set: attempt LLM summarization</li> <li>If LLM fails and <code>fallback_to_deterministic=True</code>: use deterministic summary</li> <li>If LLM fails and <code>fallback_to_deterministic=False</code>: re-raise the exception</li> <li>If <code>use_llm_for_summary=False</code>: use deterministic summary directly</li> </ol> <p>No-Op When Thresholds Not Met</p> <p>If <code>force=False</code> and the thresholds are not met, <code>compact()</code> returns a <code>CompactionResult</code> with an empty summary and all original entries preserved. Check <code>result.summary</code> to determine whether compaction actually occurred.</p>"},{"location":"core/memory-compaction/#apply_compactionhistory-compaction_result","title":"<code>apply_compaction(history, compaction_result)</code>","text":"<p>Apply a <code>CompactionResult</code> to produce a new, compacted <code>REPLHistory</code>. The resulting history has a summary entry as the first item, followed by the preserved recent entries.</p> <pre><code>result = compactor.compact(history, task=\"Analyze data\")\ncompacted_history = compactor.apply_compaction(history, result)\n\nprint(f\"Original: {len(history)} entries\")\nprint(f\"Compacted: {len(compacted_history)} entries\")\n\n# The first entry contains the summary\nfirst = compacted_history.entries[0]\nprint(first.reasoning)   # \"[COMPACTED] Working on: Analyze data. Completed 8 steps...\"\nprint(first.code)        # \"# Previous steps summarized above\"\nprint(first.output)      # \"(Compacted 10 steps)\"\n</code></pre> Parameter Type Description <code>history</code> <code>REPLHistory</code> The original history. <code>compaction_result</code> <code>CompactionResult</code> The result from <code>compact()</code>. <p>Returns: <code>REPLHistory</code> -- a new compacted history instance.</p> <p>Summary Entry Format</p> <p>The summary entry uses a special format that the LLM can recognize:</p> <ul> <li>Reasoning: <code>[COMPACTED] &lt;summary text&gt;</code></li> <li>Code: <code># Previous steps summarized above</code></li> <li>Output: <code>(Compacted N steps)</code></li> </ul> <p>If the <code>CompactionResult</code> has an empty summary (no compaction performed), the original history is returned unchanged.</p>"},{"location":"core/memory-compaction/#entry-preservation-strategy","title":"Entry Preservation Strategy","text":"<p>The compactor preserves the <code>preserve_last_n_entries</code> most recent entries unchanged. This ensures the LLM has immediate access to its latest work without needing to re-derive it from a summary.</p> <pre><code>Before compaction:\n  [Step 1] [Step 2] [Step 3] [Step 4] [Step 5] [Step 6] [Step 7] [Step 8]\n                                                          ^^^^^^^^^^^^^^^^^\n                                                          preserve_last_n=2\n\nAfter compaction:\n  [SUMMARY of Steps 1-6]  [Step 7]  [Step 8]\n</code></pre>"},{"location":"core/memory-compaction/#summarization-strategies","title":"Summarization Strategies","text":"<p>The compactor supports two summarization strategies, selected based on configuration and availability.</p>"},{"location":"core/memory-compaction/#llm-based-summarization","title":"LLM-Based Summarization","text":"<p>When <code>use_llm_for_summary=True</code> and an LLM connector is provided, the compactor sends a structured prompt requesting a concise summary.</p> <p>The prompt includes:</p> <ul> <li>The original task description</li> <li>A formatted view of each step (reasoning preview up to 200 chars, code preview up to 150 chars, output preview up to 100 chars)</li> <li>Instructions to capture: what was attempted, key findings, and resolved errors</li> </ul> <pre><code># Enable LLM summarization (default)\nconfig = CompactionConfig(use_llm_for_summary=True)\ncompactor = MemoryCompactor(config=config, llm_connector=my_llm)\n</code></pre> <p>LLM Summary Quality</p> <p>LLM-based summaries are typically more coherent and context-aware than deterministic ones. They excel at capturing the narrative arc of multi-step reasoning. However, they add latency and cost. For benchmark runs with many iterations, consider using deterministic summarization.</p>"},{"location":"core/memory-compaction/#deterministic-summarization","title":"Deterministic Summarization","text":"<p>When LLM summarization is unavailable or disabled, the compactor uses a rule-based approach that extracts key information from entries using heuristics.</p> <p>The deterministic summary components:</p> Component Source Example Task context First 100 chars of the task description <code>\"Working on: Analyze sentiment\"</code> Step statistics Count of total and successful steps <code>\"Completed 8 steps (6 successful).\"</code> LLM sub-call count Sum of <code>llm_calls</code> across entries <code>\"Made 3 LLM sub-calls.\"</code> Key findings Numeric values and key-value patterns from outputs <code>\"Key findings: accuracy=0.87; total=1523\"</code> Error summary Types of errors encountered <code>\"Resolved issues: KeyError, ValueError\"</code> <p>Success detection: An output is considered \"successful\" if it does not contain any of the keywords <code>error</code>, <code>exception</code>, <code>traceback</code>, or <code>failed</code> (case-insensitive).</p> <p>Key findings extraction:</p> <ul> <li>Numeric values are extracted via regex (<code>\\b\\d+(?:,\\d{3})*(?:\\.\\d+)?\\b</code>)</li> <li>Key-value patterns are matched with <code>(\\w+):\\s*([^\\n,]+)</code></li> <li>Up to 3 findings are included</li> </ul> <pre><code># Force deterministic summarization\nconfig = CompactionConfig(use_llm_for_summary=False)\ncompactor = MemoryCompactor(config=config)\n\nresult = compactor.compact(history, task=\"Parse the log file\")\nprint(result.summary)\n# \"Working on: Parse the log file. Completed 8 steps (6 successful).\n#  Made 3 LLM sub-calls. Key findings: lines=1024; errors=12.\n#  Resolved issues: KeyError, ValueError\"\n</code></pre> <p>Fallback Behavior</p> <p>When <code>fallback_to_deterministic=True</code> (the default), LLM summarization failures silently fall back to deterministic summarization. Set <code>fallback_to_deterministic=False</code> to let LLM errors propagate if you need strict quality guarantees.</p>"},{"location":"core/memory-compaction/#conversationmemory","title":"<code>ConversationMemory</code>","text":"<p>Manages memory across multiple conversation turns in chat-style interactions. Provides automatic compaction of turn history and context retrieval for the LLM.</p> <pre><code>from rlm_code.rlm.memory_compaction import ConversationMemory\n\nmemory = ConversationMemory(max_turns=20)\n\n# Add conversation turns\nmemory.add_turn(\n    user_message=\"What's in the dataset?\",\n    assistant_response=\"The dataset contains 1000 records with 5 columns...\",\n    task=\"Analyze dataset\",\n)\n\nmemory.add_turn(\n    user_message=\"Find the outliers\",\n    assistant_response=\"I found 12 outliers in column 'price'...\",\n)\n\n# Get context for next LLM call\ncontext = memory.get_context()\nprint(context)\n</code></pre>"},{"location":"core/memory-compaction/#constructor_1","title":"Constructor","text":"Parameter Type Default Description <code>compactor</code> <code>MemoryCompactor \\| None</code> <code>MemoryCompactor()</code> Compactor for REPL history within turns. <code>max_turns</code> <code>int</code> <code>20</code> Maximum turns to retain before auto-compaction."},{"location":"core/memory-compaction/#methods_1","title":"Methods","text":"Method Signature Description <code>add_turn</code> <code>(user_message, assistant_response, history=None, task=\"\")</code> Add a conversation turn. Auto-compacts if <code>max_turns</code> exceeded. <code>get_context</code> <code>() -&gt; str</code> Get formatted conversation context for the LLM (last 5 turns). <code>clear</code> <code>() -&gt; None</code> Clear all memory (turns and compacted summary). <p>Auto-Compaction of Turns</p> <p>When the number of turns exceeds <code>max_turns</code>, the older half is compacted into a summary string. The summary preserves the first 100 characters of each user message and assistant response from up to 3 of the compacted turns.</p>"},{"location":"core/memory-compaction/#context-format","title":"Context Format","text":"<p>The <code>get_context()</code> method returns a formatted string suitable for inclusion in an LLM prompt:</p> <pre><code>[Previous conversation (10 turns): Q: What's in the dataset?... A: The dataset contains...\n | Q: Find the outliers... A: I found 12 outliers...]\n\nUser: Show me a histogram of prices\nAssistant: Here's the histogram showing...\n\nUser: What's the median price?\nAssistant: The median price is $45.50...\n</code></pre>"},{"location":"core/memory-compaction/#end-to-end-example","title":"End-to-End Example","text":"<pre><code>from rlm_code.rlm.memory_compaction import (\n    MemoryCompactor,\n    CompactionConfig,\n    ConversationMemory,\n)\nfrom rlm_code.rlm.repl_types import REPLHistory\n\n# 1. Configure compaction\nconfig = CompactionConfig(\n    max_entries_before_compaction=8,\n    max_chars_before_compaction=5000,\n    preserve_last_n_entries=2,\n    summary_max_sentences=2,\n    use_llm_for_summary=False,  # Deterministic for this example\n)\n\n# 2. Create compactor\ncompactor = MemoryCompactor(config=config)\n\n# 3. Simulate REPL history building\nhistory = REPLHistory()\nfor i in range(10):\n    history = history.append(\n        reasoning=f\"Step {i}: analyzing chunk {i}\",\n        code=f\"result_{i} = analyze(chunks[{i}])\",\n        output=f\"Processed chunk {i}: found {i * 3} patterns\",\n    )\n\n# 4. Check and compact\nif compactor.should_compact(history):\n    result = compactor.compact(history, task=\"Analyze all data chunks\")\n    print(f\"Compression ratio: {result.compression_ratio:.0%}\")\n\n    # 5. Apply compaction\n    compacted = compactor.apply_compaction(history, result)\n    print(f\"Entries: {len(history)} -&gt; {len(compacted)}\")\n    # Entries: 10 -&gt; 3  (1 summary + 2 preserved)\n</code></pre>"},{"location":"core/memory-compaction/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"core/memory-compaction/#high-throughput-benchmarks","title":"High-Throughput Benchmarks","text":"<p>Minimize overhead by using deterministic summarization and aggressive compaction:</p> <pre><code>config = CompactionConfig(\n    min_entries_for_compaction=3,\n    max_entries_before_compaction=5,\n    max_chars_before_compaction=3000,\n    use_llm_for_summary=False,  # Avoid extra LLM calls\n    preserve_last_n_entries=1,\n    summary_max_sentences=2,\n)\n</code></pre>"},{"location":"core/memory-compaction/#long-running-research-sessions","title":"Long-Running Research Sessions","text":"<p>Preserve more context and use LLM for higher-quality summaries:</p> <pre><code>config = CompactionConfig(\n    min_entries_for_compaction=8,\n    max_entries_before_compaction=15,\n    max_chars_before_compaction=15000,\n    use_llm_for_summary=True,\n    fallback_to_deterministic=True,\n    preserve_last_n_entries=3,\n    summary_max_sentences=4,\n    include_key_findings=True,\n)\n</code></pre>"},{"location":"core/memory-compaction/#minimal-memory-small-context-window-models","title":"Minimal Memory (Small Context Window Models)","text":"<p>Compact as early and aggressively as possible:</p> <pre><code>config = CompactionConfig(\n    min_entries_for_compaction=2,\n    max_entries_before_compaction=4,\n    max_chars_before_compaction=2000,\n    use_llm_for_summary=False,\n    preserve_last_n_entries=1,\n    summary_max_sentences=1,\n)\n</code></pre>"},{"location":"core/memory-compaction/#no-compaction-large-context-window-models","title":"No Compaction (Large Context Window Models)","text":"<p>Disable compaction entirely for short tasks with large-context models:</p> <pre><code>config = CompactionConfig(\n    min_entries_for_compaction=999,  # Effectively never trigger\n    max_entries_before_compaction=999,\n    max_chars_before_compaction=999999,\n)\n</code></pre>"},{"location":"core/repl-types/","title":"REPL Types","text":"<p>Module</p> <p><code>rlm_code.rlm.repl_types</code></p> <p>The REPL types module provides the structured data types that underpin the RLM execution model. These types manage REPL state, variable metadata, execution history, and results. Based on patterns from DSPy's RLM implementation, they follow a functional, immutable-by-convention design.</p>"},{"location":"core/repl-types/#overview","title":"Overview","text":"<p>The RLM paradigm is fundamentally a REPL loop: the LLM reasons, writes code, observes output, and repeats. The types in this module capture the data flowing through that loop:</p> Type Role in the Loop <code>REPLVariable</code> Metadata about a variable in the REPL namespace (the \"context-as-variable\" innovation) <code>REPLEntry</code> A single iteration: reasoning + code + output <code>REPLHistory</code> The full sequence of iterations (immutable append) <code>REPLResult</code> The result of executing one code block <pre><code>graph TD\n    A[Task + Context] --&gt; B[REPLVariable metadata]\n    B --&gt; C[LLM sees metadata, not full context]\n    C --&gt; D[LLM generates code]\n    D --&gt; E[Code executed in REPL]\n    E --&gt; F[REPLResult captured]\n    F --&gt; G[REPLEntry created]\n    G --&gt; H[REPLHistory.append\\(\\)]\n    H --&gt; I{Done?}\n    I --&gt;|No| C\n    I --&gt;|Yes| J[Final answer]</code></pre>"},{"location":"core/repl-types/#classes","title":"Classes","text":""},{"location":"core/repl-types/#replvariable","title":"<code>REPLVariable</code>","text":"<p>Metadata about a variable stored in the REPL namespace. This is the key innovation from the RLM paper: instead of loading full context into the LLM's token window, the context is stored as a REPL variable and only metadata (name, type, length, preview) is provided to the LLM. The LLM then accesses the variable programmatically through code.</p> <pre><code>from rlm_code.rlm.repl_types import REPLVariable\n\n# Create from a Python value\nvar = REPLVariable.from_value(\n    name=\"document\",\n    value=\"This is a very long document with thousands of words...\",\n    description=\"The input document to analyze\",\n    constraints=\"Read-only. Do not modify.\",\n)\n\nprint(var.format())\n</code></pre> <p>Output: <pre><code>Variable: `document` (access it in your code)\nType: str\nDescription: The input document to analyze\nConstraints: Read-only. Do not modify.\nTotal length: 54 characters\nPreview:\n</code></pre> This is a very long document with thousands of words... <pre><code>\n</code></pre></p>"},{"location":"core/repl-types/#fields","title":"Fields","text":"Field Type Default Description <code>name</code> <code>str</code> required Variable name in the REPL namespace. <code>type_name</code> <code>str</code> required Python type name (e.g., <code>\"str\"</code>, <code>\"dict\"</code>, <code>\"DataFrame\"</code>). <code>description</code> <code>str</code> <code>\"\"</code> Human-readable description of the variable's contents. <code>constraints</code> <code>str</code> <code>\"\"</code> Usage constraints (e.g., \"Read-only\"). <code>total_length</code> <code>int</code> <code>0</code> Total character count of the string representation. <code>preview</code> <code>str</code> <code>\"\"</code> First N characters of the value for LLM orientation."},{"location":"core/repl-types/#class-constants","title":"Class Constants","text":"Constant Value Description <code>PREVIEW_LENGTH</code> <code>500</code> Default number of characters to include in the preview."},{"location":"core/repl-types/#class-methods","title":"Class Methods","text":""},{"location":"core/repl-types/#from_valuename-value-description-constraints-preview_length500","title":"<code>from_value(name, value, description=\"\", constraints=\"\", preview_length=500)</code>","text":"<p>Create a <code>REPLVariable</code> from an actual Python value, automatically extracting type information and a preview.</p> <pre><code># String value\nvar = REPLVariable.from_value(\"text\", \"Hello, world!\")\nassert var.type_name == \"str\"\nassert var.total_length == 13\n\n# Dictionary value (JSON-formatted preview)\nvar = REPLVariable.from_value(\n    \"config\",\n    {\"model\": \"gpt-4o\", \"temperature\": 0.7},\n    description=\"Model configuration\",\n)\nassert var.type_name == \"dict\"\n\n# List value\nvar = REPLVariable.from_value(\"items\", [1, 2, 3, 4, 5])\nassert var.type_name == \"list\"\n\n# Custom preview length\nvar = REPLVariable.from_value(\n    \"large_text\",\n    \"x\" * 10000,\n    preview_length=100,\n)\nassert len(var.preview) &lt;= 103  # 100 chars + \"...\"\n</code></pre> Parameter Type Default Description <code>name</code> <code>str</code> required Variable name. <code>value</code> <code>Any</code> required The actual Python value. <code>description</code> <code>str</code> <code>\"\"</code> Description of the variable. <code>constraints</code> <code>str</code> <code>\"\"</code> Usage constraints. <code>preview_length</code> <code>int</code> <code>500</code> Maximum characters in the preview. <p>Type-aware serialization for preview:</p> Value Type Serialization Method <code>str</code> Used directly <code>dict</code> or <code>list</code> <code>json.dumps(value, indent=2, default=str)</code> Other <code>str(value)</code> <p>Preview Truncation</p> <p>When the string representation exceeds <code>preview_length</code>, the preview is truncated and <code>\"...\"</code> is appended. For <code>dict</code> and <code>list</code> values, the representation is JSON-formatted with 2-space indentation before truncation.</p>"},{"location":"core/repl-types/#instance-methods","title":"Instance Methods","text":"Method Returns Description <code>format()</code> <code>str</code> Format variable metadata for inclusion in an LLM prompt. <code>to_dict()</code> <code>dict[str, Any]</code> Serialize all fields for logging or persistence."},{"location":"core/repl-types/#format","title":"<code>format()</code>","text":"<p>Format variable metadata for the LLM prompt. This is what the LLM sees instead of the full variable content.</p> <p>Output format:</p> <p><pre><code>Variable: `context` (access it in your code)\nType: str\nDescription: Legal contract to analyze\nConstraints: Must not be modified\nTotal length: 45,230 characters\nPreview:\n</code></pre> AGREEMENT made this 15th day of January, 2024, between... <pre><code>\n</code></pre></p> <p>Optional fields (<code>description</code>, <code>constraints</code>) are only included when non-empty.</p> <p>Token Savings</p> <p>The entire point of <code>REPLVariable</code> is token efficiency. A 100,000-character document stored as a REPL variable produces metadata of roughly 600--700 characters (approximately 150 tokens). The full document would consume approximately 25,000 tokens. This is a 99%+ token reduction -- the core of the RLM \"context-as-variable\" paradigm.</p>"},{"location":"core/repl-types/#to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize for logging and persistence.</p> <pre><code>var.to_dict()\n# {\n#     \"name\": \"context\",\n#     \"type_name\": \"str\",\n#     \"description\": \"Legal contract to analyze\",\n#     \"constraints\": \"\",\n#     \"total_length\": 45230,\n#     \"preview\": \"AGREEMENT made this 15th day...\",\n# }\n</code></pre> <p>Slots Optimization</p> <p><code>REPLVariable</code> uses <code>@dataclass(slots=True)</code> for reduced memory footprint per instance. This matters when tracking many variables in complex REPL environments.</p>"},{"location":"core/repl-types/#replentry","title":"<code>REPLEntry</code>","text":"<p>A single entry in the REPL history, capturing one iteration of the think-code-observe loop.</p> <pre><code>from rlm_code.rlm.repl_types import REPLEntry\n\nentry = REPLEntry(\n    reasoning=\"I need to count the words in the document\",\n    code=\"word_count = len(document.split())\\nprint(word_count)\",\n    output=\"1523\",\n    execution_time=0.05,\n    llm_calls=[{\"prompt\": \"...\", \"response\": \"...\"}],\n)\n\n# Format for display\nprint(entry.format(index=1))\n</code></pre> <p>Output: <pre><code>[Step 1]\nReasoning: I need to count the words in the document\nCode:\n```python\nword_count = len(document.split())\nprint(word_count)\n</code></pre> Output: <pre><code>1523\n</code></pre> (Made 1 sub-LLM call(s)) <pre><code>#### Fields\n\n| Field | Type | Default | Description |\n|---|---|---|---|\n| `reasoning` | `str` | `\"\"` | The LLM's reasoning or thought process for this step. |\n| `code` | `str` | `\"\"` | The Python code generated by the LLM. |\n| `output` | `str` | `\"\"` | Stdout/stderr output from executing the code. |\n| `execution_time` | `float` | `0.0` | Wall-clock execution time in seconds. |\n| `llm_calls` | `list[dict[str, Any]]` | `[]` | Records of sub-LLM calls made during code execution via `llm_query()`. |\n| `timestamp` | `str` | *auto* | ISO 8601 UTC timestamp of when the entry was created. |\n\n#### Methods\n\n##### `format(index=None)`\n\nFormat the entry for inclusion in an LLM history prompt.\n\n| Parameter | Type | Default | Description |\n|---|---|---|---|\n| `index` | `int \\| None` | `None` | Step index to display. Uses `[Step]` if `None`. |\n\n**Returns:** `str` -- formatted entry text.\n\nThe format includes:\n\n- Step header with optional index\n- Reasoning section (if non-empty)\n- Code section in a Python fenced block (if non-empty)\n- Output section in a plain fenced block (if non-empty)\n- Sub-LLM call count (if any calls were made)\n\n!!! info \"Output Truncation\"\n    Long outputs (over 2,000 characters) are automatically truncated with a `... (truncated)` marker to prevent history bloat. For the full output, access `entry.output` directly.\n\n##### `to_dict()`\n\nSerialize the entry to a dictionary for logging or persistence.\n\n**Returns:** `dict[str, Any]` containing all fields.\n\n---\n\n### `REPLHistory`\n\nImmutable history of REPL interactions. Following DSPy's functional pattern, `append()` returns a **new** `REPLHistory` instance rather than mutating in place. This enables clean trajectory building without side effects.\n\n```python\nfrom rlm_code.rlm.repl_types import REPLHistory\n\n# Start with empty history\nhistory = REPLHistory()\nassert len(history) == 0\n\n# Append returns a NEW history\nhistory = history.append(\n    reasoning=\"First, I'll check the data shape\",\n    code=\"print(len(context))\",\n    output=\"15234\",\n    execution_time=0.01,\n)\nassert len(history) == 1\n\n# Chain appends\nhistory = history.append(\n    reasoning=\"Now I'll analyze the first section\",\n    code=\"section = context[:1000]\\nprint(section[:100])\",\n    output=\"The quick brown fox...\",\n    execution_time=0.02,\n)\nassert len(history) == 2\n</code></pre></p>"},{"location":"core/repl-types/#fields_1","title":"Fields","text":"Field Type Default Description <code>entries</code> <code>list[REPLEntry]</code> <code>[]</code> The list of REPL entries."},{"location":"core/repl-types/#methods","title":"Methods","text":""},{"location":"core/repl-types/#append-reasoning-code-output-execution_time00-llm_callsnone","title":"<code>append(*, reasoning=\"\", code=\"\", output=\"\", execution_time=0.0, llm_calls=None)</code>","text":"<p>Return a new <code>REPLHistory</code> with the entry appended. All parameters are keyword-only.</p> <pre><code>new_history = history.append(\n    reasoning=\"Calculate the average\",\n    code=\"avg = sum(values) / len(values)\\nprint(avg)\",\n    output=\"42.5\",\n    execution_time=0.003,\n    llm_calls=[{\"prompt\": \"...\", \"response\": \"...\"}],\n)\n</code></pre> Parameter Type Default Description <code>reasoning</code> <code>str</code> <code>\"\"</code> LLM reasoning text. <code>code</code> <code>str</code> <code>\"\"</code> Generated Python code. <code>output</code> <code>str</code> <code>\"\"</code> Execution output. <code>execution_time</code> <code>float</code> <code>0.0</code> Execution time in seconds. <code>llm_calls</code> <code>list[dict] \\| None</code> <code>None</code> Sub-LLM call records. <p>Returns: <code>REPLHistory</code> -- a new instance with the entry appended.</p> <p>Immutability</p> <p><code>append()</code> does not modify the original history. Always capture the return value: <pre><code># Correct\nhistory = history.append(reasoning=\"...\", code=\"...\", output=\"...\")\n\n# Bug -- original history is unchanged, new history is discarded\nhistory.append(reasoning=\"...\", code=\"...\", output=\"...\")\n</code></pre></p>"},{"location":"core/repl-types/#formatmax_entries10","title":"<code>format(max_entries=10)</code>","text":"<p>Format the history for inclusion in an LLM prompt. Shows the most recent entries up to <code>max_entries</code>.</p> <pre><code>prompt_section = history.format(max_entries=5)\nprint(prompt_section)\n</code></pre> Parameter Type Default Description <code>max_entries</code> <code>int</code> <code>10</code> Maximum number of recent entries to include. <p>Returns: <code>str</code> -- formatted history text. Returns <code>\"(No prior steps)\"</code> if empty.</p> <p>Sliding Window</p> <p>When the history exceeds <code>max_entries</code>, only the most recent entries are shown, with a header indicating how many total steps exist: <code>\"(Showing last 10 of 25 steps)\"</code>. Step indices are numbered correctly relative to the full history.</p>"},{"location":"core/repl-types/#to_list","title":"<code>to_list()</code>","text":"<p>Serialize all entries to a list of dictionaries for logging.</p> <p>Returns: <code>list[dict[str, Any]]</code></p>"},{"location":"core/repl-types/#dunder-methods","title":"Dunder Methods","text":"Method Behavior <code>__len__()</code> Returns the number of entries. <code>__iter__()</code> Iterates over <code>REPLEntry</code> objects. <code>__bool__()</code> Returns <code>True</code> if there are any entries. <pre><code>history = REPLHistory()\nassert not history          # Empty history is falsy\nassert len(history) == 0\n\nhistory = history.append(code=\"x = 1\", output=\"\")\nassert history              # Non-empty history is truthy\nassert len(history) == 1\n\nfor entry in history:\n    print(entry.code)       # \"x = 1\"\n</code></pre>"},{"location":"core/repl-types/#replresult","title":"<code>REPLResult</code>","text":"<p>Result of executing a single code block in the REPL sandbox. This is the raw execution result before it is incorporated into a <code>REPLEntry</code>.</p> <pre><code>from rlm_code.rlm.repl_types import REPLResult\n\nresult = REPLResult(\n    stdout=\"42\\n\",\n    stderr=\"\",\n    locals={\"x\": 42, \"data\": [1, 2, 3]},\n    execution_time=0.15,\n    llm_calls=[],\n    success=True,\n    final_output=None,\n)\n</code></pre>"},{"location":"core/repl-types/#fields_2","title":"Fields","text":"Field Type Default Description <code>stdout</code> <code>str</code> <code>\"\"</code> Standard output captured during execution. <code>stderr</code> <code>str</code> <code>\"\"</code> Standard error captured during execution. <code>locals</code> <code>dict[str, Any]</code> <code>{}</code> The REPL namespace after execution (local variables). <code>execution_time</code> <code>float</code> <code>0.0</code> Wall-clock execution time in seconds. <code>llm_calls</code> <code>list[dict[str, Any]]</code> <code>[]</code> Sub-LLM calls made via <code>llm_query()</code> during execution. <code>success</code> <code>bool</code> <code>True</code> Whether execution completed without errors. <code>final_output</code> <code>dict[str, Any] \\| None</code> <code>None</code> Set if <code>FINAL()</code> or <code>FINAL_VAR()</code> was called during execution."},{"location":"core/repl-types/#final_output-structure","title":"<code>final_output</code> Structure","text":"<p>When <code>FINAL(answer)</code> is called: <pre><code>{\"answer\": answer, \"type\": \"direct\"}\n</code></pre></p> <p>When <code>FINAL_VAR(variable_name)</code> is called: <pre><code>{\"var\": variable_name, \"type\": \"variable\"}\n</code></pre></p>"},{"location":"core/repl-types/#methods_1","title":"Methods","text":""},{"location":"core/repl-types/#to_dict_1","title":"<code>to_dict()</code>","text":"<p>Serialize for logging. Note that <code>locals</code> values are truncated to 200 characters each to prevent oversized log entries.</p> <p>Returns: <code>dict[str, Any]</code></p> <pre><code>result.to_dict()\n# {\n#     \"stdout\": \"42\\n\",\n#     \"stderr\": \"\",\n#     \"locals\": {\"x\": \"42\", \"data\": \"[1, 2, 3]\"},\n#     \"execution_time\": 0.15,\n#     \"llm_calls\": [],\n#     \"success\": True,\n#     \"final_output\": None,\n# }\n</code></pre> <p>Checking for Termination</p> <p>The <code>final_output</code> field is the primary way to detect that the REPL code signaled completion: <pre><code>if result.final_output is not None:\n    if result.final_output[\"type\"] == \"direct\":\n        answer = result.final_output[\"answer\"]\n    elif result.final_output[\"type\"] == \"variable\":\n        var_name = result.final_output[\"var\"]\n        answer = result.locals[var_name]\n</code></pre></p>"},{"location":"core/repl-types/#type-relationships","title":"Type Relationships","text":"<p>The REPL types form a clear data pipeline through the RLM execution loop:</p> <pre><code>REPLVariable          REPLHistory\n(context metadata)    (accumulated steps)\n       |                    |\n       v                    v\n   LLM Prompt -------&gt; LLM Response\n                            |\n                            v\n                     Code Extraction\n                            |\n                            v\n                    REPL Execution\n                            |\n                            v\n                      REPLResult\n                            |\n                            v\n                      REPLEntry\n                            |\n                            v\n                  REPLHistory.append()\n                            |\n                            v\n                   Updated REPLHistory\n</code></pre>"},{"location":"core/repl-types/#how-variables-are-tracked-and-displayed","title":"How Variables Are Tracked and Displayed","text":"<p>The flow from raw data to LLM prompt:</p> <pre><code>1. User provides context data\n       |\n       v\n2. PureRLMEnvironment.initialize_context(data, description=\"...\")\n       |\n       v\n3. REPLVariable.from_value(name=\"context\", value=data)\n       |  - Determines type_name (e.g., \"str\")\n       |  - Calculates total_length (e.g., 45230)\n       |  - Generates preview (first 500 chars)\n       |\n       v\n4. Variable stored in self._variables list\n   Value stored in self._namespace[\"context\"]\n       |\n       v\n5. planner_prompt() calls var.format() for each variable\n       |\n       v\n6. LLM sees:\n   \"Variable: `context` (access it in your code)\n    Type: str\n    Description: Legal contract to analyze\n    Total length: 45,230 characters\n    Preview:\n    ```\n    AGREEMENT made this 15th day...\n    ```\"\n       |\n       v\n7. LLM writes code: print(context[:1000])\n       |\n       v\n8. Code executes in namespace where context = actual full data\n</code></pre> <p>This is the fundamental mechanism that separates RLM from traditional coding agents: the LLM prompt contains metadata about the context (approximately 150 tokens), not the context itself (approximately 11,000+ tokens).</p>"},{"location":"core/repl-types/#examples","title":"Examples","text":""},{"location":"core/repl-types/#building-a-complete-interaction","title":"Building a Complete Interaction","text":"<pre><code>from rlm_code.rlm.repl_types import REPLVariable, REPLHistory\n\n# 1. Create variable metadata for the LLM\ncontext = \"A very long document...\" * 1000\nvar = REPLVariable.from_value(\n    name=\"context\",\n    value=context,\n    description=\"Research paper to analyze\",\n)\n\n# 2. Build history through iterations\nhistory = REPLHistory()\n\n# Iteration 1: Explore the data\nhistory = history.append(\n    reasoning=\"First, I'll check the length of the context\",\n    code=\"print(f'Context length: {len(context)}')\",\n    output=\"Context length: 25000\",\n    execution_time=0.01,\n)\n\n# Iteration 2: Analyze\nhistory = history.append(\n    reasoning=\"Now I'll find key terms\",\n    code=\"words = context.split()\\nprint(f'Word count: {len(words)}')\",\n    output=\"Word count: 4167\",\n    execution_time=0.02,\n)\n\n# 3. Format for next LLM call\nprompt = f\"\"\"\n{var.format()}\n\nPrevious steps:\n{history.format()}\n\nWhat should you do next?\n\"\"\"\n</code></pre>"},{"location":"core/repl-types/#serializing-for-persistence","title":"Serializing for Persistence","text":"<pre><code>import json\n\n# Serialize history\ndata = history.to_list()\njson_str = json.dumps(data, indent=2)\n\n# Serialize variable metadata\nvar_data = var.to_dict()\n</code></pre>"},{"location":"core/repl-types/#working-with-replresult","title":"Working with REPLResult","text":"<pre><code>from rlm_code.rlm.repl_types import REPLResult\n\n# Successful execution\nresult = REPLResult(\n    stdout=\"Hello, world!\\n\",\n    stderr=\"\",\n    locals={\"greeting\": \"Hello, world!\"},\n    execution_time=0.001,\n    success=True,\n)\n\n# Failed execution\nresult = REPLResult(\n    stdout=\"\",\n    stderr=\"NameError: name 'undefined_var' is not defined\",\n    locals={},\n    execution_time=0.001,\n    success=False,\n)\n\n# Execution with FINAL\nresult = REPLResult(\n    stdout=\"\",\n    stderr=\"\",\n    locals={\"answer\": 42},\n    execution_time=0.005,\n    success=True,\n    final_output={\"answer\": 42, \"type\": \"direct\"},\n)\n</code></pre>"},{"location":"core/runner/","title":"RLM Runner","text":"<p>Module</p> <p><code>rlm_code.rlm.runner</code></p> <p>The <code>RLMRunner</code> is the multi-paradigm orchestrator at the center of RLM Code. It manages the complete lifecycle of RLM execution: task dispatch, environment selection, action proposal, sandbox execution, reward calculation, memory management, benchmark sweeps, and trajectory persistence.</p>"},{"location":"core/runner/#classes","title":"Classes","text":""},{"location":"core/runner/#rlmrunner","title":"<code>RLMRunner</code>","text":"<p>The primary orchestrator. Supports three paradigms out of the box:</p> Paradigm Environment Description Pure RLM <code>pure_rlm</code> Paper-compliant context-as-variable with <code>llm_query()</code> CodeAct <code>generic</code> Context included directly in the token window Traditional <code>dspy</code> DSPy-aware with file operations, search, and verifier suites"},{"location":"core/runner/#constructor","title":"Constructor","text":"<pre><code>class RLMRunner:\n    def __init__(\n        self,\n        llm_connector: Any,\n        execution_engine: Any,\n        run_dir: Path | None = None,\n        workdir: Path | None = None,\n        observability: RLMObservability | None = None,\n        event_bus: RLMEventBus | None = None,\n        reward_profile: RLMRewardProfile | dict[str, Any] | None = None,\n        benchmark_pack_paths: list[str | Path] | None = None,\n        max_parallelism: int = 4,\n    ):\n</code></pre> Parameter Type Default Description <code>llm_connector</code> <code>Any</code> required LLM backend connector (must implement <code>generate_response()</code>) <code>execution_engine</code> <code>Any</code> required Code execution sandbox <code>run_dir</code> <code>Path \\| None</code> Auto-detected Directory for JSONL trajectory files <code>workdir</code> <code>Path \\| None</code> <code>Path.cwd()</code> Project working directory <code>observability</code> <code>RLMObservability \\| None</code> Auto-created Observability sink manager <code>event_bus</code> <code>RLMEventBus \\| None</code> Auto-created Event bus for pub-sub <code>reward_profile</code> <code>RLMRewardProfile \\| dict \\| None</code> Default profile Reward tuning knobs <code>benchmark_pack_paths</code> <code>list[str \\| Path] \\| None</code> <code>None</code> External benchmark pack file paths <code>max_parallelism</code> <code>int</code> <code>4</code> Maximum concurrent child tasks <p>Run Directory Detection</p> <p>The runner automatically detects the run directory, checking for <code>.rlm_code/rlm/runs</code> first, then falling back to legacy <code>.dspy_code/rlm/runs</code> paths.</p>"},{"location":"core/runner/#environment-registry","title":"Environment Registry","text":"<p>On construction, the runner initializes a dictionary of environments:</p> <pre><code>self.environments = {\n    \"generic\":   GenericRLMEnvironment(...),\n    \"rlm\":       GenericRLMEnvironment(...),\n    \"dspy\":      DSPyCodingRLMEnvironment(...),\n    \"dspy-coding\": DSPyCodingRLMEnvironment(...),\n    \"framework\": DSPyCodingRLMEnvironment(...),\n    \"pure_rlm\":  PureRLMEnvironment(...),\n    \"pure-rlm\":  PureRLMEnvironment(...),\n}\n</code></pre>"},{"location":"core/runner/#run_task","title":"<code>run_task()</code>","text":"<p>The core execution method. Runs one RLM episode and persists the trajectory as JSONL.</p> <pre><code>def run_task(\n    self,\n    task: str,\n    max_steps: int = 4,\n    exec_timeout: int = 30,\n    environment: str = \"generic\",\n    sub_model: str | None = None,\n    sub_provider: str | None = None,\n    branch_width: int = 1,\n    framework: str | None = None,\n    max_depth: int = 2,\n    max_children_per_step: int = 4,\n    parallelism: int = 2,\n    time_budget_seconds: int | None = None,\n) -&gt; RLMRunResult:\n</code></pre> Parameter Type Default Description <code>task</code> <code>str</code> required Task description for the LLM <code>max_steps</code> <code>int</code> <code>4</code> Maximum iterations before forced stop <code>exec_timeout</code> <code>int</code> <code>30</code> Timeout in seconds per code execution <code>environment</code> <code>str</code> <code>\"generic\"</code> Environment to use (see registry above) <code>sub_model</code> <code>str \\| None</code> <code>None</code> Override model for sub-LLM calls <code>sub_provider</code> <code>str \\| None</code> <code>None</code> Override provider for sub-LLM calls <code>branch_width</code> <code>int</code> <code>1</code> Number of candidate actions per step (best-of-N) <code>framework</code> <code>str \\| None</code> <code>None</code> Framework adapter ID (<code>\"dspy\"</code>, <code>\"pydantic_ai\"</code>, <code>\"google_adk\"</code>) <code>max_depth</code> <code>int</code> <code>2</code> Maximum recursion depth for delegate actions <code>max_children_per_step</code> <code>int</code> <code>4</code> Maximum child tasks per delegate action <code>parallelism</code> <code>int</code> <code>2</code> Concurrent child execution limit <code>time_budget_seconds</code> <code>int \\| None</code> <code>None</code> Global time budget (kills execution if exceeded) <p>Execution Loop:</p> <ol> <li>Build planner prompt from environment, memory, and trajectory</li> <li>Propose <code>branch_width</code> candidate actions via LLM</li> <li>Select highest-scoring candidate</li> <li>Execute action (code execution, file operation, delegate, or final)</li> <li>Calculate reward with <code>RLMRewardProfile</code> and apply global scaling</li> <li>Update memory (rolling window of last 8 notes)</li> <li>Persist step as JSONL event</li> <li>Emit runtime events for observability</li> <li>Repeat until <code>done=True</code> or <code>max_steps</code> reached</li> </ol> <p>Cycle Guard</p> <p>Recursive delegate tasks are protected by a cycle guard. If a child task has the same fingerprint (task + environment hash) as an ancestor, it is immediately skipped with reward <code>-0.25</code>.</p> <p>Example:</p> <pre><code>result = runner.run_task(\n    task=\"Create a DSPy Signature for sentiment analysis\",\n    environment=\"dspy\",\n    max_steps=6,\n    exec_timeout=60,\n    branch_width=3,  # Best-of-3 candidate selection\n)\n\nprint(f\"Run ID: {result.run_id}\")\nprint(f\"Completed: {result.completed}\")\nprint(f\"Steps: {result.steps}\")\nprint(f\"Total Reward: {result.total_reward}\")\nprint(f\"Answer: {result.final_response[:200]}\")\n</code></pre>"},{"location":"core/runner/#run_benchmark","title":"<code>run_benchmark()</code>","text":"<p>Execute a benchmark preset and persist aggregate summary.</p> <pre><code>def run_benchmark(\n    self,\n    *,\n    preset: str = \"dspy_quick\",\n    limit: int | None = None,\n    environment: str | None = None,\n    framework: str | None = None,\n    max_steps: int | None = None,\n    exec_timeout: int | None = None,\n    branch_width: int = 1,\n    sub_model: str | None = None,\n    sub_provider: str | None = None,\n    pack_paths: list[str | Path] | None = None,\n) -&gt; RLMBenchmarkResult:\n</code></pre> <p>Iterates over all cases in the specified preset, running each through <code>run_task()</code> and collecting metrics. Results are persisted as JSON summaries in the benchmarks directory.</p> <p>Example:</p> <pre><code>bench = runner.run_benchmark(\n    preset=\"dspy_quick\",\n    limit=5,\n    environment=\"dspy\",\n    max_steps=4,\n)\nprint(f\"Completed: {bench.completed_cases}/{bench.total_cases}\")\nprint(f\"Avg Reward: {bench.avg_reward}\")\nprint(f\"Avg Steps: {bench.avg_steps}\")\n</code></pre>"},{"location":"core/runner/#compare_benchmarks","title":"<code>compare_benchmarks()</code>","text":"<p>Compare candidate benchmark against baseline with CI-style gate pass/fail.</p> <pre><code>def compare_benchmarks(\n    self,\n    *,\n    candidate: str = \"latest\",\n    baseline: str = \"previous\",\n    min_reward_delta: float = 0.0,\n    min_completion_delta: float = 0.0,\n    max_steps_increase: float = 0.0,\n    fail_on_completion_regression: bool = True,\n) -&gt; RLMBenchmarkComparison:\n</code></pre> <p>Computes deltas for reward, completion rate, and step count. Detects per-case regressions. Returns a <code>passed</code> boolean suitable for CI gates.</p>"},{"location":"core/runner/#run_chat_turn","title":"<code>run_chat_turn()</code>","text":"<p>Run one persistent chat turn backed by RLM episodes. Manages session state across turns with automatic memory compaction.</p> <pre><code>def run_chat_turn(\n    self,\n    message: str,\n    session_id: str = \"default\",\n    *,\n    environment: str = \"generic\",\n    max_steps: int = 4,\n    enable_compaction: bool = True,\n    compaction_limit: int = 6,\n    keep_recent: int = 4,\n    # ... additional parameters\n) -&gt; RLMRunResult:\n</code></pre>"},{"location":"core/runner/#doctor","title":"<code>doctor()</code>","text":"<p>Run readiness checks for RLM execution.</p> <pre><code>def doctor(self, environment: str = \"generic\") -&gt; list[EnvironmentDoctorCheck]:\n</code></pre> <p>Checks include:</p> <ul> <li>Run directory writability</li> <li>Sandbox runtime health</li> <li>Model connection status</li> <li>Framework adapter availability</li> <li>Environment-specific checks (workdir, pytest, DSPy imports)</li> </ul>"},{"location":"core/runner/#other-methods","title":"Other Methods","text":"Method Description <code>list_runs(limit=10)</code> List recent RLM runs from persisted JSONL trajectories <code>get_run_status(run_id)</code> Get summarized status for one run <code>load_run_events(run_id)</code> Load raw JSONL events for one run <code>visualize_run(run_id)</code> Build nested visualization payload <code>supported_environments()</code> List available environment aliases <code>supported_frameworks()</code> List available framework adapter IDs <code>benchmark_presets()</code> List available benchmark preset metadata <code>benchmark_pack_aliases()</code> List bundled benchmark pack aliases on disk <code>list_benchmark_runs(limit=20)</code> List recent benchmark summaries <code>get_chat_session(session_id)</code> Get chat session metadata <code>reset_chat_session(session_id)</code> Delete persisted chat session <code>observability_status()</code> Get configured observability sink statuses"},{"location":"core/runner/#rlmrunresult","title":"<code>RLMRunResult</code>","text":"<p>Dataclass returned by <code>run_task()</code>.</p> <pre><code>@dataclass(slots=True)\nclass RLMRunResult:\n    run_id: str                           # Unique run identifier\n    run_path: Path                        # Path to JSONL trajectory file\n    completed: bool                       # Whether the task completed successfully\n    steps: int                            # Number of steps executed\n    total_reward: float                   # Cumulative reward across all steps\n    final_response: str                   # Final answer or synthesized response\n    started_at: str                       # ISO timestamp of run start\n    finished_at: str                      # ISO timestamp of run end\n    environment: str                      # Environment name used\n    task: str                             # Original task description\n    usage_summary: dict[str, int] | None  # Token usage (total_calls, prompt_tokens, completion_tokens)\n</code></pre>"},{"location":"core/runner/#rlmbenchmarkresult","title":"<code>RLMBenchmarkResult</code>","text":"<p>Dataclass returned by <code>run_benchmark()</code>.</p> <pre><code>@dataclass(slots=True)\nclass RLMBenchmarkResult:\n    benchmark_id: str                  # Unique benchmark identifier\n    summary_path: Path                 # Path to JSON summary file\n    preset: str                        # Preset name used\n    started_at: str                    # ISO timestamp\n    finished_at: str                   # ISO timestamp\n    total_cases: int                   # Total benchmark cases\n    completed_cases: int               # Cases that completed successfully\n    avg_reward: float                  # Average reward across cases\n    avg_steps: float                   # Average steps across cases\n    case_results: list[dict[str, Any]] # Per-case result dictionaries\n</code></pre> <p>Each entry in <code>case_results</code> contains:</p> Field Type Description <code>case_id</code> <code>str</code> Unique case identifier <code>description</code> <code>str</code> Human-readable case description <code>task</code> <code>str</code> Task text <code>environment</code> <code>str</code> Environment used <code>run_id</code> <code>str</code> RLM run ID for this case <code>completed</code> <code>bool</code> Whether the case completed <code>steps</code> <code>int</code> Steps taken <code>total_reward</code> <code>float</code> Cumulative reward <code>usage</code> <code>dict</code> Token usage <code>final_response</code> <code>str</code> Final answer"},{"location":"core/runner/#rlmbenchmarkcomparison","title":"<code>RLMBenchmarkComparison</code>","text":"<p>Dataclass returned by <code>compare_benchmarks()</code>.</p> <pre><code>@dataclass(slots=True)\nclass RLMBenchmarkComparison:\n    candidate_id: str                    # Candidate benchmark ID\n    baseline_id: str                     # Baseline benchmark ID\n    candidate_path: Path                 # Path to candidate JSON\n    baseline_path: Path                  # Path to baseline JSON\n    candidate_metrics: dict[str, float]  # avg_reward, completion_rate, avg_steps\n    baseline_metrics: dict[str, float]   # avg_reward, completion_rate, avg_steps\n    deltas: dict[str, float]             # Metric deltas (candidate - baseline)\n    case_summary: dict[str, int]         # common_cases, completion_regressions, reward_regressions\n    gates: dict[str, bool]               # Gate pass/fail for each criterion\n    passed: bool                         # True if ALL gates passed\n</code></pre>"},{"location":"core/runner/#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>The runner publishes events at every stage of execution through the <code>RLMEventBus</code>:</p> Event When Published <code>run_start</code> Beginning of <code>run_task()</code> <code>step_start</code> Before each action execution <code>step_end</code> After each action execution, with reward <code>run_end</code> End of <code>run_task()</code>, with final metrics <code>run_cycle_guard</code> When a recursive task is blocked by cycle detection <p>All events include <code>run_id</code>, <code>depth</code>, and <code>parent_run_id</code> for tracing recursive execution trees.</p>"},{"location":"core/runner/#reward-calculation","title":"Reward Calculation","text":"<p>Every action result passes through:</p> <ol> <li>Environment reward -- computed by the environment based on execution outcome</li> <li>Global scaling -- <code>reward_profile.apply_global_scale(reward)</code> multiplies by <code>global_scale</code> and clamps to <code>[-1.0, 1.0]</code></li> <li>Accumulation -- added to <code>total_reward</code> for the run</li> </ol> <p>See Environments for the full <code>RLMRewardProfile</code> specification.</p>"},{"location":"core/runner/#delegate-actions-recursive-execution","title":"Delegate Actions (Recursive Execution)","text":"<p>When the planner proposes a <code>delegate</code> or <code>delegate_batch</code> action, the runner:</p> <ol> <li>Checks depth against <code>max_depth</code> guard</li> <li>Resolves context references from the <code>LazyFileContext</code> store</li> <li>Spawns child <code>run_task()</code> calls (potentially in parallel)</li> <li>Aggregates child results into a single <code>EnvironmentActionResult</code></li> <li>Applies cycle detection via task fingerprinting</li> </ol> <pre><code>result = runner.run_task(\n    task=\"Decompose this large analysis into subtasks\",\n    environment=\"dspy\",\n    max_depth=3,\n    max_children_per_step=4,\n    parallelism=2,\n    time_budget_seconds=300,\n)\n</code></pre>"},{"location":"core/runner/#runtime-health-detection","title":"Runtime Health Detection","text":"<p>The <code>doctor()</code> method performs comprehensive readiness checks:</p> <pre><code>checks = runner.doctor(environment=\"dspy\")\nfor check in checks:\n    print(f\"[{check.status}] {check.name}: {check.detail}\")\n    if check.recommendation:\n        print(f\"  Recommendation: {check.recommendation}\")\n</code></pre> <p>Output example: <pre><code>[pass] rlm_run_dir: Run directory: /project/.rlm_code/rlm/runs\n[pass] sandbox_runtime: local: Python sandbox available\n[pass] model_connection: Connected model: gpt-4o\n[pass] workdir_exists: Workdir exists: /project\n[pass] pytest_cli: pytest available at /usr/bin/pytest\n[pass] dspy_import: DSPy import check passed.\n</code></pre></p>"},{"location":"core/termination/","title":"Termination Patterns","text":"<p>Module</p> <p><code>rlm_code.rlm.termination</code></p> <p>The termination module implements the <code>FINAL()</code> and <code>FINAL_VAR()</code> patterns from the RLM paper. These patterns provide a clean, structured mechanism for the LLM to signal task completion and return results from within the REPL execution loop.</p>"},{"location":"core/termination/#overview","title":"Overview","text":"<p>In the RLM paradigm, the LLM operates inside an iterative REPL loop: it reasons, writes code, observes output, and repeats. The termination module answers the critical question: how does the LLM signal that it is done?</p> <p>Two complementary patterns are provided:</p> Pattern Purpose Use Case <code>FINAL(answer)</code> Return a direct answer value Simple answers, strings, dicts <code>FINAL_VAR(\"name\")</code> Return the value of a REPL variable Large computed results, DataFrames, complex objects <p>Both patterns work by raising a <code>FinalOutput</code> exception, which the runner catches to cleanly exit the REPL loop without requiring special return-value plumbing.</p> <pre><code>graph LR\n    A[LLM generates code] --&gt; B{Code contains FINAL?}\n    B --&gt;|FINAL\\(answer\\)| C[FinalOutput exception raised]\n    B --&gt;|FINAL_VAR\\(name\\)| D[FinalOutput exception raised]\n    B --&gt;|No| E[Continue REPL loop]\n    C --&gt; F[Runner catches exception]\n    D --&gt; F\n    F --&gt; G[Extract answer / resolve variable]\n    G --&gt; H[Return result]</code></pre>"},{"location":"core/termination/#functions","title":"Functions","text":""},{"location":"core/termination/#finalanswer","title":"<code>FINAL(answer)</code>","text":"<p>Signal completion with a direct answer value. This immediately terminates the RLM loop.</p> <pre><code>from rlm_code.rlm.termination import FINAL\n\n# Direct string answer\nFINAL(\"The answer is 42\")\n\n# Dictionary answer\nFINAL({\"sentiment\": \"positive\", \"confidence\": 0.95})\n\n# List answer\nFINAL([\"item1\", \"item2\", \"item3\"])\n</code></pre> Parameter Type Description <code>answer</code> <code>Any</code> The final answer to return. Can be any Python value. <p>Returns: <code>NoReturn</code> -- this function always raises <code>FinalOutput</code>.</p> <p>The <code>answer</code> parameter is wrapped in a dict as <code>{\"answer\": answer, \"type\": \"direct\"}</code> inside the <code>FinalOutput</code> exception.</p> <p>When to use FINAL vs FINAL_VAR</p> <p>Use <code>FINAL()</code> when the answer is a short, self-contained value that fits naturally as a function argument. Use <code>FINAL_VAR()</code> when the answer is a large computed object already stored in a variable -- this avoids duplicating it in the function call.</p>"},{"location":"core/termination/#final_varvariable_name","title":"<code>FINAL_VAR(variable_name)</code>","text":"<p>Signal completion by referencing a variable in the REPL namespace. The runner resolves the variable value after catching the exception.</p> <pre><code>from rlm_code.rlm.termination import FINAL_VAR\n\n# In REPL code generated by the LLM:\nresult = analyze_document(context)\nsummary = result[\"summary\"]\nFINAL_VAR(\"summary\")\n</code></pre> Parameter Type Description <code>variable_name</code> <code>str</code> Name of the variable in the REPL namespace to return. <p>Returns: <code>NoReturn</code> -- this function always raises <code>FinalOutput</code>.</p> <p>The variable name is wrapped as <code>{\"var\": variable_name, \"type\": \"variable\"}</code> inside the <code>FinalOutput</code> exception. The runner then calls <code>resolve_final_var()</code> to retrieve the actual value from the REPL namespace.</p> <p>Variable Must Exist</p> <p>The referenced variable must exist in the REPL namespace at the time of resolution. If it does not, a <code>KeyError</code> is raised with a helpful message listing the available variables.</p>"},{"location":"core/termination/#detect_final_in_texttext","title":"<code>detect_final_in_text(text)</code>","text":"<p>Detect <code>FINAL()</code> or <code>FINAL_VAR()</code> patterns in LLM response text (not in executable code). This handles cases where the LLM writes the termination call in its natural-language response rather than inside a code block.</p> <pre><code>from rlm_code.rlm.termination import detect_final_in_text\n\n# Detect in LLM prose\nresult = detect_final_in_text('Based on my analysis, FINAL(\"42\")')\nassert result.detected is True\nassert result.final_type == \"direct\"\nassert result.content == \"42\"\n\n# Detect variable reference\nresult = detect_final_in_text('I have stored the answer. FINAL_VAR(\"result\")')\nassert result.detected is True\nassert result.final_type == \"variable\"\nassert result.content == \"result\"\n\n# No termination signal\nresult = detect_final_in_text(\"Let me continue analyzing...\")\nassert result.detected is False\n</code></pre> Parameter Type Description <code>text</code> <code>str</code> LLM response text to scan. <p>Returns: <code>FinalDetection</code> dataclass.</p> <p>Uses multiple regex patterns to handle various formatting styles:</p> <p>FINAL patterns:</p> Pattern Example Match <code>FINAL\\s*\\(\\s*(.+?)\\s*\\)</code> <code>FINAL(The answer is 42)</code> <code>FINAL\\s*\\(\\s*[\"\\'](.+?)[\"\\']\\s*\\)</code> <code>FINAL(\"The answer is 42\")</code> <code>FINAL\\s*\\(\\s*\"\"\"(.+?)\"\"\"\\s*\\)</code> <code>FINAL(\"\"\"Multi-line answer\"\"\")</code> <p>FINAL_VAR patterns:</p> Pattern Example Match <code>FINAL_VAR\\s*\\(\\s*['\\\"]?(\\w+)['\\\"]?\\s*\\)</code> <code>FINAL_VAR(result)</code> <code>FINAL_VAR\\s*\\(\\s*[\"\\'](\\w+)[\"\\']\\s*\\)</code> <code>FINAL_VAR(\"result\")</code> <p>Detection Priority</p> <p><code>FINAL_VAR</code> patterns are checked before <code>FINAL</code> patterns because <code>FINAL_VAR</code> is the more specific match. This prevents <code>FINAL_VAR(\"x\")</code> from being incorrectly parsed as a <code>FINAL()</code> call with <code>VAR(\"x\")</code> as the argument.</p>"},{"location":"core/termination/#detect_final_in_codecode","title":"<code>detect_final_in_code(code)</code>","text":"<p>Detect if executable code contains <code>FINAL()</code> or <code>FINAL_VAR()</code> calls. This is used to anticipate termination before actually executing the code.</p> <pre><code>from rlm_code.rlm.termination import detect_final_in_code\n\n# Detect in code\nresult = detect_final_in_code('answer = 42\\nFINAL(answer)')\nassert result.detected is True\nassert result.final_type == \"direct\"\n\n# Detect variable reference in code\nresult = detect_final_in_code('FINAL_VAR(\"my_result\")')\nassert result.detected is True\nassert result.final_type == \"variable\"\nassert result.content == \"my_result\"\n</code></pre> Parameter Type Description <code>code</code> <code>str</code> Python code string to scan. <p>Returns: <code>FinalDetection</code> dataclass.</p> <p>This function uses word-boundary-aware patterns (<code>\\b</code>) to avoid false positives from variable names like <code>FINALIZE</code> or <code>FINAL_VALUE</code>.</p> <p>Content for Direct FINAL</p> <p>When <code>detect_final_in_code()</code> finds a <code>FINAL(</code> call (without <code>_VAR</code>), <code>content</code> is set to <code>None</code> because the actual answer value can only be determined at runtime after execution. For <code>FINAL_VAR()</code> calls, the variable name is extracted statically.</p>"},{"location":"core/termination/#resolve_final_varvariable_name-namespace","title":"<code>resolve_final_var(variable_name, namespace)</code>","text":"<p>Resolve a <code>FINAL_VAR</code> reference by looking up the variable in the REPL namespace.</p> <pre><code>from rlm_code.rlm.termination import resolve_final_var\n\nnamespace = {\"result\": 42, \"data\": [1, 2, 3]}\n\nvalue = resolve_final_var(\"result\", namespace)\nassert value == 42\n\n# Missing variable raises KeyError with helpful message\ntry:\n    resolve_final_var(\"missing_var\", namespace)\nexcept KeyError as e:\n    print(e)\n    # \"FINAL_VAR referenced variable 'missing_var' not found in REPL namespace.\n    #  Available variables: ['result', 'data']\"\n</code></pre> Parameter Type Description <code>variable_name</code> <code>str</code> Name of the variable to resolve. <code>namespace</code> <code>dict[str, Any]</code> The REPL's locals/globals namespace. <p>Returns: The value of the variable.</p> <p>Raises: <code>KeyError</code> if the variable is not found (includes a list of available variables in the error message).</p>"},{"location":"core/termination/#extract_code_blockstext-languagerepl","title":"<code>extract_code_blocks(text, language=\"repl\")</code>","text":"<p>Extract code blocks from LLM response text. Looks for markdown-style fenced code blocks with the specified language tag.</p> <pre><code>from rlm_code.rlm.termination import extract_code_blocks\n\nresponse = \"\"\"\nLet me analyze the data.\n\n```python\nresult = sum(data)\nprint(result)\n```\n\nAnd then finalize:\n\n```repl\nFINAL(result)\n```\n\"\"\"\n\nblocks = extract_code_blocks(response)\n# Returns: ['result = sum(data)\\nprint(result)', 'FINAL(result)']\n</code></pre> Parameter Type Default Description <code>text</code> <code>str</code> required LLM response text containing code blocks. <code>language</code> <code>str</code> <code>\"repl\"</code> Primary language tag to look for. <p>Returns: <code>list[str]</code> -- list of code strings extracted from code blocks.</p> <p>Search order:</p> <ol> <li>Fenced blocks with the specified language tag (<code>```repl</code> or <code>```python</code>)</li> <li>If none found, untagged code blocks (<code>```</code>) that look like Python code</li> </ol> <p>Heuristic for untagged blocks: Must contain at least one of <code>import</code>, <code>def</code>, <code>class</code>, <code>print(</code>, or <code>=</code>.</p> <p>Fallback Behavior</p> <p>If no code blocks with the specified language tag (or <code>python</code>) are found, the function falls back to untagged code blocks and applies the Python keyword heuristic. This handles cases where the LLM omits the language tag from its code blocks.</p>"},{"location":"core/termination/#format_final_answeranswer","title":"<code>format_final_answer(answer)</code>","text":"<p>Format a final answer for display or return. Handles various answer types with appropriate formatting.</p> <pre><code>from rlm_code.rlm.termination import format_final_answer\n\n# String passthrough\nassert format_final_answer(\"hello\") == \"hello\"\n\n# Dict with \"answer\" key -- extracts the value\nassert format_final_answer({\"answer\": 42}) == \"42\"\n\n# Dict without \"answer\" key -- JSON formatted\nresult = format_final_answer({\"key\": \"value\", \"count\": 10})\n# Returns: '{\\n  \"key\": \"value\",\\n  \"count\": 10\\n}'\n\n# List -- joined by newlines\nassert format_final_answer([\"line1\", \"line2\"]) == \"line1\\nline2\"\n\n# Other types -- str() conversion\nassert format_final_answer(42) == \"42\"\n</code></pre> Parameter Type Description <code>answer</code> <code>Any</code> The final answer to format. <p>Returns: <code>str</code> -- the formatted answer.</p> Input Type Formatting Behavior <code>str</code> Returned as-is <code>dict</code> with <code>\"answer\"</code> key Returns <code>str(answer[\"answer\"])</code> <code>dict</code> without <code>\"answer\"</code> key JSON-formatted with 2-space indent <code>list</code> Items joined by newlines Other <code>str()</code> conversion"},{"location":"core/termination/#classes","title":"Classes","text":""},{"location":"core/termination/#finaloutput","title":"<code>FinalOutput</code>","text":"<p>Control-flow exception raised when <code>FINAL()</code> or <code>FINAL_VAR()</code> is called. This follows the pattern from DSPy's RLM implementation where termination is handled as an exception to cleanly exit the REPL execution loop.</p> <pre><code>from rlm_code.rlm.termination import FinalOutput, FINAL\n\n# Typically not instantiated directly -- use FINAL() or FINAL_VAR()\ntry:\n    FINAL(\"The answer is 42\")\nexcept FinalOutput as e:\n    print(e.output)\n    # {\"answer\": \"The answer is 42\", \"type\": \"direct\"}\n</code></pre> Attribute Type Description <code>output</code> <code>dict[str, Any]</code> Dictionary containing the answer or variable reference. <p>The <code>output</code> dictionary has two possible shapes:</p> Key Value (FINAL) Value (FINAL_VAR) <code>\"answer\"</code> The direct answer value not present <code>\"var\"</code> not present The variable name string <code>\"type\"</code> <code>\"direct\"</code> <code>\"variable\"</code> <p>Design Pattern</p> <p>Using an exception for termination provides a clean way to exit from arbitrarily nested code execution -- even from inside helper functions, loops, or <code>llm_query()</code> callbacks. It means the LLM does not need to structure its code with explicit return paths.</p>"},{"location":"core/termination/#finaldetection","title":"<code>FinalDetection</code>","text":"<p>Dataclass holding the result of detecting <code>FINAL</code>/<code>FINAL_VAR</code> patterns in text or code. Uses <code>@dataclass(slots=True)</code> for memory efficiency.</p> <pre><code>from rlm_code.rlm.termination import FinalDetection\n\n# Constructed by detect_final_in_text() and detect_final_in_code()\ndetection = FinalDetection(\n    detected=True,\n    final_type=\"direct\",\n    content=\"42\",\n    raw_match='FINAL(\"42\")',\n)\n</code></pre> Field Type Default Description <code>detected</code> <code>bool</code> required Whether a termination pattern was found. <code>final_type</code> <code>str \\| None</code> <code>None</code> <code>\"direct\"</code> for <code>FINAL()</code>, <code>\"variable\"</code> for <code>FINAL_VAR()</code>. <code>content</code> <code>str \\| None</code> <code>None</code> The extracted answer or variable name. <code>raw_match</code> <code>str \\| None</code> <code>None</code> The full matched pattern string from the regex."},{"location":"core/termination/#regex-pattern-reference","title":"Regex Pattern Reference","text":"<p>The module defines two sets of compiled regex patterns for flexible detection of termination calls across different LLM formatting styles.</p>"},{"location":"core/termination/#final_patterns","title":"<code>FINAL_PATTERNS</code>","text":"<p>Three patterns for detecting <code>FINAL(answer)</code>:</p> Pattern Flags Matches <code>FINAL\\s*\\(\\s*(.+?)\\s*\\)(?:\\s*$\\|\\n)</code> <code>DOTALL \\| MULTILINE</code> Multiline <code>FINAL(...)</code> <code>FINAL\\s*\\(\\s*[\"\\'](.+?)[\"\\']\\s*\\)</code> <code>DOTALL</code> <code>FINAL(\"quoted string\")</code> <code>FINAL\\s*\\(\\s*\"\"\"(.+?)\"\"\"\\s*\\)</code> <code>DOTALL</code> <code>FINAL(\"\"\"triple quoted\"\"\")</code>"},{"location":"core/termination/#final_var_patterns","title":"<code>FINAL_VAR_PATTERNS</code>","text":"<p>Two patterns for detecting <code>FINAL_VAR(name)</code>:</p> Pattern Matches <code>FINAL_VAR\\s*\\(\\s*['\\\"]?(\\w+)['\\\"]?\\s*\\)</code> <code>FINAL_VAR(name)</code> or <code>FINAL_VAR(\"name\")</code> <code>FINAL_VAR\\s*\\(\\s*[\"\\'](\\w+)[\"\\']\\s*\\)</code> <code>FINAL_VAR(\"name\")</code> or <code>FINAL_VAR('name')</code> <p>Whitespace Tolerance</p> <p>All patterns tolerate arbitrary whitespace around parentheses and arguments, so <code>FINAL( answer )</code>, <code>FINAL (answer)</code>, and <code>FINAL(answer)</code> all match correctly.</p>"},{"location":"core/termination/#end-to-end-flow","title":"End-to-End Flow","text":"<p>Here is how termination works in a complete RLM execution:</p> <pre><code>LLM Response\n    |\n    v\nextract_code_blocks() --&gt; [\"code with FINAL(...)\"]\n    |\n    v\ndetect_final_in_code() --&gt; FinalDetection(detected=True)\n    |\n    v\nexec(code, namespace) --&gt; raises FinalOutput\n    |\n    v\nCaught by PureRLMEnvironment._execute_code()\n    |\n    v\nCheck final_output[\"type\"]:\n    |\n    +-- \"direct\" --&gt; format_final_answer(answer)\n    |\n    +-- \"variable\" --&gt; resolve_final_var(var_name, namespace)\n                           --&gt; format_final_answer(value)\n    |\n    v\nEnvironmentActionResult(done=True, final_response=answer)\n</code></pre> <p>If the LLM does not use code blocks but writes <code>FINAL()</code> directly in text:</p> <pre><code>LLM Response Text\n    |\n    v\ndetect_final_in_text() --&gt; FinalDetection(detected=True, content=\"answer\")\n    |\n    v\nEnvironmentActionResult(done=True, final_response=\"answer\")\n</code></pre>"},{"location":"core/termination/#integration-with-the-runner","title":"Integration with the Runner","text":"<p>The termination module is imported and used by the <code>PureRLMEnvironment</code> to manage the complete execution lifecycle:</p> <pre><code>from rlm_code.rlm.termination import (\n    FINAL, FINAL_VAR, FinalOutput,\n    detect_final_in_code, detect_final_in_text,\n    extract_code_blocks, format_final_answer,\n    resolve_final_var,\n)\n\n# Inside the REPL execution loop:\ntry:\n    exec(code, namespace)\nexcept FinalOutput as e:\n    if e.output[\"type\"] == \"variable\":\n        # Resolve the variable from namespace\n        answer = resolve_final_var(e.output[\"var\"], namespace)\n    else:\n        # Direct answer\n        answer = e.output[\"answer\"]\n\n    formatted = format_final_answer(answer)\n    # Return formatted answer as the run result\n</code></pre> <p>Exception-Based Control Flow</p> <p>The <code>FINAL()</code> / <code>FINAL_VAR()</code> mechanism uses exceptions for control flow. This is intentional: it provides a clean way to exit arbitrarily deep call stacks within the REPL without requiring cooperative return-value threading. However, bare <code>except Exception</code> clauses in REPL code will catch <code>FinalOutput</code> and prevent termination. The <code>PureRLMEnvironment</code> handles this by catching <code>FinalOutput</code> at the outermost execution boundary.</p>"},{"location":"core/termination/#examples","title":"Examples","text":""},{"location":"core/termination/#basic-task-completion","title":"Basic Task Completion","text":"<pre><code># LLM-generated REPL code:\ndata = [1, 2, 3, 4, 5]\ntotal = sum(data)\naverage = total / len(data)\nFINAL(f\"The average is {average}\")\n</code></pre>"},{"location":"core/termination/#variable-based-completion","title":"Variable-Based Completion","text":"<pre><code># LLM-generated REPL code for large output:\nimport json\n\nresults = {}\nfor key in context.keys():\n    results[key] = analyze(context[key])\n\n# Store in variable to avoid token-window bloat\nfinal_report = json.dumps(results, indent=2)\nFINAL_VAR(\"final_report\")\n</code></pre>"},{"location":"core/termination/#multi-step-analysis-with-sub-llm","title":"Multi-Step Analysis with Sub-LLM","text":"<pre><code># LLM-generated REPL code with recursive LLM calls:\nchunks = [context[i:i+1000] for i in range(0, len(context), 1000)]\nsummaries = []\n\nfor chunk in chunks:\n    summary = llm_query(f\"Summarize: {chunk}\")\n    summaries.append(summary)\n\ncombined = \"\\n\".join(summaries)\nfinal_answer = llm_query(f\"Synthesize these summaries: {combined}\")\nFINAL(final_answer)\n</code></pre>"},{"location":"core/termination/#conditional-termination","title":"Conditional Termination","text":"<pre><code># LLM-generated REPL code with conditional FINAL:\nword_count = len(context.split())\n\nif word_count &lt; 100:\n    FINAL(\"Document too short for meaningful analysis\")\n\n# If we get here, document is long enough\nanalysis = perform_detailed_analysis(context)\nFINAL_VAR(\"analysis\")\n</code></pre>"},{"location":"core/trajectory/","title":"Trajectory Logging","text":"<p>Module</p> <p><code>rlm_code.rlm.trajectory</code></p> <p>Trajectory logging provides JSONL-based execution tracing for RLM runs. It records every phase of execution -- reasoning, code, output, LLM calls, child agents, and termination -- in a format compatible with agent evaluation frameworks and visualization tools.</p>"},{"location":"core/trajectory/#overview","title":"Overview","text":"<p>Every RLM run produces a trajectory: a chronological sequence of events capturing what the LLM thought, what code it wrote, what output it received, and how it arrived at its final answer. Trajectories are stored as JSONL (one JSON object per line) for streaming-friendly append-only writes.</p> <pre><code>traces/run_001.jsonl\n{\"event_type\": \"run_start\", \"timestamp\": 1706400000.0, \"run_id\": \"run_001\", \"data\": {\"task\": \"...\"}}\n{\"event_type\": \"iteration_reasoning\", \"timestamp\": 1706400001.2, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"reasoning\": \"...\"}}\n{\"event_type\": \"iteration_code\", \"timestamp\": 1706400001.5, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"code\": \"...\"}}\n{\"event_type\": \"iteration_output\", \"timestamp\": 1706400002.1, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"output\": \"...\"}, \"duration_ms\": 600}\n{\"event_type\": \"final_detected\", \"timestamp\": 1706400005.0, \"run_id\": \"run_001\", \"data\": {\"answer\": \"...\"}}\n{\"event_type\": \"run_end\", \"timestamp\": 1706400005.1, \"run_id\": \"run_001\", \"data\": {\"success\": true}, \"duration_ms\": 5100}\n</code></pre>"},{"location":"core/trajectory/#classes","title":"Classes","text":""},{"location":"core/trajectory/#trajectoryeventtype","title":"<code>TrajectoryEventType</code>","text":"<p>Enumeration of 18 event types for trajectory logging.</p> <pre><code>class TrajectoryEventType(str, Enum):\n    # Run lifecycle\n    RUN_START = \"run_start\"\n    RUN_END = \"run_end\"\n\n    # REPL iterations\n    ITERATION_START = \"iteration_start\"\n    ITERATION_REASONING = \"iteration_reasoning\"\n    ITERATION_CODE = \"iteration_code\"\n    ITERATION_OUTPUT = \"iteration_output\"\n    ITERATION_END = \"iteration_end\"\n\n    # LLM calls\n    LLM_REQUEST = \"llm_request\"\n    LLM_RESPONSE = \"llm_response\"\n\n    # Sub-LLM (llm_query from code)\n    SUB_LLM_REQUEST = \"sub_llm_request\"\n    SUB_LLM_RESPONSE = \"sub_llm_response\"\n\n    # Child agents\n    CHILD_SPAWN = \"child_spawn\"\n    CHILD_RESULT = \"child_result\"\n\n    # Termination\n    FINAL_DETECTED = \"final_detected\"\n\n    # Context\n    CONTEXT_LOAD = \"context_load\"\n    CONTEXT_UPDATE = \"context_update\"\n\n    # Memory\n    MEMORY_COMPACT = \"memory_compact\"\n\n    # Errors\n    ERROR = \"error\"\n</code></pre> Category Event Types Run lifecycle <code>RUN_START</code>, <code>RUN_END</code> Iterations <code>ITERATION_START</code>, <code>ITERATION_REASONING</code>, <code>ITERATION_CODE</code>, <code>ITERATION_OUTPUT</code>, <code>ITERATION_END</code> LLM calls <code>LLM_REQUEST</code>, <code>LLM_RESPONSE</code> Sub-LLM <code>SUB_LLM_REQUEST</code>, <code>SUB_LLM_RESPONSE</code> Child agents <code>CHILD_SPAWN</code>, <code>CHILD_RESULT</code> Termination <code>FINAL_DETECTED</code> Context <code>CONTEXT_LOAD</code>, <code>CONTEXT_UPDATE</code> Memory <code>MEMORY_COMPACT</code> Errors <code>ERROR</code>"},{"location":"core/trajectory/#trajectoryevent","title":"<code>TrajectoryEvent</code>","text":"<p>A single event in a trajectory.</p> <pre><code>@dataclass\nclass TrajectoryEvent:\n    event_type: TrajectoryEventType          # Event category\n    timestamp: float = field(...)            # Unix timestamp (time.time())\n    run_id: str = \"\"                         # Run identifier\n    iteration: int | None = None             # Iteration number\n    depth: int = 0                           # Recursion depth (0 = root)\n    parent_id: str | None = None             # Parent agent ID\n\n    # Event-specific data\n    data: dict[str, Any] = field(...)        # Arbitrary event payload\n\n    # Metrics\n    tokens_in: int | None = None             # Input tokens\n    tokens_out: int | None = None            # Output tokens\n    duration_ms: float | None = None         # Duration in milliseconds\n</code></pre> Field Type Description <code>event_type</code> <code>TrajectoryEventType</code> Category of the event <code>timestamp</code> <code>float</code> Unix timestamp (from <code>time.time()</code>) <code>run_id</code> <code>str</code> Correlates events within a single run <code>iteration</code> <code>int \\| None</code> Which iteration this event belongs to <code>depth</code> <code>int</code> Recursion depth (0 for root agent, 1+ for children) <code>parent_id</code> <code>str \\| None</code> ID of the parent agent (for child events) <code>data</code> <code>dict[str, Any]</code> Event-specific payload <code>tokens_in</code> <code>int \\| None</code> Input token count <code>tokens_out</code> <code>int \\| None</code> Output token count <code>duration_ms</code> <code>float \\| None</code> Duration in milliseconds"},{"location":"core/trajectory/#serialization","title":"Serialization","text":"<pre><code>event = TrajectoryEvent(\n    event_type=TrajectoryEventType.ITERATION_CODE,\n    run_id=\"run_abc123\",\n    iteration=2,\n    data={\"code\": \"print(len(context))\"},\n)\n\nd = event.to_dict()\n# {\"event_type\": \"iteration_code\", \"timestamp\": 1706400001.5,\n#  \"run_id\": \"run_abc123\", \"iteration\": 2,\n#  \"data\": {\"code\": \"print(len(context))\"}}\n</code></pre>"},{"location":"core/trajectory/#deserialization","title":"Deserialization","text":"<pre><code>event = TrajectoryEvent.from_dict({\n    \"event_type\": \"iteration_output\",\n    \"timestamp\": 1706400002.1,\n    \"run_id\": \"run_abc123\",\n    \"iteration\": 2,\n    \"data\": {\"output\": \"45230\"},\n    \"duration_ms\": 15.3,\n})\n</code></pre>"},{"location":"core/trajectory/#trajectorylogger","title":"<code>TrajectoryLogger</code>","text":"<p>JSONL trajectory logger for RLM execution. Provides convenience methods for logging every event type.</p> <pre><code>class TrajectoryLogger:\n    def __init__(\n        self,\n        output_path: str | Path,\n        run_id: str | None = None,\n        metadata: dict[str, Any] | None = None,\n    ):\n</code></pre> Parameter Type Default Description <code>output_path</code> <code>str \\| Path</code> required Path to the JSONL output file <code>run_id</code> <code>str \\| None</code> Auto-generated Run identifier (default: <code>run_{timestamp_ms}</code>) <code>metadata</code> <code>dict[str, Any] \\| None</code> <code>None</code> Arbitrary metadata stored with <code>run_start</code> <p>Directory Creation</p> <p>The logger automatically creates parent directories if they do not exist.</p>"},{"location":"core/trajectory/#context-manager-support","title":"Context Manager Support","text":"<pre><code>with TrajectoryLogger(\"traces/run_001.jsonl\") as logger:\n    logger.log_run_start(task=\"Analyze data\")\n    # ... log events ...\n    logger.log_run_end(success=True, answer=\"42\")\n</code></pre>"},{"location":"core/trajectory/#logging-methods","title":"Logging Methods","text":""},{"location":"core/trajectory/#log_eventevent","title":"<code>log_event(event)</code>","text":"<p>Log a raw <code>TrajectoryEvent</code>. Automatically sets <code>run_id</code>, <code>iteration</code>, <code>depth</code>, and <code>parent_id</code> from logger state.</p>"},{"location":"core/trajectory/#log_run_starttask-context_lengthnone-modelnone","title":"<code>log_run_start(task, context_length=None, model=None)</code>","text":"<p>Log the beginning of a run.</p> <pre><code>logger.log_run_start(\n    task=\"Analyze sentiment of customer reviews\",\n    context_length=45230,\n    model=\"gpt-4o\",\n)\n</code></pre>"},{"location":"core/trajectory/#log_run_endsuccess-answernone-total_tokensnone-duration_secondsnone","title":"<code>log_run_end(success, answer=None, total_tokens=None, duration_seconds=None)</code>","text":"<p>Log the end of a run.</p> <pre><code>logger.log_run_end(\n    success=True,\n    answer=\"Overall sentiment is positive (87% confidence)\",\n    total_tokens=8523,\n)\n</code></pre>"},{"location":"core/trajectory/#log_iteration_startiteration","title":"<code>log_iteration_start(iteration)</code>","text":"<p>Log the start of an iteration. Updates the logger's current iteration counter.</p>"},{"location":"core/trajectory/#log_iterationiteration-reasoning-code-output-duration_msnone-tokens_usednone","title":"<code>log_iteration(iteration, reasoning, code, output, duration_ms=None, tokens_used=None)</code>","text":"<p>Convenience method to log a complete iteration (reasoning + code + output) in one call.</p> <pre><code>logger.log_iteration(\n    iteration=1,\n    reasoning=\"First, explore the data structure\",\n    code='print(f\"Length: {len(context)}\")',\n    output=\"Length: 45230\",\n    duration_ms=15.3,\n)\n</code></pre>"},{"location":"core/trajectory/#log_llm_callprompt-response-tokens_innone-tokens_outnone-duration_msnone-is_sub_llmfalse","title":"<code>log_llm_call(prompt, response, tokens_in=None, tokens_out=None, duration_ms=None, is_sub_llm=False)</code>","text":"<p>Log an LLM call (root or sub-LLM).</p> <pre><code># Root LLM call\nlogger.log_llm_call(\n    prompt=\"Analyze this data...\",\n    response=\"The data shows...\",\n    tokens_in=500,\n    tokens_out=200,\n    duration_ms=1200,\n)\n\n# Sub-LLM call from llm_query()\nlogger.log_llm_call(\n    prompt=\"Summarize this chunk...\",\n    response=\"This chunk discusses...\",\n    is_sub_llm=True,\n)\n</code></pre> <p>Response Truncation</p> <p>Responses longer than 1,000 characters are truncated in the log to keep file sizes manageable.</p>"},{"location":"core/trajectory/#log_child_spawnchild_id-task-depth","title":"<code>log_child_spawn(child_id, task, depth)</code>","text":"<p>Log a child agent being spawned.</p>"},{"location":"core/trajectory/#log_child_resultchild_id-result-success","title":"<code>log_child_result(child_id, result, success)</code>","text":"<p>Log a child agent's result. Results are truncated to 500 characters.</p>"},{"location":"core/trajectory/#log_finalanswer","title":"<code>log_final(answer)</code>","text":"<p>Log final answer detection.</p>"},{"location":"core/trajectory/#log_context_loadcontext_type-length-previewnone","title":"<code>log_context_load(context_type, length, preview=None)</code>","text":"<p>Log context being loaded. Preview is truncated to 200 characters.</p>"},{"location":"core/trajectory/#log_errorerror-tracebacknone","title":"<code>log_error(error, traceback=None)</code>","text":"<p>Log an error with optional traceback.</p>"},{"location":"core/trajectory/#depth-management","title":"Depth Management","text":"<p>For recursive/child agent tracing:</p> <pre><code>logger.push_depth(\"child_agent_001\")  # depth becomes 1\n# ... log child events ...\nlogger.pop_depth()                     # depth returns to 0\n</code></pre>"},{"location":"core/trajectory/#close","title":"<code>close()</code>","text":"<p>Close the underlying file handle. Called automatically by the context manager.</p>"},{"location":"core/trajectory/#trajectoryviewer","title":"<code>TrajectoryViewer</code>","text":"<p>Viewer for trajectory JSONL files. Provides visualization and analysis.</p> <pre><code>class TrajectoryViewer:\n    def __init__(self, trajectory_path: str | Path):\n</code></pre> <p>Events are loaded from the JSONL file on construction. Invalid lines are silently skipped.</p>"},{"location":"core/trajectory/#methods","title":"Methods","text":""},{"location":"core/trajectory/#events-listtrajectoryevent","title":"<code>events() -&gt; list[TrajectoryEvent]</code>","text":"<p>Get all loaded events.</p>"},{"location":"core/trajectory/#iterations-iteratorlisttrajectoryevent","title":"<code>iterations() -&gt; Iterator[list[TrajectoryEvent]]</code>","text":"<p>Yield events grouped by iteration number.</p> <pre><code>viewer = TrajectoryViewer(\"traces/run_001.jsonl\")\nfor iteration_events in viewer.iterations():\n    print(f\"Iteration with {len(iteration_events)} events\")\n</code></pre>"},{"location":"core/trajectory/#summary-dictstr-any","title":"<code>summary() -&gt; dict[str, Any]</code>","text":"<p>Get a comprehensive trajectory summary.</p> <pre><code>summary = viewer.summary()\n</code></pre> <p>Returns:</p> Key Type Description <code>run_id</code> <code>str</code> Run identifier <code>task</code> <code>str</code> Original task <code>success</code> <code>bool</code> Whether the run succeeded <code>answer</code> <code>str</code> Final answer <code>total_events</code> <code>int</code> Total event count <code>total_iterations</code> <code>int</code> Number of unique iterations <code>max_depth</code> <code>int</code> Maximum recursion depth reached <code>total_tokens_in</code> <code>int</code> Sum of all input tokens <code>total_tokens_out</code> <code>int</code> Sum of all output tokens <code>total_tokens</code> <code>int</code> <code>total_tokens_in + total_tokens_out</code> <code>total_duration_ms</code> <code>float</code> Sum of all event durations <code>event_counts</code> <code>dict[str, int]</code> Count of each event type"},{"location":"core/trajectory/#format_tree-str","title":"<code>format_tree() -&gt; str</code>","text":"<p>Format the trajectory as a human-readable tree visualization.</p> <pre><code>print(viewer.format_tree())\n</code></pre> <p>Example output:</p> <pre><code>Trajectory: run_abc123\nTask: Analyze sentiment of customer reviews...\nStatus: SUCCESS\n\n[Iteration 1]\n  THINK: First, let me explore the structure of the context dat...\n  CODE: print(f\"Context length: {len(context)} chars\")...\n  OUTPUT: Context length: 45230 chars... (15ms)\n[Iteration 2]\n  THINK: Now let me process chunks using llm_query_batched...\n  CODE: chunks = [context[i:i+10000] for i in range(0, len(cont...\n  OUTPUT: Processed 5 chunks successfully... (3200ms)\n  -&gt; SUB_LLM_CALL\n[Iteration 3]\n  THINK: Aggregate summaries and produce final answer...\n  CODE: final = llm_query(f\"Combine: {summaries}\")...\n  FINAL: Overall sentiment is positive (87% confidence)...\n\nSummary: 3 iterations, 8523 tokens, 4500ms\n</code></pre>"},{"location":"core/trajectory/#export_htmloutput_path","title":"<code>export_html(output_path)</code>","text":"<p>Export the trajectory as an interactive HTML file with a dark theme, collapsible iterations, and syntax-highlighted code blocks.</p> <pre><code>viewer.export_html(\"traces/run_001.html\")\n</code></pre> <p>The HTML includes:</p> <ul> <li>Header with run metadata and summary metrics (status, iterations, tokens, duration, depth)</li> <li>Timeline with collapsible iteration sections</li> <li>Color-coded events (reasoning in orange, code in monospace, output in green, finals in teal, errors in red)</li> <li>Click-to-expand iterations (first iteration expanded by default)</li> <li>Sub-LLM and child events indented to show nesting</li> </ul>"},{"location":"core/trajectory/#standalone-functions","title":"Standalone Functions","text":""},{"location":"core/trajectory/#load_trajectorypath-trajectoryviewer","title":"<code>load_trajectory(path) -&gt; TrajectoryViewer</code>","text":"<p>Convenience function to load a trajectory file for viewing.</p> <pre><code>from rlm_code.rlm.trajectory import load_trajectory\n\nviewer = load_trajectory(\"traces/run_001.jsonl\")\nprint(viewer.summary())\n</code></pre>"},{"location":"core/trajectory/#compare_trajectoriespaths-dictstr-any","title":"<code>compare_trajectories(paths) -&gt; dict[str, Any]</code>","text":"<p>Compare multiple trajectory files and compute aggregate statistics.</p> <pre><code>from rlm_code.rlm.trajectory import compare_trajectories\n\ncomparison = compare_trajectories([\n    \"traces/run_001.jsonl\",\n    \"traces/run_002.jsonl\",\n    \"traces/run_003.jsonl\",\n])\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"trajectories\": [\n        {\n            \"path\": \"traces/run_001.jsonl\",\n            \"run_id\": \"run_001\",\n            \"task\": \"Analyze sentiment...\",\n            \"success\": True,\n            \"iterations\": 3,\n            \"tokens\": 8523,\n            \"duration_ms\": 4500,\n        },\n        # ... more trajectories ...\n    ],\n    \"comparison\": {\n        \"avg_iterations\": 3.7,\n        \"avg_tokens\": 9100,\n        \"avg_duration_ms\": 5200,\n        \"success_rate\": 0.67,  # 2 out of 3 succeeded\n    },\n}\n</code></pre>"},{"location":"core/trajectory/#jsonl-format-specification","title":"JSONL Format Specification","text":"<p>Each line in a trajectory JSONL file is a valid JSON object with the following structure:</p>"},{"location":"core/trajectory/#required-fields","title":"Required Fields","text":"Field Type Description <code>event_type</code> <code>string</code> One of the <code>TrajectoryEventType</code> values <code>timestamp</code> <code>float</code> Unix timestamp <code>run_id</code> <code>string</code> Run identifier"},{"location":"core/trajectory/#optional-fields","title":"Optional Fields","text":"Field Type Description <code>iteration</code> <code>integer</code> Iteration number <code>depth</code> <code>integer</code> Recursion depth (omitted when 0) <code>parent_id</code> <code>string</code> Parent agent ID <code>data</code> <code>object</code> Event-specific payload <code>tokens_in</code> <code>integer</code> Input tokens <code>tokens_out</code> <code>integer</code> Output tokens <code>duration_ms</code> <code>float</code> Duration in milliseconds"},{"location":"core/trajectory/#example-complete-trajectory","title":"Example Complete Trajectory","text":"<pre><code>{\"event_type\": \"run_start\", \"timestamp\": 1706400000.0, \"run_id\": \"run_001\", \"data\": {\"task\": \"Analyze sentiment\", \"context_length\": 45230, \"model\": \"gpt-4o\"}}\n{\"event_type\": \"iteration_start\", \"timestamp\": 1706400000.1, \"run_id\": \"run_001\", \"iteration\": 1}\n{\"event_type\": \"iteration_reasoning\", \"timestamp\": 1706400001.0, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"reasoning\": \"Explore context structure\"}}\n{\"event_type\": \"iteration_code\", \"timestamp\": 1706400001.1, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"code\": \"print(len(context))\"}}\n{\"event_type\": \"iteration_output\", \"timestamp\": 1706400001.5, \"run_id\": \"run_001\", \"iteration\": 1, \"data\": {\"output\": \"45230\"}, \"duration_ms\": 15}\n{\"event_type\": \"sub_llm_request\", \"timestamp\": 1706400002.0, \"run_id\": \"run_001\", \"iteration\": 2, \"data\": {\"prompt\": \"Summarize...\"}, \"tokens_in\": 500}\n{\"event_type\": \"sub_llm_response\", \"timestamp\": 1706400003.5, \"run_id\": \"run_001\", \"iteration\": 2, \"data\": {\"response\": \"This discusses...\"}, \"tokens_out\": 200, \"duration_ms\": 1500}\n{\"event_type\": \"final_detected\", \"timestamp\": 1706400005.0, \"run_id\": \"run_001\", \"iteration\": 3, \"data\": {\"answer\": \"Sentiment is positive\"}}\n{\"event_type\": \"run_end\", \"timestamp\": 1706400005.1, \"run_id\": \"run_001\", \"data\": {\"success\": true, \"answer\": \"Sentiment is positive\", \"total_iterations\": 3}, \"duration_ms\": 5100}\n</code></pre>"},{"location":"core/trajectory/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code>from rlm_code.rlm.trajectory import TrajectoryLogger, TrajectoryViewer\n\n# --- Logging ---\nwith TrajectoryLogger(\"traces/my_run.jsonl\", metadata={\"model\": \"gpt-4o\"}) as logger:\n    logger.log_run_start(task=\"Analyze data\", context_length=50000)\n\n    logger.log_iteration(\n        iteration=1,\n        reasoning=\"Explore data structure\",\n        code='print(type(context), len(context))',\n        output=\"&lt;class 'str'&gt; 50000\",\n        duration_ms=12,\n    )\n\n    logger.log_llm_call(\n        prompt=\"Summarize the first section...\",\n        response=\"The first section covers...\",\n        tokens_in=600,\n        tokens_out=150,\n        duration_ms=1100,\n        is_sub_llm=True,\n    )\n\n    logger.log_final(\"Analysis complete: found 3 key themes\")\n    logger.log_run_end(success=True, answer=\"3 key themes\", total_tokens=2500)\n\n# --- Viewing ---\nviewer = TrajectoryViewer(\"traces/my_run.jsonl\")\nprint(viewer.format_tree())\nviewer.export_html(\"traces/my_run.html\")\n\nsummary = viewer.summary()\nprint(f\"Run {summary['run_id']}: {summary['total_iterations']} iterations, \"\n      f\"{summary['total_tokens']} tokens\")\n</code></pre>"},{"location":"getting-started/","title":"\ud83d\ude80 Getting Started","text":"<p>Welcome to RLM Code, the Research Playground and Evaluation OS for Recursive Language Model (RLM) agentic systems. RLM Code provides a unified TUI-based development environment for building, benchmarking, and optimizing agent workflows through slash commands and natural language.</p>"},{"location":"getting-started/#what-is-rlm-code","title":"\ud83e\uddea What is RLM Code?","text":"<p>RLM Code implements the Recursive Language Model paradigm from the 2025 \"Recursive Language Models\" paper. It extends the paper's concepts with:</p> <ul> <li>\ud83e\udde0 Context-as-variable: Context is stored as a REPL variable rather than in the token window, enabling unbounded output and token-efficient processing</li> <li>\ud83d\udd01 Deep recursion: Support for recursion depth &gt; 1, exceeding the paper's original limitation</li> <li>\ud83d\udd00 Multi-paradigm execution: Pure RLM, CodeAct, and Traditional paradigms with side-by-side comparison</li> <li>\ud83d\udcca Pluggable observability: MLflow, OpenTelemetry, LangSmith, LangFuse, and Logfire integrations</li> <li>\ud83d\udce6 Sandbox runtimes: Local, Docker, Apple Container, Modal, E2B, and Daytona execution environments</li> </ul>"},{"location":"getting-started/#problem-focus","title":"\ud83c\udfaf Problem Focus","text":"<p>RLM (the method) addresses long-context reasoning. RLM Code is the tooling layer for researchers who want to implement, evaluate, and operate that workflow.</p> <p>RLM Code is optimized for workflows where:</p> <ul> <li>Context is too large to fit comfortably in one prompt.</li> <li>You need programmatic inspection and decomposition instead of full-context prompt injection.</li> <li>You want to compare recursive symbolic execution against harness-style and direct baselines under the same benchmark suite.</li> </ul> <p>For detailed mode behavior and neutral tradeoff guidance, see Execution Patterns.</p>"},{"location":"getting-started/#where-to-go-next","title":"\ud83d\udcda Where to Go Next","text":"Guide Description \ud83e\udded Start Here (Simple) Plain-language onboarding: what this is, what to install, and safe first run \ud83d\udce6 Installation System requirements, package installation, optional dependencies, and verification \u26a1 Quick Start Launch the TUI, connect a model, run your first benchmark, explore the Research tab \ud83e\uddd1\u200d\ud83d\udd2c Researcher Onboarding Researcher-first workflows and complete command handbook \ud83d\udcbb CLI Reference Complete reference for the entry point and all 50+ slash commands \u2699\ufe0f Configuration Full <code>rlm_config.yaml</code> schema, environment variables, and ConfigManager API"},{"location":"getting-started/#quick-overview","title":"\u26a1 Quick Overview","text":"<pre><code># Install\npip install rlm-code\n\n# Launch the unified TUI\nrlm-code\n\n# Connect to a model and run a benchmark\n/connect anthropic claude-opus-4-6\n/rlm bench preset=dspy_quick\n/rlm bench compare candidate=latest baseline=previous\n</code></pre> <p>\ud83c\udd95 First Time?</p> <p>Start with the \ud83d\udce6 Installation guide to set up your environment, then follow the \u26a1 Quick Start for a hands-on walkthrough.</p> <p>\ud83d\udda5\ufe0f Unified TUI</p> <p>RLM Code ships a single TUI with 5 tabs: \ud83d\udd01 RLM, \ud83d\udcc1 Files, \ud83d\udccb Details, \u26a1 Shell, and \ud83d\udd2c Research. Use <code>rlm-code</code> to launch, and press <code>Ctrl+5</code> to access the Research tab for experiment tracking, benchmarks, and session replay.</p>"},{"location":"getting-started/cli/","title":"CLI Reference","text":"<p>RLM Code ships one CLI entry point (<code>rlm-code</code>) and a rich slash-command surface inside the TUI. This page reflects the current implementation in <code>rlm_code/main.py</code>, <code>rlm_code/ui/tui_app.py</code>, and <code>rlm_code/commands/slash_commands.py</code>.</p>"},{"location":"getting-started/cli/#entry-point","title":"Entry Point","text":""},{"location":"getting-started/cli/#rlm-code","title":"<code>rlm-code</code>","text":"<p>Launch the unified TUI (RLM, Files, Details, Shell, Research).</p> <pre><code>rlm-code [OPTIONS]\n</code></pre> Flag Short Description <code>--verbose</code> <code>-v</code> Enable verbose logs <code>--debug</code> Enable debug mode with full traceback <code>--version</code> Print version info and exit <code>--skip-safety-check</code> Skip startup directory safety checks (hidden/internal) <p>Examples:</p> <pre><code>rlm-code\nrlm-code -v\nrlm-code --debug\nrlm-code --version\n</code></pre>"},{"location":"getting-started/cli/#startup-safety-checks","title":"Startup Safety Checks","text":"<p>On startup, RLM Code checks the working directory:</p> <ul> <li>Warns when run from home/personal folders.</li> <li>Blocks high-risk system directories.</li> <li>Recommends running from a dedicated project workspace.</li> </ul>"},{"location":"getting-started/cli/#command-surfaces","title":"Command Surfaces","text":"<p>RLM Code has two command layers in the TUI:</p> <ol> <li>TUI-native commands in <code>tui_app.py</code> (layout/navigation/shell shortcuts).</li> <li>Full slash handler commands in <code>slash_commands.py</code> (RLM, sandbox, MCP, exports, etc.).</li> </ol>"},{"location":"getting-started/cli/#tui-native-commands","title":"TUI-native commands","text":"Command Description <code>/help</code> Show in-TUI quick help <code>/workflow</code> Show recommended RLM workflow <code>/connect</code> Open interactive connect picker (<code>/connect acp</code> opens ACP picker) <code>/models</code> Show model/provider status <code>/status</code> Refresh status panel <code>/snapshot [file]</code> Snapshot file/project baseline <code>/diff [file]</code> Diff against snapshot <code>/view &lt;chat\\|files\\|details\\|shell\\|research\\|next\\|prev&gt;</code> Switch active view (<code>chat</code> route opens the RLM tab) <code>/layout &lt;single\\|multi&gt;</code> Toggle one-screen vs multi-pane layout <code>/pane &lt;files\\|details\\|shell&gt; [show\\|hide\\|toggle]</code> Pane visibility <code>/focus &lt;chat\\|default&gt;</code> Focus controls <code>/copy</code> Copy last assistant response <code>/shell [command]</code> Open shell tab or run command in shell tab <code>/rml ...</code> Alias for <code>/rlm ...</code> <code>/exit</code> or <code>/quit</code> Exit TUI <p>Shell shortcuts:</p> <ul> <li><code>!&lt;cmd&gt;</code>: run command inline in chat.</li> <li><code>&gt;&lt;cmd&gt;</code>: alternate inline shell shortcut.</li> </ul>"},{"location":"getting-started/cli/#full-slash-commands","title":"Full Slash Commands","text":""},{"location":"getting-started/cli/#core","title":"Core","text":"Command Description <code>/init</code> Initialize project config and scan workspace <code>/connect [provider model [api-key] [base-url]]</code> Connect directly, or run with no args for interactive picker <code>/models</code> Model/provider listing <code>/status</code> Connection + runtime status <code>/disconnect</code> Disconnect current model <code>/help</code> Command help <code>/intro</code> Intro guide <code>/clear</code> Clear conversation <code>/history</code> Show conversation history <code>/exit</code> Exit"},{"location":"getting-started/cli/#rlm","title":"RLM","text":"Command Description <code>/rlm run &lt;task&gt; ...</code> Run one RLM episode <code>/rlm bench ...</code> Run benchmark preset/list/pack <code>/rlm bench compare ...</code> Candidate vs baseline gate compare <code>/rlm bench validate ... [--json]</code> CI-style gate output <code>/rlm bench report ...</code> Export compare report (<code>markdown</code>/<code>csv</code>/<code>json</code>) <code>/rlm import-evals pack=...</code> Preview external eval packs <code>/rlm judge pred=... ref=...</code> LLM-judge predictions <code>/rlm frameworks</code> Adapter readiness table <code>/rlm viz [run_id\\|latest]</code> Trajectory tree visualization <code>/rlm status [run_id]</code> Run status <code>/rlm abort [run_id\\|all]</code> Cooperative cancel for active runs <code>/rlm replay &lt;run_id&gt;</code> Replay stored trajectory <code>/rlm doctor [env=...] [--json]</code> Environment diagnostics <code>/rlm chat &lt;message&gt; ...</code> Persistent RLM chat sessions <code>/rlm chat status [session=name]</code> Chat memory stats <code>/rlm chat reset [session=name]</code> Reset chat memory <code>/rlm observability</code> Observability sink status"},{"location":"getting-started/cli/#harness","title":"Harness","text":"Command Description <code>/harness tools [mcp=on\\|off]</code> List harness tools (local + optional MCP) <code>/harness doctor</code> Harness tool coverage report <code>/harness run &lt;task&gt; [steps=N] [mcp=on\\|off] [tools=name[,name2]]</code> Run tool-driven coding loop"},{"location":"getting-started/cli/#sandbox","title":"Sandbox","text":"Command Description <code>/sandbox status</code> Runtime health + pure RLM backend status <code>/sandbox doctor</code> Detailed runtime diagnostics <code>/sandbox use &lt;runtime&gt;</code> Switch runtime (<code>local</code>, <code>docker</code>, <code>apple-container</code>, <code>modal</code>, <code>e2b</code>, <code>daytona</code>) <code>/sandbox profile &lt;secure\\|dev\\|custom&gt;</code> Apply superbox runtime policy preset <code>/sandbox backend &lt;exec\\|monty\\|docker&gt; [ack=I_UNDERSTAND_EXEC_IS_UNSAFE]</code> Set Pure RLM backend <code>/sandbox strict &lt;on\\|off&gt;</code> Toggle pure RLM strict mode <code>/sandbox output-mode &lt;truncate\\|summarize\\|metadata&gt;</code> Control REPL output compaction <code>/sandbox apple &lt;on\\|off&gt;</code> Enable/disable Apple container runtime gate <p>Notes:</p> <ul> <li><code>backend=exec</code> requires explicit ack token and sets unsafe opt-in.</li> <li><code>profile secure</code> applies Docker-first strict defaults for research/production.</li> </ul>"},{"location":"getting-started/cli/#execution-and-validation","title":"Execution and Validation","text":"Command Description <code>/run [timeout=N]</code> Execute generated code <code>/validate [file]</code> Validate code quality/safety patterns <code>/test [file]</code> Run tests"},{"location":"getting-started/cli/#mcp","title":"MCP","text":"Command Description <code>/mcp-servers</code> List configured servers <code>/mcp-connect &lt;server&gt;</code> Connect server <code>/mcp-disconnect &lt;server&gt;</code> Disconnect server <code>/mcp-tools [server]</code> List tools <code>/mcp-call &lt;server&gt; &lt;tool&gt; [args]</code> Call tool <code>/mcp-resources [server]</code> List resources <code>/mcp-read &lt;server&gt; &lt;uri&gt;</code> Read resource <code>/mcp-prompts [server]</code> List prompts <code>/mcp-prompt &lt;server&gt; &lt;name&gt; [args]</code> Fetch prompt"},{"location":"getting-started/cli/#sessions-export-and-project","title":"Sessions, Export, and Project","text":"Command Description <code>/sessions</code> List sessions <code>/session ...</code> Save/load/delete session state <code>/save [file]</code> Save latest output/code <code>/export ...</code> Export session/package/config/conversation <code>/import ...</code> Import session/config <code>/project ...</code> Project info commands <code>/save-data [file]</code> Save generated data"},{"location":"getting-started/cli/#templates-and-dspy-helpers","title":"Templates and DSPy helpers","text":"Command Description <code>/demo</code> Demo workflows <code>/eval</code> Generate/evaluate code <code>/optimize</code>, <code>/optimize-start</code>, <code>/optimize-status</code>, <code>/optimize-cancel</code>, <code>/optimize-resume</code> GEPA optimization flow <code>/examples</code> Template examples <code>/predictors</code>, <code>/adapters</code>, <code>/retrievers</code> DSPy component references <code>/async</code>, <code>/streaming</code>, <code>/data</code>, <code>/explain</code>, <code>/reference</code> DSPy/usage help"},{"location":"getting-started/cli/#environment-variables-common","title":"Environment Variables (Common)","text":"Variable Purpose <code>OPENAI_API_KEY</code> OpenAI model auth <code>ANTHROPIC_API_KEY</code> Anthropic model auth <code>GEMINI_API_KEY</code> / <code>GOOGLE_API_KEY</code> Gemini model auth <p>TUI behavior tunables include:</p> <ul> <li><code>RLM_TUI_HISTORY_ITEMS</code></li> <li><code>RLM_TUI_HISTORY_ITEM_CHARS</code></li> <li><code>RLM_TUI_HISTORY_TOTAL_CHARS</code></li> <li><code>RLM_TUI_THINK_TICK</code></li> <li><code>RLM_TUI_EVENT_FLUSH_SECONDS</code></li> <li><code>RLM_TUI_EVENT_BATCH_LIMIT</code></li> <li><code>RLM_TUI_ACP_DISCOVERY_TIMEOUT_SECONDS</code></li> <li><code>RLM_TUI_ACP_CACHE_TTL_SECONDS</code></li> <li><code>RLM_TUI_HARNESS_AUTO</code></li> <li><code>RLM_TUI_HARNESS_AUTO_MCP</code></li> <li><code>RLM_TUI_HARNESS_AUTO_STEPS</code></li> <li><code>RLM_TUI_HARNESS_PREVIEW_STEPS</code></li> <li><code>RLM_TUI_INPUT_DEBOUNCE_SECONDS</code></li> <li><code>RLM_TUI_CHAT_MAX_LINES</code></li> <li><code>RLM_TUI_TOOL_MAX_LINES</code></li> <li><code>RLM_TUI_EVENT_MAX_LINES</code></li> </ul> <p>See Configuration for full config schema.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>RLM Code uses a project config file (<code>rlm_config.yaml</code>) managed by <code>ConfigManager</code> in <code>rlm_code.core.config</code>. This page documents the current typed schema and runtime behavior.</p>"},{"location":"getting-started/configuration/#file-resolution","title":"File Resolution","text":"<p>RLM Code loads config in this order:</p> <ol> <li><code>rlm_config.yaml</code> (primary)</li> <li><code>dspy_config.yaml</code> (legacy fallback)</li> <li>Built-in defaults (if neither file exists)</li> </ol> <p><code>/init</code> creates <code>rlm_config.yaml</code> with a minimal, commented template.</p>"},{"location":"getting-started/configuration/#minimal-recommended-config","title":"Minimal Recommended Config","text":"<pre><code>name: my-project\n\ndefault_model: gpt-5.3-codex\n\nmodels:\n  openai_api_key: null\n  openai_model: gpt-5.3-codex\n\nsandbox:\n  runtime: docker\n  superbox_profile: secure\n  pure_rlm_backend: docker\n  pure_rlm_strict: true\n  pure_rlm_allow_unsafe_exec: false\n\nrlm:\n  default_benchmark_preset: dspy_quick\n</code></pre>"},{"location":"getting-started/configuration/#full-project-schema-rlm_configyaml","title":"Full Project Schema (<code>rlm_config.yaml</code>)","text":""},{"location":"getting-started/configuration/#top-level","title":"Top-level","text":"Key Type Default <code>name</code> <code>str</code> project directory name <code>version</code> <code>str</code> <code>0.1.0</code> <code>dspy_version</code> <code>str</code> <code>2.4.0</code> <code>default_model</code> <code>str \\| null</code> <code>null</code> <code>output_directory</code> <code>str</code> <code>generated</code> <code>template_preferences</code> <code>dict[str, Any]</code> <code>{}</code> <code>mcp_servers</code> <code>dict[str, dict[str, Any]]</code> <code>{}</code>"},{"location":"getting-started/configuration/#models","title":"<code>models</code>","text":"Key Type Default <code>ollama_endpoint</code> <code>str \\| null</code> <code>http://localhost:11434</code> <code>ollama_models</code> <code>list[str]</code> <code>[]</code> <code>anthropic_api_key</code> <code>str \\| null</code> <code>null</code> <code>anthropic_model</code> <code>str</code> <code>claude-opus-4-6</code> <code>openai_api_key</code> <code>str \\| null</code> <code>null</code> <code>openai_model</code> <code>str</code> <code>gpt-5.3-codex</code> <code>gemini_api_key</code> <code>str \\| null</code> <code>null</code> <code>gemini_model</code> <code>str</code> <code>gemini-2.5-flash</code> <code>reflection_model</code> <code>str \\| null</code> <code>null</code>"},{"location":"getting-started/configuration/#sandbox","title":"<code>sandbox</code>","text":"Key Type Default <code>runtime</code> <code>str</code> <code>local</code> <code>default_timeout_seconds</code> <code>int</code> <code>30</code> <code>memory_limit_mb</code> <code>int</code> <code>512</code> <code>allowed_mount_roots</code> <code>list[str]</code> <code>['.', '/tmp', '/var/folders', '/private/tmp', '/private/var/folders']</code> <code>env_allowlist</code> <code>list[str]</code> <code>[]</code> <code>superbox_profile</code> <code>str</code> <code>custom</code> <code>superbox_auto_fallback</code> <code>bool</code> <code>true</code> <code>superbox_fallback_runtimes</code> <code>list[str]</code> <code>['docker', 'apple-container', 'local']</code> <code>apple_container_enabled</code> <code>bool</code> <code>false</code> <code>pure_rlm_backend</code> <code>str</code> <code>docker</code> <code>pure_rlm_allow_unsafe_exec</code> <code>bool</code> <code>false</code> <code>pure_rlm_strict</code> <code>bool</code> <code>false</code> <code>pure_rlm_output_mode</code> <code>str</code> <code>summarize</code> <code>pure_rlm_max_iteration_output_chars</code> <code>int</code> <code>12000</code> <code>monty_type_check</code> <code>bool</code> <code>false</code> <code>monty_max_allocations</code> <code>int \\| null</code> <code>null</code> <code>monty_max_memory</code> <code>int \\| null</code> <code>null</code>"},{"location":"getting-started/configuration/#sandboxdocker","title":"<code>sandbox.docker</code>","text":"Key Type Default <code>image</code> <code>str</code> <code>python:3.11-slim</code> <code>memory_limit_mb</code> <code>int</code> <code>512</code> <code>cpus</code> <code>float \\| null</code> <code>1.0</code> <code>network_enabled</code> <code>bool</code> <code>false</code> <code>extra_args</code> <code>list[str]</code> <code>[]</code>"},{"location":"getting-started/configuration/#sandboxapple","title":"<code>sandbox.apple</code>","text":"Key Type Default <code>image</code> <code>str</code> <code>docker.io/library/python:3.11-slim</code> <code>memory_limit_mb</code> <code>int</code> <code>512</code> <code>cpus</code> <code>float \\| null</code> <code>1.0</code> <code>network_enabled</code> <code>bool</code> <code>false</code> <code>extra_args</code> <code>list[str]</code> <code>[]</code>"},{"location":"getting-started/configuration/#rlm","title":"<code>rlm</code>","text":"Key Type Default <code>default_benchmark_preset</code> <code>str</code> <code>dspy_quick</code> <code>benchmark_pack_paths</code> <code>list[str]</code> <code>[]</code> <code>reward</code> <code>RLMRewardConfig</code> defaults below"},{"location":"getting-started/configuration/#rlmreward","title":"<code>rlm.reward</code>","text":"Key Default <code>global_scale</code> <code>1.0</code> <code>run_python_base</code> <code>0.1</code> <code>run_python_success_bonus</code> <code>0.7</code> <code>run_python_failure_penalty</code> <code>0.3</code> <code>run_python_stderr_penalty</code> <code>0.1</code> <code>dspy_pattern_match_bonus</code> <code>0.03</code> <code>dspy_pattern_bonus_cap</code> <code>0.2</code> <code>verifier_base</code> <code>0.15</code> <code>verifier_score_weight</code> <code>0.5</code> <code>verifier_compile_bonus</code> <code>0.2</code> <code>verifier_compile_penalty</code> <code>0.35</code> <code>verifier_pytest_bonus</code> <code>0.25</code> <code>verifier_pytest_penalty</code> <code>0.25</code> <code>verifier_validation_bonus</code> <code>0.15</code> <code>verifier_validation_penalty</code> <code>0.3</code> <code>verifier_warning_penalty_per_warning</code> <code>0.03</code> <code>verifier_warning_penalty_cap</code> <code>0.15</code>"},{"location":"getting-started/configuration/#other-typed-sections","title":"Other typed sections","text":"<ul> <li><code>gepa_config</code>: optimization knobs (<code>max_iterations</code>, <code>population_size</code>, <code>mutation_rate</code>, <code>crossover_rate</code>, <code>evaluation_metric</code>).</li> <li><code>quality_scoring</code>: penalties and grade thresholds.</li> <li><code>retry_config</code>: retry/backoff strategy.</li> <li><code>cache_config</code>: generation cache settings.</li> </ul>"},{"location":"getting-started/configuration/#sandbox-profiles-and-security","title":"Sandbox Profiles and Security","text":"<p>Use <code>/sandbox profile</code> for quick policy presets:</p> <ul> <li><code>secure</code>: Docker-first + strict pure RLM defaults, no unsafe exec.</li> <li><code>dev</code>: Docker-first with local-friendly fallback chain.</li> <li><code>custom</code>: your manual values (set automatically when you change runtime/backend flags directly).</li> </ul> <p>Unsafe pure RLM backend is explicit opt-in:</p> <pre><code>/sandbox backend exec ack=I_UNDERSTAND_EXEC_IS_UNSAFE\n</code></pre> <p>Without the ack token, exec backend is rejected.</p>"},{"location":"getting-started/configuration/#runtime-notes","title":"Runtime Notes","text":"<ul> <li><code>sandbox.runtime</code> supports: <code>local</code>, <code>docker</code>, <code>apple-container</code>, <code>modal</code>, <code>e2b</code>, <code>daytona</code>.</li> <li>Runtime selection and fallback are resolved by <code>rlm_code.sandbox.superbox.Superbox</code>.</li> <li>Cloud runtime availability is determined by installed SDK/CLI + auth environment.</li> </ul>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#model-auth","title":"Model auth","text":"<ul> <li><code>OPENAI_API_KEY</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GEMINI_API_KEY</code> / <code>GOOGLE_API_KEY</code></li> </ul>"},{"location":"getting-started/configuration/#tui-behavior","title":"TUI behavior","text":"<ul> <li><code>RLM_TUI_HISTORY_ITEMS</code></li> <li><code>RLM_TUI_HISTORY_ITEM_CHARS</code></li> <li><code>RLM_TUI_HISTORY_TOTAL_CHARS</code></li> <li><code>RLM_TUI_THINK_TICK</code></li> <li><code>RLM_TUI_EVENT_FLUSH_SECONDS</code></li> <li><code>RLM_TUI_EVENT_BATCH_LIMIT</code></li> <li><code>RLM_TUI_ACP_DISCOVERY_TIMEOUT_SECONDS</code></li> <li><code>RLM_TUI_ACP_CACHE_TTL_SECONDS</code></li> <li><code>RLM_TUI_HARNESS_AUTO</code></li> <li><code>RLM_TUI_HARNESS_AUTO_MCP</code></li> <li><code>RLM_TUI_HARNESS_AUTO_STEPS</code></li> <li><code>RLM_TUI_HARNESS_PREVIEW_STEPS</code></li> <li><code>RLM_TUI_INPUT_DEBOUNCE_SECONDS</code></li> <li><code>RLM_TUI_CHAT_MAX_LINES</code></li> <li><code>RLM_TUI_TOOL_MAX_LINES</code></li> <li><code>RLM_TUI_EVENT_MAX_LINES</code></li> </ul>"},{"location":"getting-started/configuration/#programmatic-api","title":"Programmatic API","text":"<pre><code>from pathlib import Path\nfrom rlm_code.core.config import ConfigManager\n\nmanager = ConfigManager(Path.cwd())\nconfig = manager.config\n\nconfig.sandbox.superbox_profile = \"secure\"\nconfig.sandbox.runtime = \"docker\"\nconfig.sandbox.pure_rlm_backend = \"docker\"\n\nmanager.save_config(minimal=False)\n</code></pre> <p>Useful helpers:</p> <ul> <li><code>ConfigManager.is_project_initialized()</code></li> <li><code>ConfigManager.set_model_config(provider, **kwargs)</code></li> <li><code>ConfigManager.get_mcp_servers()</code> / <code>add_mcp_server()</code> / <code>remove_mcp_server()</code></li> </ul>"},{"location":"getting-started/configuration/#advanced-standalone-rlmyaml","title":"Advanced: Standalone <code>rlm.yaml</code>","text":"<p>RLM Code still includes <code>rlm_code.rlm.config_schema</code> (<code>RLMConfig</code>) for standalone RLM-engine config workflows. Use this only if you intentionally manage a separate <code>rlm.yaml</code> pipeline; the primary TUI/CLI path uses <code>rlm_config.yaml</code> from <code>core.config</code>.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers how to install RLM Code, its optional dependencies, and how to verify your installation.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"Requirement Minimum Recommended Python 3.11 3.12+ OS Linux, macOS, Windows macOS (Apple Silicon) or Linux Memory 2 GB 8 GB+ Disk 200 MB 1 GB+ (for traces and benchmark artifacts)"},{"location":"getting-started/installation/#install-uv","title":"Install uv","text":"<p>We recommend uv as the primary way to install and manage RLM Code.</p> <pre><code># macOS / Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or with Homebrew\nbrew install uv\n</code></pre> <p>Why uv?</p> <p><code>uv</code> is 10-100x faster than pip for dependency resolution. <code>uv tool install</code> creates an isolated environment for CLI tools - no virtualenv management needed. If you don't have Python 3.11+ installed, uv can install it for you:</p> <pre><code>uv python install 3.12\n</code></pre>"},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"uv tool install (Recommended)uv pip installpip <pre><code>uv tool install \"rlm-code[tui,llm-all]\"\n</code></pre> <p>This installs <code>rlm-code</code> as a globally available command in its own isolated environment. No virtualenv activation needed - just run <code>rlm-code</code> from anywhere.</p> <p>If you prefer to install into an existing virtual environment:</p> <pre><code>uv pip install \"rlm-code[tui,llm-all]\"\n</code></pre> <pre><code>pip install \"rlm-code[tui,llm-all]\"\n</code></pre> <p>This installs the core package, the TUI, and all LLM provider clients:</p> Dependency Purpose <code>click</code> &gt;= 8.0 CLI framework <code>dspy</code> &gt;= 3.0.4 DSPy integration <code>rich</code> &gt;= 13.7.0 Terminal formatting and panels <code>requests</code> &gt;= 2.28.0 HTTP client <code>pyyaml</code> &gt;= 6.0 YAML configuration parsing <code>mcp</code> &gt;= 1.2.1 Model Context Protocol support <code>anyio</code> &gt;= 4.5 Async I/O <code>httpx</code> &gt;= 0.27.1 HTTP/2 client <code>pydantic</code> &gt;= 2.11.0 Data validation <code>jsonschema</code> &gt;= 4.20.0 Schema validation <code>packaging</code> &gt;= 23.0 Version parsing <code>textual</code> &gt;= 0.86.0 Terminal UI framework (via <code>[tui]</code> extra) <code>openai</code> &gt;= 2.8.1 OpenAI client (via <code>[llm-all]</code> extra) <code>anthropic</code> &gt;= 0.39.0 Anthropic client (via <code>[llm-all]</code> extra) <code>google-genai</code> &gt;= 1.52.0 Gemini client (via <code>[llm-all]</code> extra)"},{"location":"getting-started/installation/#minimal-installation","title":"Minimal Installation","text":"<p>If you only need one LLM provider:</p> uv toolpip <pre><code># Core + TUI + Anthropic only\nuv tool install \"rlm-code[tui,anthropic]\"\n\n# Core + TUI + OpenAI only\nuv tool install \"rlm-code[tui,openai]\"\n\n# Core + TUI + Gemini only\nuv tool install \"rlm-code[tui,gemini]\"\n</code></pre> <pre><code>pip install \"rlm-code[tui,anthropic]\"\npip install \"rlm-code[tui,openai]\"\npip install \"rlm-code[tui,gemini]\"\n</code></pre> Extra Package Version <code>openai</code> <code>openai</code> &gt;= 2.8.1, &lt; 3.0 <code>anthropic</code> <code>anthropic</code> &gt;= 0.39.0, &lt; 1.0 <code>gemini</code> <code>google-genai</code> &gt;= 1.52.0, &lt; 2.0 <code>llm-all</code> All of the above --"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributors or those who want to run from source:</p> uv (Recommended)pip <pre><code>git clone https://github.com/SuperagenticAI/rlm-code.git\ncd rlm-code\nuv sync --all-extras\nuv run pytest\n</code></pre> <pre><code>git clone https://github.com/SuperagenticAI/rlm-code.git\ncd rlm-code\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev,tui,llm-all]\"\n</code></pre> <p>The <code>dev</code> extra installs:</p> Dependency Purpose <code>pytest</code> &gt;= 8.0 Test framework <code>pytest-cov</code> &gt;= 4.1 Coverage reporting <code>pytest-asyncio</code> &gt;= 0.23 Async test support <code>pytest-xdist</code> &gt;= 3.5 Parallel test execution <code>hypothesis</code> &gt;= 6.100 Property-based testing <code>ruff</code> &gt;= 0.8.0 Linting and formatting <code>mypy</code> &gt;= 1.13 Static type checking <code>pre-commit</code> &gt;= 4.0 Git hooks"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#runtime-backend-requirements","title":"Runtime Backend Requirements","text":"<p>Pick at least one secure backend before running serious experiments.</p> Backend Install Requirement Typical Use <code>docker</code> Install Docker Desktop / OrbStack / Colima Recommended secure default <code>monty</code> <code>pip install pydantic-monty</code> Local secure pure-RLM backend without Docker <code>apple-container</code> Install Apple's <code>container</code> CLI and verify <code>container system status</code> macOS-only experimental runtime"},{"location":"getting-started/installation/#observability-integrations","title":"Observability Integrations","text":"<p>If you installed with <code>uv tool install</code>, use <code>uv tool install --with</code> to add extras, or reinstall with additional extras:</p> <pre><code>uv tool install \"rlm-code[tui,llm-all,mlflow]\"\n</code></pre> MLflowOpenTelemetryLangSmithLangFuseLogfire <pre><code>uv tool install \"rlm-code[tui,llm-all,mlflow]\"\n</code></pre> <pre><code># If using uv tool, reinstall with the extra packages:\nuv tool install \"rlm-code[tui,llm-all]\" --with opentelemetry-api --with opentelemetry-sdk --with opentelemetry-exporter-otlp-proto-grpc\n</code></pre> <pre><code>uv tool install \"rlm-code[tui,llm-all]\" --with langsmith\n</code></pre> <pre><code>uv tool install \"rlm-code[tui,llm-all]\" --with langfuse\n</code></pre> <pre><code>uv tool install \"rlm-code[tui,llm-all]\" --with logfire\n</code></pre> Integration Package Environment Variable MLflow <code>mlflow</code> &gt;= 2.17.0 <code>MLFLOW_TRACKING_URI</code> OpenTelemetry <code>opentelemetry-sdk</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> LangSmith <code>langsmith</code> <code>LANGCHAIN_API_KEY</code> LangFuse <code>langfuse</code> <code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code> Logfire <code>logfire</code> <code>LOGFIRE_TOKEN</code>"},{"location":"getting-started/installation/#framework-adapters","title":"Framework Adapters","text":"<pre><code>uv tool install \"rlm-code[tui,llm-all,frameworks]\"\n</code></pre> Extra Package Purpose <code>pydantic</code> <code>pydantic-ai</code> &gt;= 0.4.0 Pydantic AI framework adapter <code>adk</code> <code>google-adk</code> &gt;= 1.12.0 Google Agent Development Kit adapter <code>frameworks</code> Both of the above All framework adapters"},{"location":"getting-started/installation/#mcp-websocket-transport","title":"MCP WebSocket Transport","text":"<pre><code>uv tool install \"rlm-code[tui,llm-all,mcp-ws]\"\n</code></pre> <p>Adds <code>websockets</code> &gt;= 15.0.1 for WebSocket-based MCP server transport.</p>"},{"location":"getting-started/installation/#docker-runtime","title":"Docker Runtime","text":"<p>Docker is used as a sandbox runtime for isolated code execution. No pip install is needed, but Docker must be available on the system:</p> <pre><code># macOS\nbrew install --cask docker\n\n# Linux (Ubuntu/Debian)\nsudo apt-get install docker.io\n\n# Verify Docker is running\ndocker info\n</code></pre>"},{"location":"getting-started/installation/#monty-backend","title":"Monty Backend","text":"<p>Monty is an optional secure backend for pure RLM execution:</p> <pre><code>pip install pydantic-monty\n</code></pre> <p>In TUI:</p> <pre><code>/sandbox backend monty\n</code></pre>"},{"location":"getting-started/installation/#apple-container-runtime-macos-experimental","title":"Apple Container Runtime (macOS, Experimental)","text":"<pre><code>container --version\ncontainer system status\n</code></pre> <p>In TUI:</p> <pre><code>/sandbox apple on\n/sandbox use apple-container\n</code></pre>"},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<pre><code>uv tool install \"rlm-code[docs]\"\n</code></pre> <p>Installs <code>mkdocs</code>, <code>mkdocs-material</code>, <code>mkdocstrings</code>, and <code>mkdocs-minify-plugin</code> for building these docs locally.</p>"},{"location":"getting-started/installation/#full-installation-everything","title":"Full Installation (Everything)","text":"<p>To install with all optional dependencies at once:</p> uv toolpip <pre><code>uv tool install \"rlm-code[tui,llm-all,mlflow,frameworks,mcp-ws]\"\n</code></pre> <pre><code>pip install \"rlm-code[tui,llm-all,mlflow,frameworks,mcp-ws]\"\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify that everything works:</p>"},{"location":"getting-started/installation/#check-the-version","title":"Check the version","text":"<pre><code>rlm-code --version\n</code></pre>"},{"location":"getting-started/installation/#check-sandbox-runtimes","title":"Check sandbox runtimes","text":"<p>Launch the TUI and run the sandbox doctor:</p> <pre><code>rlm-code\n</code></pre> <p>Then in the TUI:</p> <pre><code>/sandbox doctor\n</code></pre> <p>This runs diagnostics on all available sandbox runtimes (local, Docker, Apple Container, Modal, E2B, Daytona) and reports their health status.</p>"},{"location":"getting-started/installation/#check-observability-sinks","title":"Check observability sinks","text":"<pre><code>/rlm observability\n</code></pre> <p>This displays the status of all configured observability sinks (Local JSONL, MLflow, OpenTelemetry, LangSmith, LangFuse, Logfire).</p>"},{"location":"getting-started/installation/#verify-python-environment","title":"Verify Python environment","text":"<pre><code>python -c \"import rlm_code; print(rlm_code.__version__)\"\n</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"uv toolpip <pre><code>uv tool upgrade rlm-code\n</code></pre> <pre><code>pip install --upgrade \"rlm-code[tui,llm-all]\"\n</code></pre>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"uv toolpip <pre><code>uv tool uninstall rlm-code\n</code></pre> <pre><code>pip uninstall rlm-code\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>ModuleNotFoundError: textual</p> <p>The TUI requires the <code>textual</code> package. Reinstall with the <code>tui</code> extra:</p> <pre><code>uv tool install \"rlm-code[tui,llm-all]\"\n</code></pre> <p>Docker daemon not running</p> <p>If <code>/sandbox doctor</code> reports Docker as unavailable, ensure the Docker daemon is running:</p> <pre><code># macOS\nopen -a Docker\n\n# Linux\nsudo systemctl start docker\n</code></pre> <p>Permission denied on /tmp</p> <p>Some sandbox operations write to temporary directories. Ensure your user has write access to <code>/tmp</code> or set the <code>TMPDIR</code> environment variable to a writable path.</p> <p>DSPy not found</p> <p>RLM Code requires DSPy &gt;= 3.0.4. Reinstall to pick up the latest dependencies:</p> <pre><code>uv tool install --force \"rlm-code[tui,llm-all]\"\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through launching RLM Code, connecting to an LLM, running your first benchmark, validating results, and exploring the Research tab - all in under 10 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li> Python 3.11+ installed</li> <li> RLM Code installed (<code>uv tool install \"rlm-code[tui,llm-all]\"</code>)</li> <li> At least one LLM API key (OpenAI, Anthropic, or Gemini) or a local Ollama instance</li> </ul> <p>Local Models</p> <p>You can use RLM Code entirely with local models via Ollama. No API keys needed:</p> <pre><code>ollama pull llama3.2\n</code></pre>"},{"location":"getting-started/quickstart/#step-1-launch-the-tui","title":"Step 1: Launch the TUI","text":"<p>Navigate to a project directory (not your home directory) and launch:</p> <pre><code>mkdir -p ~/projects/rlm-demo &amp;&amp; cd ~/projects/rlm-demo\nrlm-code\n</code></pre> <p>Directory Safety Check</p> <p>RLM Code performs a safety check on startup. It will warn you if you are running from your home directory, Desktop, Documents, or a system directory. Always run from a dedicated project directory.</p> <p>You should see the RLM Research Lab TUI with 5 tabs: RLM, Files, Details, Shell, and Research. The RLM tab is active by default.</p>"},{"location":"getting-started/quickstart/#step-2-initialize-your-project","title":"Step 2: Initialize Your Project","text":"<p>Initialize a project configuration file:</p> <pre><code>/init\n</code></pre> <p>This creates an <code>rlm_config.yaml</code> in your current directory with default settings. The initializer scans your project for existing files and frameworks.</p>"},{"location":"getting-started/quickstart/#step-3-connect-to-a-model","title":"Step 3: Connect to a Model","text":"<p>Use the <code>/connect</code> command to connect to an LLM provider:</p> Anthropic (Claude)OpenAI (GPT-5)GeminiOllama (Local) <pre><code>/connect anthropic claude-opus-4-6\n</code></pre> <p>Note</p> <p>Requires <code>ANTHROPIC_API_KEY</code> in your environment or <code>.env</code> file.</p> <pre><code>/connect openai gpt-5.3-codex\n</code></pre> <p>Note</p> <p>Requires <code>OPENAI_API_KEY</code> in your environment or <code>.env</code> file.</p> <pre><code>/connect gemini gemini-2.5-flash\n</code></pre> <p>Note</p> <p>Requires <code>GEMINI_API_KEY</code> or <code>GOOGLE_API_KEY</code> in your environment or <code>.env</code> file.</p> <pre><code>/connect ollama llama3.2\n</code></pre> <p>Note</p> <p>Requires a running Ollama server at <code>http://localhost:11434</code>.</p>"},{"location":"getting-started/quickstart/#interactive-model-picker","title":"Interactive Model Picker","text":"<p>For an interactive keyboard-driven model selection experience:</p> <pre><code>/connect\n</code></pre> <p>Run <code>/connect</code> with no arguments to open the guided picker and choose mode/provider/model interactively.</p>"},{"location":"getting-started/quickstart/#verify-connection","title":"Verify Connection","text":"<p>Check that your model is connected:</p> <pre><code>/status\n</code></pre> <p>This shows the current model, provider, connection status, sandbox runtime, and observability sinks.</p>"},{"location":"getting-started/quickstart/#step-4-run-a-benchmark","title":"Step 4: Run a Benchmark","text":"<p>RLM Code ships with 10+ built-in benchmark presets. Start with the quick DSPy smoke test:</p> <pre><code>/rlm bench preset=dspy_quick\n</code></pre> <p>This runs 3 benchmark cases (Build Signature, Build Module, Add Tests) through the RLM loop: context -&gt; action proposal -&gt; sandbox execution -&gt; observation -&gt; reward -&gt; memory update.</p>"},{"location":"getting-started/quickstart/#list-available-presets","title":"List Available Presets","text":"<pre><code>/rlm bench list\n</code></pre> <p>Available built-in presets:</p> Preset Cases Description <code>dspy_quick</code> 3 Fast DSPy coding loop smoke test <code>dspy_extended</code> 5 Broader DSPy coding loop sweep <code>generic_smoke</code> 2 Generic environment safety/sanity checks <code>pure_rlm_smoke</code> 3 Pure RLM paper-compliant mode smoke test <code>pure_rlm_context</code> 4 Pure RLM context-as-variable paradigm tests <code>deep_recursion</code> 3 Deep recursion tests (depth &gt; 1) <code>paradigm_comparison</code> 3 Side-by-side paradigm comparison benchmarks <code>oolong_style</code> 4 OOLONG-style long context benchmarks <code>browsecomp_style</code> 3 BrowseComp-Plus style web reasoning benchmarks <code>token_efficiency</code> 3 Token efficiency comparison benchmarks"},{"location":"getting-started/quickstart/#run-a-pure-rlm-benchmark","title":"Run a Pure RLM Benchmark","text":"<p>The Pure RLM benchmarks exercise the paper's core paradigm:</p> <pre><code>/rlm bench preset=pure_rlm_smoke\n</code></pre> <p>These tests use <code>context</code> as a REPL variable, <code>llm_query()</code> for recursive LLM calls, <code>FINAL()</code> and <code>FINAL_VAR()</code> for termination, and <code>SHOW_VARS()</code> for state inspection.</p>"},{"location":"getting-started/quickstart/#load-external-benchmark-packs","title":"Load External Benchmark Packs","text":"<p>You can load benchmarks from YAML, JSON, or JSONL files:</p> <pre><code>/rlm bench pack=my_benchmarks.yaml\n</code></pre> <p>Supported formats include explicit preset mappings, Pydantic-style dataset cases, Google ADK eval sets, and generic record datasets.</p>"},{"location":"getting-started/quickstart/#safety-and-budget-guardrails-recommended","title":"Safety and Budget Guardrails (Recommended)","text":"<p>Before larger runs, keep strict limits:</p> <pre><code>/rlm run \"small scoped task\" steps=4 timeout=30 budget=60\n</code></pre> <ul> <li><code>steps</code> caps planner iterations</li> <li><code>timeout</code> caps per-action execution time</li> <li><code>budget</code> caps total run time</li> </ul> <p>If a run is going out of control, cancel it:</p> <pre><code>/rlm abort all\n</code></pre> <p>Or cancel one run by id:</p> <pre><code>/rlm abort &lt;run_id&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-inspect-benchmark-results","title":"Step 5: Inspect Benchmark Results","text":"<p>After running benchmarks, inspect the Research tab and (after at least two runs) use compare/report commands:</p> <pre><code>/rlm bench compare candidate=latest baseline=previous\n/rlm bench report candidate=latest baseline=previous format=markdown\n</code></pre> <p>The Research -&gt; Benchmarks panel updates automatically after <code>/rlm bench</code>.</p>"},{"location":"getting-started/quickstart/#step-6-compare-paradigms","title":"Step 6: Compare Paradigms","text":"<p>Run the same task through multiple paradigms and compare:</p> <pre><code>/rlm bench preset=paradigm_comparison\n</code></pre> <p>This runs document summarization, information extraction, and multi-hop reasoning tasks through Pure RLM, CodeAct, and Traditional paradigms side by side.</p> <p>Use the comparison command for direct A/B analysis:</p> <pre><code>/rlm bench compare candidate=latest baseline=previous\n</code></pre>"},{"location":"getting-started/quickstart/#step-7-session-replay","title":"Step 7: Session Replay","text":"<p>Every RLM run generates a trajectory that can be replayed step by step.</p>"},{"location":"getting-started/quickstart/#load-a-session-for-replay","title":"Load a Session for Replay","text":"<pre><code>/rlm status\n/rlm replay &lt;run_id&gt;\n</code></pre> <p>This loads the most recent run and enters replay mode with forward/backward navigation:</p> <ul> <li>Step forward: View the next action, observation, and reward</li> <li>Step backward: Go back to a previous state</li> <li>Jump to step: Go directly to any step number</li> <li>Find errors: Jump to steps that produced errors</li> <li>View summary: See session-level statistics</li> </ul> <p>Use the <code>run_id</code> printed by <code>/rlm status</code> (or from the Research tab).</p>"},{"location":"getting-started/quickstart/#step-8-explore-slash-commands","title":"Step 8: Explore Slash Commands","text":"<p>RLM Code has 50+ slash commands. Here are the most useful ones to explore next:</p>"},{"location":"getting-started/quickstart/#rlm-commands","title":"RLM Commands","text":"<pre><code>/rlm run \"Analyze this code and suggest improvements\"\n/rlm status\n/rlm doctor\n/rlm chat \"What patterns does this codebase use?\"\n/rlm observability\n</code></pre>"},{"location":"getting-started/quickstart/#sandbox-commands","title":"Sandbox Commands","text":"<pre><code>/sandbox status\n/sandbox doctor\n/sandbox profile secure\n/sandbox use docker\n</code></pre>"},{"location":"getting-started/quickstart/#harness-commands","title":"Harness Commands","text":"<pre><code>/harness tools\n/harness run \"fix failing tests\" steps=8 mcp=on\n</code></pre> <p>With ACP:</p> <pre><code>/connect acp\n/harness run \"implement feature with tests\" steps=8 mcp=on\n</code></pre> <p>In Local/BYOK modes, likely coding prompts can auto-route to harness. In ACP mode, auto-routing is intentionally disabled; use <code>/harness run ...</code> directly.</p>"},{"location":"getting-started/quickstart/#file-and-layout-commands","title":"File and Layout Commands","text":"<pre><code>/snapshot          # Take a project snapshot\n/diff              # Show file diffs\n/view chat         # Switch to chat view\n/layout multi      # Switch to multi-pane layout\n/pane files show   # Show the files panel\n/focus chat        # Focus the chat input\n</code></pre> <p>For the complete researcher command handbook, see Researcher Onboarding.</p>"},{"location":"getting-started/quickstart/#shell-access","title":"Shell Access","text":"<pre><code>/shell ls -la\n!python --version\n</code></pre> <p>Shell Shortcut</p> <p>Prefix any command with <code>!</code> to run it as a shell command directly:</p> <pre><code>!pip list | grep dspy\n</code></pre>"},{"location":"getting-started/quickstart/#get-help","title":"Get Help","text":"<pre><code>/help\n</code></pre>"},{"location":"getting-started/quickstart/#step-9-explore-the-research-tab","title":"Step 9: Explore the Research Tab","text":"<p>After running a benchmark, press <code>Ctrl+5</code> to switch to the Research tab:</p> <ul> <li>Dashboard: See run metrics, reward sparkline, and summary</li> <li>Trajectory: Step-by-step breakdown of agent actions and rewards</li> <li>Benchmarks: Leaderboard table from all your runs</li> <li>Replay: Step-through controls for time-travel debugging</li> <li>Events: Live event stream from the RLM event bus</li> </ul> <p>Research Tab</p> <p>The Research tab updates automatically when you run <code>/rlm bench</code> or <code>/rlm run</code> commands. No manual refresh needed!</p>"},{"location":"getting-started/quickstart/#full-workflow-example","title":"Full Workflow Example","text":"<p>Here is a complete workflow from start to finish:</p> <pre><code># Create a project directory\nmkdir -p ~/projects/rlm-eval &amp;&amp; cd ~/projects/rlm-eval\n\n# Launch RLM Code\nrlm-code\n</code></pre> <pre><code># Initialize the project\n/init\n\n# Connect to Claude Opus 4.6\n/connect anthropic claude-opus-4-6\n\n# Check everything is working\n/status\n/sandbox doctor\n\n# Run the Pure RLM smoke test\n/rlm bench preset=pure_rlm_smoke\n\n# Compare the latest run with previous baseline\n/rlm bench compare candidate=latest baseline=previous\n\n# Export a benchmark report\n/rlm bench report candidate=latest baseline=previous format=markdown\n\n# Run a more comprehensive benchmark\n/rlm bench preset=dspy_extended\n\n# Compare paradigms\n/rlm bench preset=paradigm_comparison\n\n# Replay the last session\n/rlm status\n/rlm replay &lt;run_id&gt;\n\n# Check observability sinks\n/rlm observability\n\n# Run an ad-hoc task\n/rlm run \"Write a Python function that finds the longest common subsequence\"\n\n# Export results\n/export results.json\n\n# Exit\n/exit\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>CLI Reference: Complete documentation for all commands and flags</li> <li>Configuration: Customize every aspect of RLM Code via <code>rlm_config.yaml</code></li> <li>Core Engine: RLM Runner, Environments, and Event System</li> <li>Research Tab: Deep dive into the experiment tracking interface</li> <li>Observability: MLflow, OpenTelemetry, LangSmith, LangFuse, Logfire</li> <li>Sandbox Runtimes: Superbox runtime selection, Docker/Monty/cloud guidance</li> </ul>"},{"location":"getting-started/researcher-onboarding/","title":"Researcher Onboarding","text":"<p>This page is the researcher-focused command handbook for the current RLM Code implementation. It complements:</p> <ul> <li>Quick Start for first run</li> <li>CLI Reference for deeper option details</li> <li>Execution Patterns for concept-level mode selection</li> </ul>"},{"location":"getting-started/researcher-onboarding/#first-10-minutes","title":"First 10 Minutes","text":"<p>Use this flow to get productive quickly:</p> <pre><code>rlm-code\n/init\n/connect\n/status\n/sandbox profile secure\n/sandbox status\n/rlm run \"Summarize this repository architecture\" env=pure_rlm steps=4\n/rlm bench list\n/rlm bench preset=dspy_quick\n/rlm frameworks\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#recommended-research-loops","title":"Recommended Research Loops","text":""},{"location":"getting-started/researcher-onboarding/#loop-a-baseline-candidate-gate","title":"Loop A: Baseline -&gt; Candidate -&gt; Gate","text":"<pre><code>/rlm bench preset=pure_rlm_smoke\n/rlm bench preset=pure_rlm_smoke framework=dspy-rlm\n/rlm bench compare candidate=latest baseline=previous\n/rlm bench validate candidate=latest baseline=previous --json\n/rlm bench report candidate=latest baseline=previous format=markdown\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#loop-b-long-context-replay","title":"Loop B: Long Context + Replay","text":"<pre><code>/rlm run \"Analyze long context behavior on this task\" env=pure_rlm depth=3 children=4 parallel=2\n/rlm status\n/rlm replay &lt;run_id&gt;\n/rlm viz latest depth=3 children=on\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#loop-c-llm-judge-workflow","title":"Loop C: LLM Judge Workflow","text":"<pre><code>/rlm judge pred=predictions.jsonl ref=reference.json judge=openai/gpt-5-mini\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#loop-d-controlled-mode-comparison","title":"Loop D: Controlled Mode Comparison","text":"<pre><code>/rlm bench preset=paradigm_comparison mode=native\n/rlm bench preset=paradigm_comparison mode=harness\n/rlm bench preset=paradigm_comparison mode=direct-llm\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#loop-e-acp-harness","title":"Loop E: ACP + Harness","text":"<pre><code>/connect acp\n/harness tools mcp=on\n/harness run \"implement task and add tests\" steps=8 mcp=on\n</code></pre> <p>Note: ACP keeps chat auto-routing to harness disabled by default; use <code>/harness run</code> explicitly.</p> <p>For a cleaner pure-recursive experiment setup, disable TUI harness auto-routing:</p> <pre><code>export RLM_TUI_HARNESS_AUTO=0\nrlm-code\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#tui-only-commands","title":"TUI-Only Commands","text":"<p>These are handled directly by the Textual app and are optimized for interaction speed.</p> Command Purpose <code>/help</code> In-TUI quick help <code>/workflow</code> Show recommended RLM workflow <code>/connect</code> Open keyboard connection picker <code>/models</code> Provider/model status <code>/status</code> Refresh status strip <code>/snapshot [file]</code> Save baseline for diffing <code>/diff [file]</code> Show changes vs snapshot <code>/view &lt;chat\\|files\\|details\\|shell\\|research\\|next\\|prev&gt;</code> Switch active view (<code>chat</code> route opens the RLM tab) <code>/layout &lt;single\\|multi&gt;</code> One-screen vs multi-pane mode <code>/pane &lt;files\\|details\\|shell&gt; [show\\|hide\\|toggle]</code> Toggle pane visibility <code>/focus &lt;chat\\|default&gt;</code> Focus controls <code>/copy</code> Copy latest assistant response <code>/shell [command]</code> Open or execute in shell pane <code>/rml ...</code> Alias for <code>/rlm ...</code> <code>/exit</code>, <code>/quit</code> Exit TUI"},{"location":"getting-started/researcher-onboarding/#full-slash-command-inventory","title":"Full Slash Command Inventory","text":"<p>This is the complete slash-command surface currently registered in <code>rlm_code/commands/slash_commands.py</code>.</p>"},{"location":"getting-started/researcher-onboarding/#core-and-session","title":"Core and Session","text":"Command Purpose <code>/init</code> Initialize project configuration <code>/project info</code> Show project context summary <code>/connect [provider model [api-key] [base-url]]</code> Connect directly, or run with no args for interactive picker <code>/models</code> List model/provider options <code>/status</code> Connection + runtime status <code>/disconnect</code> Disconnect current model <code>/history [all]</code> Show conversation history <code>/clear</code> Clear active conversation <code>/save &lt;filename&gt;</code> Save generated code <code>/sessions</code> List saved sessions <code>/session save [name]</code> Save current session <code>/session load &lt;name&gt;</code> Load session <code>/session delete &lt;name&gt;</code> Delete session <code>/help</code> Full command help <code>/intro</code> Intro walkthrough <code>/exit</code> Exit"},{"location":"getting-started/researcher-onboarding/#rlm-and-research","title":"RLM and Research","text":"Command Purpose <code>/rlm run &lt;task&gt; ...</code> Run an RLM episode <code>/rlm bench [list\\|preset=name] ...</code> Run/list benchmark presets <code>/rlm bench compare ...</code> Compare candidate vs baseline <code>/rlm bench validate ... [--json]</code> CI-style pass/fail gate <code>/rlm bench report ...</code> Export compare report <code>/rlm import-evals pack=&lt;path[,path2]&gt; [limit=N]</code> Preview external eval packs <code>/rlm judge pred=&lt;predictions.jsonl&gt; ref=&lt;reference.json&gt; ...</code> LLM-judge predictions <code>/rlm frameworks</code> Framework adapter readiness <code>/rlm viz [run_id\\|latest] ...</code> Trajectory visualization <code>/rlm status [run_id]</code> Show run status <code>/rlm abort [run_id\\|all]</code> Cancel active run(s) cooperatively <code>/rlm replay &lt;run_id&gt;</code> Replay run events <code>/rlm doctor [env=...] [--json]</code> Environment diagnostics <code>/rlm chat &lt;message&gt; ...</code> Persistent recursive chat turn <code>/rlm chat status [session=name]</code> Chat memory stats <code>/rlm chat reset [session=name]</code> Reset chat session <code>/rlm observability</code> Sink availability and status"},{"location":"getting-started/researcher-onboarding/#sandbox-and-superbox-controls","title":"Sandbox and Superbox Controls","text":"Command Purpose <code>/sandbox status</code> Runtime and backend health <code>/sandbox doctor</code> Runtime diagnostics <code>/sandbox use &lt;runtime&gt;</code> Switch runtime (<code>local</code>, <code>docker</code>, <code>apple-container</code>, <code>modal</code>, <code>e2b</code>, <code>daytona</code>) <code>/sandbox profile &lt;secure\\|dev\\|custom&gt;</code> Apply superbox profile <code>/sandbox backend &lt;exec\\|monty\\|docker&gt; [ack=I_UNDERSTAND_EXEC_IS_UNSAFE]</code> Set pure RLM backend <code>/sandbox strict &lt;on\\|off&gt;</code> Toggle strict mode <code>/sandbox output-mode &lt;truncate\\|summarize\\|metadata&gt;</code> Output compaction behavior <code>/sandbox apple &lt;on\\|off&gt;</code> Apple runtime gate"},{"location":"getting-started/researcher-onboarding/#harness","title":"Harness","text":"Command Purpose <code>/harness tools [mcp=on\\|off]</code> List harness tools <code>/harness doctor</code> Coverage parity check <code>/harness run &lt;task&gt; [steps=N] [mcp=on\\|off] [tools=name[,name2]]</code> Tool-driven coding loop"},{"location":"getting-started/researcher-onboarding/#mcp","title":"MCP","text":"Command Purpose <code>/mcp-servers</code> List configured MCP servers <code>/mcp-connect &lt;server&gt;</code> Connect MCP server <code>/mcp-disconnect &lt;server&gt;</code> Disconnect MCP server <code>/mcp-tools [server]</code> List tools <code>/mcp-call &lt;server&gt; &lt;tool&gt; [json-args]</code> Invoke tool <code>/mcp-resources [server]</code> List resources <code>/mcp-read &lt;server&gt; &lt;uri&gt;</code> Read resource <code>/mcp-prompts [server]</code> List prompts <code>/mcp-prompt &lt;server&gt; &lt;prompt-name&gt; [json-args]</code> Fetch prompt"},{"location":"getting-started/researcher-onboarding/#execution-optimization-exportimport-dspy-helpers","title":"Execution, Optimization, Export/Import, DSPy Helpers","text":"Command Purpose <code>/validate [file]</code> Validate generated code <code>/run [timeout=N]</code> Execute generated code <code>/test [file]</code> Run tests <code>/optimize [budget]</code> Optimization workflow helper <code>/optimize-start [budget]</code> Start optimization <code>/optimize-status</code> Optimization status <code>/optimize-cancel</code> Cancel optimization <code>/optimize-resume [workflow-id]</code> Resume optimization <code>/export &lt;session\\|package\\|config\\|conversation&gt; [name]</code> Export assets <code>/import &lt;session\\|config&gt; &lt;file&gt;</code> Import assets <code>/save-data [file]</code> Save generated dataset <code>/demo [mcp\\|complete]</code> Demo workflows <code>/eval [metrics]</code> Evaluation helper <code>/examples [list\\|show\\|generate] ...</code> Template workflows <code>/predictors [name]</code> Predictor reference <code>/adapters [name]</code> Adapter reference <code>/retrievers [name]</code> Retriever reference <code>/async</code> Async usage help <code>/streaming</code> Streaming usage help <code>/data [task] [count]</code> Data generation helper <code>/explain [topic]</code> Explain features/topics <code>/reference [topic]</code> Reference lookup"},{"location":"getting-started/researcher-onboarding/#researcher-defaults","title":"Researcher Defaults","text":"<p>For safer and more reproducible runs:</p> <pre><code>/sandbox profile secure\n/sandbox backend docker\n/sandbox output-mode metadata\n</code></pre> <p>Use <code>backend=exec</code> only with explicit acknowledgement:</p> <pre><code>/sandbox backend exec ack=I_UNDERSTAND_EXEC_IS_UNSAFE\n</code></pre>"},{"location":"getting-started/researcher-onboarding/#cost-and-run-control","title":"Cost and Run Control","text":"<p>Use bounded settings for exploratory runs:</p> <pre><code>/rlm run \"task\" steps=4 timeout=30 budget=60\n</code></pre> <p>For benchmarks, start small:</p> <pre><code>/rlm bench preset=dspy_quick limit=1\n</code></pre> <p>If spend or runtime is getting out of hand:</p> <pre><code>/rlm abort all\n</code></pre> <p>Use <code>/rlm status</code> to confirm whether a run completed or was cancelled.</p>"},{"location":"getting-started/start-here/","title":"Start Here (Simple)","text":"<p>This page is the shortest path to understand RLM Code and start safely.</p>"},{"location":"getting-started/start-here/#what-rlm-code-is","title":"What RLM Code Is","text":"<p>RLM Code is a terminal app for running research experiments with language models.</p> <p>It helps you:</p> <ul> <li>run recursive RLM workflows (<code>/rlm run</code>)</li> <li>run benchmark packs (<code>/rlm bench ...</code>)</li> <li>compare runs (<code>/rlm bench compare ...</code>)</li> <li>replay what happened (<code>/rlm replay &lt;run_id&gt;</code>)</li> <li>run coding-agent harness workflows (<code>/harness run ...</code>)</li> </ul>"},{"location":"getting-started/start-here/#what-rlm-code-is-not","title":"What RLM Code Is Not","text":"<p>RLM Code is not:</p> <ul> <li>a one-click product for non-technical users</li> <li>a guaranteed cheap tool (LLM calls can become expensive)</li> <li>a replacement for your own evaluation criteria</li> <li>fully safe if you force unsafe backend settings (<code>exec</code>)</li> </ul>"},{"location":"getting-started/start-here/#what-you-must-install","title":"What You Must Install","text":"<p>Required:</p> <ol> <li>Python 3.11+</li> <li><code>uv</code> (recommended installer)</li> <li><code>rlm-code</code> package</li> <li>At least one model route:</li> <li>BYOK API key (OpenAI/Anthropic/Gemini), or</li> <li>local model server (for example Ollama)</li> </ol> <p>Recommended for safe execution:</p> <ol> <li>Docker runtime (preferred default)</li> <li>or Monty backend (<code>pip install pydantic-monty</code>) if you do not want Docker</li> </ol> <p>Optional:</p> <ol> <li>Apple container runtime (<code>container</code> CLI, macOS only, experimental)</li> <li>cloud runtimes (Modal/E2B/Daytona) if needed</li> </ol>"},{"location":"getting-started/start-here/#first-safe-session","title":"First Safe Session","text":"<pre><code>uv tool install \"rlm-code[tui,llm-all]\"\nrlm-code\n</code></pre> <p>In TUI:</p> <pre><code>/connect\n/sandbox profile secure\n/sandbox backend docker\n/sandbox doctor\n/rlm run \"small test task\" steps=4 timeout=30 budget=60\n/rlm status\n</code></pre>"},{"location":"getting-started/start-here/#use-it-as-a-coding-agent-simple","title":"Use It as a Coding Agent (Simple)","text":"<p>You can use RLM Code like a coding assistant without running harness commands first.</p> <p>Just connect once, then ask coding tasks directly in chat.</p> <pre><code>/connect\n</code></pre> <p>or</p> <pre><code>/connect acp\n</code></pre> <p>Then type normal prompts in chat, for example:</p> <pre><code>fix failing tests in this repo and explain the root cause\n</code></pre> <pre><code>implement a parser for this config format and add unit tests\n</code></pre> <pre><code>refactor this module for readability and keep behavior unchanged\n</code></pre> <p>Optional advanced mode:</p> <ul> <li>Use <code>/harness run ...</code> when you want explicit tool-loop control.</li> <li>Use <code>/rlm run ...</code> when you want explicit recursive experiment control.</li> </ul>"},{"location":"getting-started/start-here/#cost-safety-warning","title":"Cost + Safety Warning","text":"<p>RLM experiments can trigger many model calls (especially recursive runs).</p> <p>Always start with small limits:</p> <ul> <li><code>steps=4</code></li> <li><code>timeout=30</code></li> <li><code>budget=60</code></li> <li>small benchmark limits first (for example <code>limit=1</code>)</li> </ul> <p>If a run is going out of control, stop it:</p> <pre><code>/rlm abort all\n</code></pre> <p>Or stop one run:</p> <pre><code>/rlm abort &lt;run_id&gt;\n</code></pre> <p>Use <code>/rlm status</code> to monitor the run and confirm whether it completed or was cancelled.</p>"},{"location":"getting-started/start-here/#fast-command-cheat-sheet","title":"Fast Command Cheat Sheet","text":"Command Why you use it <code>/connect</code> Connect model <code>/sandbox profile secure</code> Apply secure defaults <code>/sandbox backend docker</code> Force Docker backend <code>/sandbox backend monty</code> Use Monty backend <code>/sandbox doctor</code> Verify runtimes and backend <code>/rlm run \"&lt;task&gt;\" steps=4 timeout=30 budget=60</code> Run a bounded experiment <code>/rlm bench list</code> Show available benchmark presets <code>/rlm bench preset=&lt;name&gt; limit=1</code> Run a small benchmark first <code>/connect acp</code> Connect through ACP profile <code>type coding task in chat</code> Default coding-agent flow (no harness command required) <code>/harness run \"&lt;task&gt;\" steps=8 mcp=on</code> Optional explicit tool-loop mode <code>/rlm status</code> Check latest run <code>/rlm abort [run_id|all]</code> Cancel active run(s) <code>/rlm replay &lt;run_id&gt;</code> Inspect full trajectory"},{"location":"integrations/","title":"Integrations","text":"<p>RLM Code is designed to integrate with the broader AI ecosystem through three integration surfaces: LLM providers for model connectivity, MCP servers for tool interoperability, and framework adapters for plugging into third-party agent frameworks.</p>"},{"location":"integrations/#integration-architecture","title":"Integration Architecture","text":"<pre><code>graph LR\n    RLM[RLM Code] --&gt; LLM[LLM Providers]\n    RLM --&gt; MCP[MCP Server/Client]\n    RLM --&gt; FW[Framework Adapters]\n\n    LLM --&gt; Local[Local Models]\n    LLM --&gt; Cloud[Cloud APIs]\n\n    MCP --&gt; Claude[Claude Desktop]\n    MCP --&gt; VSCode[VS Code]\n    MCP --&gt; ExtMCP[External MCP Servers]\n\n    FW --&gt; PydAI[Pydantic AI]\n    FW --&gt; ADK[Google ADK]</code></pre>"},{"location":"integrations/#llm-providers","title":"LLM Providers","text":"<p>RLM Code supports 20+ model providers across two connection modes:</p> Mode Description Providers Local Self-hosted models, no API key required Ollama, LM Studio, vLLM, SGLang, TGI, MLX, llama.cpp, OpenAI-Compatible BYOK Cloud APIs, bring-your-own-key OpenAI, Anthropic, Gemini, xAI, Mistral, DeepSeek, Moonshot, Alibaba, OpenRouter, OpenCode, Groq, Together, SiliconFlow, Fireworks, Perplexity, Cerebras <p>All providers are managed through a unified <code>ProviderRegistry</code> with automatic alias resolution and model inference.</p> <p>See LLM Providers for the complete provider reference.</p>"},{"location":"integrations/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>RLM Code implements both an MCP server and an MCP client:</p> Component Purpose MCP Server Exposes RLM capabilities as MCP tools for external clients (Claude Desktop, VS Code) MCP Client Connects to external MCP servers for tool augmentation"},{"location":"integrations/#server-tools","title":"Server Tools","text":"<p>The RLM MCP Server exposes five tools:</p> Tool Description <code>rlm_execute</code> Execute tasks using RLM paradigms <code>rlm_query</code> Query large contexts efficiently <code>rlm_compare</code> Compare paradigms on the same task <code>rlm_benchmark</code> Run preset benchmark suites <code>rlm_trajectory</code> View or export execution trajectories"},{"location":"integrations/#client-capabilities","title":"Client Capabilities","text":"<p>The MCP Client Manager supports:</p> <ul> <li>Three transports: stdio, SSE, WebSocket</li> <li>Auto-reconnect on closed connections</li> <li>Retry logic with configurable backoff</li> <li>Environment variable resolution in configuration</li> </ul> <p>See MCP Server for the full MCP reference.</p>"},{"location":"integrations/#framework-adapters","title":"Framework Adapters","text":"<p>Framework adapters allow RLM Code to delegate execution to third-party agent frameworks while maintaining RLM trajectory logging and reward tracking.</p> Framework Adapter Class Install Extra Pydantic AI <code>PydanticAIFrameworkAdapter</code> <code>rlm-code[pydantic]</code> Google ADK <code>GoogleADKFrameworkAdapter</code> <code>rlm-code[adk]</code> <p>All adapters implement the <code>RLMFrameworkAdapter</code> protocol and are registered through the <code>FrameworkAdapterRegistry</code>.</p> <p>See Framework Adapters for implementation details.</p>"},{"location":"integrations/#quick-configuration","title":"Quick Configuration","text":"LLM ProviderMCP ServerFramework Adapter <pre><code># Local (Ollama)\nrlm-code\n&gt; /connect ollama qwen2.5-coder:7b\n\n# Cloud (OpenAI)\nexport OPENAI_API_KEY=\"sk-...\"\nrlm-code\n&gt; /connect openai gpt-4o\n</code></pre> <pre><code>{\n  \"mcpServers\": {\n    \"rlm-code\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"rlm_code.mcp.server\"]\n    }\n  }\n}\n</code></pre> <pre><code>from rlm_code.rlm.frameworks import FrameworkAdapterRegistry\n\nregistry = FrameworkAdapterRegistry.default(workdir=\"/tmp/work\")\nadapter = registry.get(\"pydantic-ai\")\n</code></pre>"},{"location":"integrations/#next-steps","title":"Next Steps","text":"<ul> <li>LLM Providers -- Full provider registry, model catalog, env vars</li> <li>MCP Server -- Server tools, client manager, transport configuration</li> <li>Framework Adapters -- Adapter protocol, Pydantic AI, Google ADK</li> </ul>"},{"location":"integrations/frameworks/","title":"Framework Adapters","text":"<p>RLM Code can delegate task execution to external agentic frameworks through a protocol-based adapter system. Each adapter converts framework-native execution into RLM trajectory-compatible step records, enabling unified observability and comparison across execution backends.</p>"},{"location":"integrations/frameworks/#module","title":"Module","text":"<pre><code>rlm_code.rlm.frameworks\n  +-- base.py                 -- Protocol, data classes\n  +-- registry.py             -- Adapter registry\n  +-- (runner-native)         -- `framework=native` path in RLMRunner\n  +-- dspy_rlm_adapter.py     -- DSPy native RLM adapter\n  +-- adk_rlm_adapter.py      -- ADK sample native RLM adapter\n  +-- pydantic_ai_adapter.py  -- Pydantic AI adapter\n  +-- google_adk_adapter.py   -- Google ADK adapter\n  +-- deepagents_adapter.py   -- DeepAgents (LangGraph) adapter\n</code></pre>"},{"location":"integrations/frameworks/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    Runner[\"RLM Runner\"] --&gt; Registry[\"FrameworkAdapterRegistry\"]\n    Registry --&gt;|\"get('dspy-rlm')\"| DSPyRLM[\"DSPyRLMFrameworkAdapter\"]\n    Registry --&gt;|\"get('adk-rlm')\"| ADKRLM[\"ADKRLMFrameworkAdapter\"]\n    Registry --&gt;|\"get('pydantic-ai')\"| PydAI[\"PydanticAIFrameworkAdapter\"]\n    Registry --&gt;|\"get('google-adk')\"| ADK[\"GoogleADKFrameworkAdapter\"]\n    Registry --&gt;|\"get('deepagents')\"| DA[\"DeepAgentsFrameworkAdapter\"]\n    DSPyRLM --&gt; Agent0[\"dspy.RLM\"]\n    ADKRLM --&gt; Agent00[\"adk_rlm.completion\"]\n    PydAI --&gt; Agent1[\"Pydantic AI Agent\"]\n    ADK --&gt; Agent2[\"Google ADK LlmAgent\"]\n    DA --&gt; Agent3[\"DeepAgents LangGraph Agent\"]\n    Agent0 --&gt; Steps0[\"FrameworkStepRecord[]\"]\n    Agent00 --&gt; Steps00[\"FrameworkStepRecord[]\"]\n    Agent1 --&gt; Steps1[\"FrameworkStepRecord[]\"]\n    Agent2 --&gt; Steps2[\"FrameworkStepRecord[]\"]\n    Agent3 --&gt; Steps3[\"FrameworkStepRecord[]\"]\n    Steps0 --&gt; Result[\"FrameworkEpisodeResult\"]\n    Steps00 --&gt; Result\n    Steps1 --&gt; Result[\"FrameworkEpisodeResult\"]\n    Steps2 --&gt; Result\n    Steps3 --&gt; Result</code></pre>"},{"location":"integrations/frameworks/#data-classes","title":"Data Classes","text":""},{"location":"integrations/frameworks/#frameworksteprecord","title":"<code>FrameworkStepRecord</code>","text":"<p>One framework step converted into RLM trajectory-compatible form.</p> <pre><code>from rlm_code.rlm.frameworks import FrameworkStepRecord\n\nstep = FrameworkStepRecord(\n    action=\"tool_call\",\n    observation={\"tool_name\": \"search\", \"args\": {\"query\": \"RLM\"}},\n    reward=0.02,\n    done=False,\n)\n</code></pre> Field Type Default Description <code>action</code> <code>str</code> -- Action type (e.g., <code>\"tool_call\"</code>, <code>\"model_response\"</code>) <code>observation</code> <code>dict[str, Any]</code> <code>{}</code> Step output data <code>reward</code> <code>float</code> <code>0.0</code> Reward signal for this step <code>done</code> <code>bool</code> <code>False</code> Whether the episode is complete"},{"location":"integrations/frameworks/#frameworkepisoderesult","title":"<code>FrameworkEpisodeResult</code>","text":"<p>Result payload returned by a framework adapter run.</p> <pre><code>from rlm_code.rlm.frameworks import FrameworkEpisodeResult\n\nresult = FrameworkEpisodeResult(\n    completed=True,\n    final_response=\"The answer is 42.\",\n    steps=[step1, step2, step3],\n    total_reward=0.35,\n    usage_summary={\"prompt_tokens\": 200, \"completion_tokens\": 100},\n    metadata={\"framework\": \"pydantic-ai\", \"resolved_model\": \"openai:gpt-4o\"},\n)\n</code></pre> Field Type Default Description <code>completed</code> <code>bool</code> -- Whether the task completed <code>final_response</code> <code>str</code> -- Final text output <code>steps</code> <code>list[FrameworkStepRecord]</code> <code>[]</code> Step records for trajectory <code>total_reward</code> <code>float</code> <code>0.0</code> Cumulative reward (clamped to [-1, 1]) <code>usage_summary</code> <code>dict[str, int] | None</code> <code>None</code> Token usage summary <code>metadata</code> <code>dict[str, Any]</code> <code>{}</code> Framework-specific metadata"},{"location":"integrations/frameworks/#adapter-protocol","title":"Adapter Protocol","text":""},{"location":"integrations/frameworks/#rlmframeworkadapter","title":"<code>RLMFrameworkAdapter</code>","text":"<p>The <code>RLMFrameworkAdapter</code> is a Python <code>Protocol</code> that all adapters must satisfy.</p> <pre><code>from rlm_code.rlm.frameworks import RLMFrameworkAdapter\n\nclass MyAdapter:\n    framework_id: str = \"my-framework\"\n\n    def doctor(self) -&gt; tuple[bool, str]:\n        \"\"\"Check if the framework is installed and ready.\"\"\"\n        ...\n\n    def run_episode(\n        self,\n        *,\n        task: str,\n        llm_connector: Any,\n        max_steps: int,\n        exec_timeout: int,\n        workdir: str,\n        sub_model: str | None = None,\n        sub_provider: str | None = None,\n        context: dict[str, Any] | None = None,\n    ) -&gt; FrameworkEpisodeResult:\n        \"\"\"Execute one framework-native task run.\"\"\"\n        ...\n</code></pre> Member Signature Description <code>framework_id</code> <code>str</code> Unique adapter identifier <code>doctor()</code> <code>() -&gt; tuple[bool, str]</code> Readiness check (ok, detail message) <code>run_episode()</code> <code>(*, task, llm_connector, max_steps, ...) -&gt; FrameworkEpisodeResult</code> Execute a task"},{"location":"integrations/frameworks/#run_episode-parameters","title":"<code>run_episode</code> Parameters","text":"Parameter Type Description <code>task</code> <code>str</code> Task description to execute <code>llm_connector</code> <code>Any</code> Active <code>LLMConnector</code> instance <code>max_steps</code> <code>int</code> Maximum iterations <code>exec_timeout</code> <code>int</code> Timeout in seconds <code>workdir</code> <code>str</code> Working directory path <code>sub_model</code> <code>str | None</code> Override model name <code>sub_provider</code> <code>str | None</code> Override provider name <code>context</code> <code>dict | None</code> Additional context data"},{"location":"integrations/frameworks/#adapter-registry","title":"Adapter Registry","text":"<p>The <code>FrameworkAdapterRegistry</code> manages adapter registration and lookup.</p> <pre><code>from rlm_code.rlm.frameworks import FrameworkAdapterRegistry\n\nregistry = FrameworkAdapterRegistry.default(workdir=\"/path/to/project\")\n</code></pre>"},{"location":"integrations/frameworks/#methods","title":"Methods","text":"Method Signature Description <code>register()</code> <code>(adapter: RLMFrameworkAdapter) -&gt; None</code> Register an adapter <code>get()</code> <code>(framework_id: str | None) -&gt; Adapter | None</code> Look up adapter by ID <code>list_ids()</code> <code>() -&gt; list[str]</code> List registered adapter IDs <code>default()</code> <code>classmethod(*, workdir: str) -&gt; Registry</code> Create registry with built-in adapters <code>doctor()</code> <code>() -&gt; list[dict[str, Any]]</code> Check readiness of all adapters"},{"location":"integrations/frameworks/#default-registry","title":"Default Registry","text":"<p>The <code>default()</code> factory registers all built-in adapters:</p> <pre><code>registry = FrameworkAdapterRegistry.default(workdir=\"/my/project\")\nprint(registry.list_ids())\n# [\"adk-rlm\", \"deepagents\", \"dspy-rlm\", \"google-adk\", \"pydantic-ai\"]\n</code></pre> <p><code>native</code> is intentionally not listed here because it is built directly into <code>RLMRunner</code> (not adapter-registered).</p>"},{"location":"integrations/frameworks/#doctor-output","title":"Doctor Output","text":"<pre><code>results = registry.doctor()\n# [\n#   {\"framework\": \"adk-rlm\", \"ok\": False, \"mode\": \"native_rlm\", \"detail\": \"adk_rlm not installed...\"},\n#   {\"framework\": \"dspy-rlm\", \"ok\": True, \"mode\": \"native_rlm\", \"detail\": \"dspy RLM available\"},\n#   {\"framework\": \"google-adk\", \"ok\": False, \"detail\": \"google-adk not installed...\"},\n#   {\"framework\": \"pydantic-ai\", \"ok\": True, \"detail\": \"pydantic-ai available\"},\n# ]\n</code></pre>"},{"location":"integrations/frameworks/#tui-cli-usage","title":"TUI / CLI Usage","text":"<p>Use the shell command to inspect adapter readiness:</p> <pre><code>/rlm frameworks\n</code></pre> <p>Then select an adapter in any run path:</p> <pre><code>/rlm run Investigate memory leak framework=dspy-rlm env=generic\n/rlm run Build an RLM planner framework=adk-rlm env=generic\n/rlm chat Continue analysis framework=deepagents\n</code></pre> <p>Compatibility alias: <code>framework=dspy</code> maps to <code>framework=dspy-rlm</code>. Use <code>framework=native env=pure_rlm</code> for paper-style built-in recursive mode.</p>"},{"location":"integrations/frameworks/#dspy-rlm-adapter","title":"DSPy RLM Adapter","text":"<p>Module: <code>rlm_code.rlm.frameworks.dspy_rlm_adapter</code></p> <p>Adapter ID: <code>dspy-rlm</code></p> <p>This adapter uses DSPy's native <code>dspy.RLM</code> abstraction (reference: <code>dspy.RLM</code> from the installed DSPy package) and returns one normalized <code>FrameworkEpisodeResult</code>.</p>"},{"location":"integrations/frameworks/#installation","title":"Installation","text":"<pre><code>pip install dspy\n</code></pre>"},{"location":"integrations/frameworks/#notes","title":"Notes","text":"<ol> <li>Requires a DSPy build that exposes <code>dspy.RLM</code>.</li> <li>Uses <code>dspy.settings.lm</code> if already configured, otherwise resolves from    the connected RLM Code model.</li> </ol>"},{"location":"integrations/frameworks/#adk-sample-rlm-adapter","title":"ADK Sample RLM Adapter","text":"<p>Module: <code>rlm_code.rlm.frameworks.adk_rlm_adapter</code></p> <p>Adapter ID: <code>adk-rlm</code></p> <p>This adapter runs the Google ADK sample RLM implementation (reference: <code>adk_rlm/main.py</code> from the vendored sample package) through <code>adk_rlm.completion(...)</code>.</p>"},{"location":"integrations/frameworks/#installation_1","title":"Installation","text":"<pre><code>pip install 'rlm-code[adk]'\n</code></pre>"},{"location":"integrations/frameworks/#notes_1","title":"Notes","text":"<ol> <li><code>adk_rlm</code> is vendored inside <code>rlm-code</code> for parity with the ADK reference sample.</li> <li><code>google-adk</code> + <code>google-genai</code> still need to be installed via extras.</li> <li>Defaults to a Gemini model when no active sub-model is provided.</li> </ol>"},{"location":"integrations/frameworks/#pydantic-ai-adapter","title":"Pydantic AI Adapter","text":"<p>Module: <code>rlm_code.rlm.frameworks.pydantic_ai_adapter</code></p> <p>Adapter ID: <code>pydantic-ai</code></p>"},{"location":"integrations/frameworks/#installation_2","title":"Installation","text":"<pre><code>pip install 'rlm-code[pydantic]'\n# or directly:\npip install pydantic-ai\n</code></pre>"},{"location":"integrations/frameworks/#how-it-works","title":"How It Works","text":"<ol> <li>Resolves the model name to Pydantic AI's <code>provider:model</code> format</li> <li>Creates a <code>pydantic_ai.Agent</code> with system instructions</li> <li>Calls <code>agent.run_sync(task)</code></li> <li>Extracts step records from message parts</li> </ol>"},{"location":"integrations/frameworks/#model-resolution","title":"Model Resolution","text":"<p>The adapter normalizes provider names to Pydantic AI's expected format:</p> RLM Provider Pydantic AI Provider <code>gemini</code>, <code>google</code>, <code>google-genai</code> <code>google-gla</code> <code>lmstudio</code>, <code>vllm</code>, <code>sglang</code>, <code>tgi</code>, <code>openai-compatible</code>, <code>opencode</code> <code>openai</code> (with base URL) <code>ollama</code>, <code>local</code> <code>ollama</code> Others Used as-is <p>Environment Passthrough</p> <p>For local providers mapped to <code>openai</code>, the adapter sets <code>OPENAI_BASE_URL</code> and <code>OPENAI_API_KEY</code> environment variables so the OpenAI SDK routes to the correct local endpoint.</p>"},{"location":"integrations/frameworks/#step-extraction","title":"Step Extraction","text":"<p>Messages from Pydantic AI are parsed by part type:</p> Part Type Action Reward <code>ToolCall</code> <code>tool_call</code> +0.02 <code>ToolReturn</code> <code>tool_result</code> +0.06 <code>RetryPrompt</code> <code>retry_prompt</code> -0.05 Other (text) <code>model_part</code> +0.05 <p>If no steps are extracted, a single <code>model_response</code> step is created with reward <code>+0.25</code> (if output is non-empty) or <code>-0.1</code> (if empty).</p>"},{"location":"integrations/frameworks/#example","title":"Example","text":"<pre><code>from rlm_code.rlm.frameworks import PydanticAIFrameworkAdapter\n\nadapter = PydanticAIFrameworkAdapter(workdir=\"/my/project\")\n\n# Check readiness\nok, detail = adapter.doctor()\nprint(ok, detail)  # True, \"pydantic-ai available\"\n\n# Run a task\nresult = adapter.run_episode(\n    task=\"Explain the Fibonacci sequence\",\n    llm_connector=my_connector,\n    max_steps=3,\n    exec_timeout=60,\n    workdir=\"/my/project\",\n)\nprint(result.final_response)\nprint(result.total_reward)\n</code></pre>"},{"location":"integrations/frameworks/#google-adk-adapter","title":"Google ADK Adapter","text":"<p>Module: <code>rlm_code.rlm.frameworks.google_adk_adapter</code></p> <p>Adapter ID: <code>google-adk</code></p>"},{"location":"integrations/frameworks/#installation_3","title":"Installation","text":"<pre><code>pip install 'rlm-code[adk]'\n# or directly:\npip install google-adk\n</code></pre>"},{"location":"integrations/frameworks/#how-it-works_1","title":"How It Works","text":"<ol> <li>Resolves the model name for the ADK runtime</li> <li>Creates a <code>google.adk.agents.LlmAgent</code> with instructions</li> <li>Uses <code>InMemoryRunner</code> with an async event stream</li> <li>Extracts step records from ADK events</li> </ol>"},{"location":"integrations/frameworks/#model-resolution_1","title":"Model Resolution","text":"RLM Provider ADK Model Format <code>gemini</code>, <code>google</code>, <code>google-genai</code> Bare model name (e.g., <code>gemini-2.5-pro</code>) Models with <code>/</code> Used as-is Others <code>provider/model</code>"},{"location":"integrations/frameworks/#event-serialization","title":"Event Serialization","text":"<p>ADK events are parsed by content type:</p> Content Type Action Reward Text part <code>model_text</code> +0.05 Function call <code>tool_call</code> +0.02 Function response <code>tool_result</code> +0.06 Other <code>model_part</code> +0.00"},{"location":"integrations/frameworks/#async-execution","title":"Async Execution","text":"<p>The Google ADK adapter uses <code>async for</code> to stream events from the runner. When called from a synchronous context, it spawns a background thread to run the async event loop:</p> <pre><code># Internal helper for sync/async bridging\ndef _run_coro_sync(coro):\n    try:\n        asyncio.get_running_loop()\n    except RuntimeError:\n        return asyncio.run(coro)\n    # If loop is running, use a thread\n    thread = threading.Thread(target=lambda: asyncio.run(coro), daemon=True)\n    thread.start()\n    thread.join()\n</code></pre>"},{"location":"integrations/frameworks/#example_1","title":"Example","text":"<pre><code>from rlm_code.rlm.frameworks import GoogleADKFrameworkAdapter\n\nadapter = GoogleADKFrameworkAdapter(workdir=\"/my/project\")\n\n# Check readiness\nok, detail = adapter.doctor()\nprint(ok, detail)  # True, \"google-adk available\"\n\n# Run a task\nresult = adapter.run_episode(\n    task=\"Analyze this dataset\",\n    llm_connector=my_connector,\n    max_steps=5,\n    exec_timeout=120,\n    workdir=\"/my/project\",\n    sub_provider=\"gemini\",\n    sub_model=\"gemini-2.5-pro\",\n)\nprint(result.completed)\nprint(len(result.steps))\n</code></pre>"},{"location":"integrations/frameworks/#deepagents-adapter","title":"DeepAgents Adapter","text":"<p>Module: <code>rlm_code.rlm.frameworks.deepagents_adapter</code></p> <p>Adapter ID: <code>deepagents</code></p> <p>The DeepAgents adapter integrates DeepAgents, a LangGraph-based agentic framework, into the RLM execution pipeline. It converts LangGraph message histories into RLM trajectory-compatible step records, enabling unified observability across all framework backends.</p>"},{"location":"integrations/frameworks/#installation_4","title":"Installation","text":"<pre><code>pip install 'rlm-code[deepagents]'\n# or directly:\npip install deepagents langchain-core\n</code></pre>"},{"location":"integrations/frameworks/#how-it-works_2","title":"How It Works","text":"<ol> <li>Resolves the model name to DeepAgents' <code>provider:model</code> format</li> <li>Selects a backend (<code>StateBackend</code>, <code>FilesystemBackend</code>, or <code>LocalShellBackend</code>)</li> <li>Creates a deep agent via <code>create_deep_agent()</code> with system instructions</li> <li>Invokes the agent with a <code>HumanMessage</code></li> <li>Extracts step records from LangChain message history</li> </ol>"},{"location":"integrations/frameworks/#model-resolution_2","title":"Model Resolution","text":"<p>The adapter normalizes RLM provider names to DeepAgents' expected format:</p> RLM Provider DeepAgents Provider <code>anthropic</code>, <code>claude</code> <code>anthropic</code> <code>gemini</code>, <code>google</code>, <code>google-genai</code> <code>google-genai</code> <code>lmstudio</code>, <code>vllm</code>, <code>sglang</code>, <code>tgi</code>, <code>openai-compatible</code>, <code>opencode</code> <code>openai</code> (with base URL) <code>ollama</code>, <code>local</code> <code>ollama</code> Others Used as-is <p>If the model name already contains a colon (e.g., <code>anthropic:claude-opus-4-6</code>), it is passed through unchanged.</p> <p>Environment Passthrough</p> <p>For local providers mapped to <code>openai</code>, the adapter sets <code>OPENAI_BASE_URL</code> and <code>OPENAI_API_KEY</code> environment variables so the OpenAI SDK routes to the correct local endpoint. For <code>ollama</code>, it sets <code>OLLAMA_BASE_URL</code> similarly.</p>"},{"location":"integrations/frameworks/#backend-selection","title":"Backend Selection","text":"<p>DeepAgents supports multiple execution backends, selectable via the <code>context</code> parameter:</p> Backend Context Key Description <code>StateBackend</code> <code>deepagents_backend: \"state\"</code> (default) In-memory state management <code>FilesystemBackend</code> <code>deepagents_backend: \"filesystem\"</code> File-based state in <code>workdir</code> <code>LocalShellBackend</code> <code>deepagents_backend: \"local_shell\"</code> Shell command execution in <code>workdir</code> <pre><code># Use filesystem backend\nresult = adapter.run_episode(\n    task=\"Organize these files\",\n    llm_connector=connector,\n    max_steps=10,\n    exec_timeout=60,\n    workdir=\"/my/project\",\n    context={\"deepagents_backend\": \"filesystem\"},\n)\n</code></pre>"},{"location":"integrations/frameworks/#step-extraction_1","title":"Step Extraction","text":"<p>LangChain messages are parsed into <code>FrameworkStepRecord</code> entries:</p> Message Type Action Reward Notes AI text <code>model_text</code> +0.05 Non-empty text content AI tool call <code>tool_call</code> +0.02 Standard tool invocation AI planning tool <code>tool_call</code> +0.03 <code>write_todos</code> or <code>read_todos</code> Tool result (ok) <code>tool_result</code> +0.06 Successful tool execution Tool result (err) <code>tool_result</code> -0.05 Error status or <code>\"Error\"</code> prefix Human/System (skipped) -- Not converted to steps <p>Steps are capped at 80 per episode. The <code>total_reward</code> is clamped to <code>[-1.0, 1.0]</code>.</p> <p>Planning Tool Bonus</p> <p>The <code>write_todos</code> and <code>read_todos</code> tools receive a higher reward (+0.03 vs +0.02) to encourage structured planning behavior in agents.</p>"},{"location":"integrations/frameworks/#example_2","title":"Example","text":"<pre><code>from rlm_code.rlm.frameworks import DeepAgentsFrameworkAdapter\n\nadapter = DeepAgentsFrameworkAdapter(workdir=\"/my/project\")\n\n# Check readiness\nok, detail = adapter.doctor()\nprint(ok, detail)  # True, \"deepagents available\"\n\n# Run a task\nresult = adapter.run_episode(\n    task=\"Analyze this codebase and suggest improvements\",\n    llm_connector=my_connector,\n    max_steps=10,\n    exec_timeout=120,\n    workdir=\"/my/project\",\n    sub_provider=\"anthropic\",\n    sub_model=\"claude-sonnet-4-20250514\",\n)\nprint(result.completed)\nprint(result.final_response)\nprint(f\"Steps: {len(result.steps)}, Reward: {result.total_reward:.2f}\")\n</code></pre>"},{"location":"integrations/frameworks/#writing-a-custom-adapter","title":"Writing a Custom Adapter","text":"<p>To add support for a new framework:</p> <ol> <li>Implement the protocol:</li> </ol> <pre><code>from dataclasses import dataclass\nfrom rlm_code.rlm.frameworks.base import (\n    FrameworkEpisodeResult,\n    FrameworkStepRecord,\n    RLMFrameworkAdapter,\n)\n\n@dataclass(slots=True)\nclass MyFrameworkAdapter:\n    workdir: str\n    framework_id: str = \"my-framework\"\n\n    def doctor(self) -&gt; tuple[bool, str]:\n        try:\n            import my_framework\n            return (True, \"my-framework available\")\n        except ImportError:\n            return (False, \"my-framework not installed\")\n\n    def run_episode(self, *, task, llm_connector, max_steps, exec_timeout,\n                    workdir, sub_model=None, sub_provider=None, context=None):\n        # ... run the framework ...\n        return FrameworkEpisodeResult(\n            completed=True,\n            final_response=\"...\",\n            steps=[FrameworkStepRecord(action=\"run\", reward=0.5)],\n            total_reward=0.5,\n        )\n</code></pre> <ol> <li>Register the adapter:</li> </ol> <pre><code>from rlm_code.rlm.frameworks import FrameworkAdapterRegistry\n\nregistry = FrameworkAdapterRegistry()\nregistry.register(MyFrameworkAdapter(workdir=\"/path\"))\n</code></pre> <ol> <li>Use it:</li> </ol> <pre><code>adapter = registry.get(\"my-framework\")\nresult = adapter.run_episode(task=\"...\", ...)\n</code></pre> <p>Reward Conventions</p> <p>Assign small positive rewards for progress signals (tool calls, model responses) and small negative rewards for retries or errors. The <code>total_reward</code> is clamped to the <code>[-1.0, 1.0]</code> range.</p>"},{"location":"integrations/frameworks/#class-reference","title":"Class Reference","text":""},{"location":"integrations/frameworks/#dspyrlmframeworkadapter","title":"DSPyRLMFrameworkAdapter","text":"Field / Method Description <code>workdir</code> Working directory path <code>framework_id</code> <code>\"dspy-rlm\"</code> <code>adapter_mode</code> <code>\"native_rlm\"</code> <code>reference_impl</code> <code>\"dspy.RLM (installed package)\"</code> <code>doctor()</code> Check if <code>dspy</code> and <code>dspy.RLM</code> are available <code>run_episode()</code> Execute one task via <code>dspy.RLM</code> <code>_resolve_lm()</code> Resolve configured <code>dspy.LM</code> for sub-model/provider"},{"location":"integrations/frameworks/#adkrlmframeworkadapter","title":"ADKRLMFrameworkAdapter","text":"Field / Method Description <code>workdir</code> Working directory path <code>framework_id</code> <code>\"adk-rlm\"</code> <code>adapter_mode</code> <code>\"native_rlm\"</code> <code>reference_impl</code> <code>\"adk_rlm/main.py (vendored sample package)\"</code> <code>doctor()</code> Check if <code>adk_rlm</code> sample package is importable <code>run_episode()</code> Execute via <code>adk_rlm.completion()</code>"},{"location":"integrations/frameworks/#pydanticaiframeworkadapter","title":"PydanticAIFrameworkAdapter","text":"Field / Method Description <code>workdir</code> Working directory path <code>framework_id</code> <code>\"pydantic-ai\"</code> <code>doctor()</code> Check if <code>pydantic_ai</code> is importable <code>run_episode()</code> Execute via <code>pydantic_ai.Agent.run_sync()</code> <code>_resolve_model()</code> Normalize provider to Pydantic AI format <code>_extract_steps()</code> Parse message parts into <code>FrameworkStepRecord</code> list <code>_serialize_part()</code> Convert a message part to a dictionary"},{"location":"integrations/frameworks/#googleadkframeworkadapter","title":"GoogleADKFrameworkAdapter","text":"Field / Method Description <code>workdir</code> Working directory path <code>framework_id</code> <code>\"google-adk\"</code> <code>doctor()</code> Check if <code>google.adk</code> is importable <code>run_episode()</code> Execute via <code>InMemoryRunner.run_async()</code> <code>_resolve_model()</code> Normalize provider to ADK model format <code>_serialize_event()</code> Convert an ADK event to a dictionary"},{"location":"integrations/frameworks/#deepagentsframeworkadapter","title":"DeepAgentsFrameworkAdapter","text":"Field / Method Description <code>workdir</code> Working directory path <code>framework_id</code> <code>\"deepagents\"</code> <code>doctor()</code> Check if <code>deepagents</code> and <code>langchain-core</code> are importable <code>run_episode()</code> Execute via <code>create_deep_agent().invoke()</code> <code>_resolve_model()</code> Normalize provider to DeepAgents <code>provider:model</code> format <code>_resolve_backend()</code> Select backend from context (<code>state</code>, <code>filesystem</code>, <code>local_shell</code>) <code>_extract_steps()</code> Parse LangChain messages into <code>FrameworkStepRecord</code> list <code>_extract_final_response()</code> Extract last AI text from message history <code>_serialize_tool_call()</code> Convert a tool call dict/object to plain dict"},{"location":"integrations/llm-providers/","title":"LLM Providers","text":"<p>RLM Code connects to 20+ model providers through a unified <code>LLMConnector</code> backed by a <code>ProviderRegistry</code>. Providers are organized into two connection modes: Local (no API key required) and BYOK (Bring Your Own Key).</p>"},{"location":"integrations/llm-providers/#module","title":"Module","text":"<pre><code>rlm_code.models.llm_connector      -- Connection and generation\nrlm_code.models.providers.registry  -- Provider metadata and resolution\nrlm_code.models.providers.model_catalog  -- Curated model lists\n</code></pre>"},{"location":"integrations/llm-providers/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    User[\"User: /connect provider model\"] --&gt; LLM[LLMConnector]\n    LLM --&gt; Registry[ProviderRegistry]\n    Registry --&gt; Resolve[\"Resolve provider + model\"]\n    Resolve --&gt; Adapter{\"Adapter Type\"}\n    Adapter --&gt;|ollama| Ollama[Ollama API]\n    Adapter --&gt;|openai| OpenAI[OpenAI SDK]\n    Adapter --&gt;|anthropic| Anthropic[Anthropic SDK]\n    Adapter --&gt;|gemini| Gemini[Google GenAI SDK]\n    Adapter --&gt;|openai_compatible| Compatible[\"OpenAI-Compatible HTTP\"]</code></pre>"},{"location":"integrations/llm-providers/#provider-registry","title":"Provider Registry","text":"<p>The <code>ProviderRegistry</code> holds a <code>ProviderSpec</code> for each supported provider and resolves provider names, aliases, and model identifiers.</p>"},{"location":"integrations/llm-providers/#providerspec-dataclass","title":"<code>ProviderSpec</code> Dataclass","text":"<pre><code>from rlm_code.models.providers.registry import ProviderSpec\n\nspec = ProviderSpec(\n    provider_id=\"openai\",\n    display_name=\"OpenAI\",\n    adapter_type=\"openai\",\n    aliases=[\"gpt\"],\n    api_key_env_vars=[\"OPENAI_API_KEY\"],\n    base_url_env_var=\"OPENAI_API_BASE\",\n    docs_url=\"https://platform.openai.com/\",\n    requires_api_key=True,\n    connection_type=\"byok\",\n    category=\"US Labs\",\n    example_models=[\"gpt-5\", \"gpt-4o\", \"gpt-4o-mini\"],\n)\n</code></pre> Field Type Description <code>provider_id</code> <code>str</code> Canonical identifier (e.g., <code>\"openai\"</code>) <code>display_name</code> <code>str</code> Human-readable name <code>adapter_type</code> <code>str</code> Backend adapter: <code>ollama</code>, <code>openai</code>, <code>anthropic</code>, <code>gemini</code>, <code>openai_compatible</code> <code>aliases</code> <code>list[str]</code> Alternative names (e.g., <code>[\"gpt\"]</code> for OpenAI) <code>api_key_env_vars</code> <code>list[str]</code> Environment variables for API key lookup <code>base_url_env_var</code> <code>str | None</code> Environment variable for base URL <code>default_base_url</code> <code>str | None</code> Default endpoint URL <code>docs_url</code> <code>str | None</code> Provider documentation URL <code>requires_api_key</code> <code>bool</code> Whether an API key is required <code>connection_type</code> <code>str</code> <code>\"byok\"</code> or <code>\"local\"</code> <code>category</code> <code>str</code> Grouping: <code>\"Local\"</code>, <code>\"US Labs\"</code>, <code>\"China Labs\"</code>, <code>\"Model Hosts\"</code>, <code>\"Other Labs\"</code> <code>example_models</code> <code>list[str]</code> Example model identifiers"},{"location":"integrations/llm-providers/#registry-methods","title":"Registry Methods","text":"<pre><code>from rlm_code.models.providers.registry import ProviderRegistry\n\nregistry = ProviderRegistry.default()\n</code></pre> Method Signature Description <code>default()</code> <code>classmethod -&gt; ProviderRegistry</code> Build registry with all 20+ providers <code>list_providers()</code> <code>(connection_type: str | None) -&gt; list[ProviderSpec]</code> List providers, optionally filtered <code>get()</code> <code>(provider_name: str) -&gt; ProviderSpec | None</code> Resolve by ID or alias <code>infer_provider_from_model()</code> <code>(model_name: str) -&gt; ProviderSpec | None</code> Infer from <code>provider/model</code> format <code>normalize_model_name()</code> <code>(provider_name: str, model_name: str) -&gt; str</code> Strip redundant provider prefix"},{"location":"integrations/llm-providers/#local-providers","title":"Local Providers","text":"<p>Local providers run models on your own hardware. No API key is required.</p>"},{"location":"integrations/llm-providers/#ollama","title":"Ollama","text":"Field Value Provider ID <code>ollama</code> Aliases <code>local-ollama</code> Adapter <code>ollama</code> Default URL <code>http://localhost:11434</code> Env Var <code>OLLAMA_HOST</code> Docs ollama.com <pre><code># Connect\n/connect ollama qwen2.5-coder:7b\n/connect ollama llama3.2:3b\n/connect ollama gpt-oss:20b\n</code></pre> <p>Ollama Model Discovery</p> <p>RLM Code auto-discovers running Ollama models by querying the <code>/api/tags</code> endpoint. Models are listed with their full names including tags (e.g., <code>gpt-oss:120b</code>).</p>"},{"location":"integrations/llm-providers/#lm-studio","title":"LM Studio","text":"Field Value Provider ID <code>lmstudio</code> Aliases <code>lm-studio</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:1234/v1</code> Env Var <code>LMSTUDIO_BASE_URL</code> Docs lmstudio.ai"},{"location":"integrations/llm-providers/#vllm","title":"vLLM","text":"Field Value Provider ID <code>vllm</code> Aliases <code>vllm-server</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:8000/v1</code> Env Var <code>VLLM_BASE_URL</code> Docs docs.vllm.ai"},{"location":"integrations/llm-providers/#sglang","title":"SGLang","text":"Field Value Provider ID <code>sglang</code> Aliases <code>sg-lang</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:30000/v1</code> Env Var <code>SGLANG_BASE_URL</code> Docs docs.sglang.ai"},{"location":"integrations/llm-providers/#hugging-face-tgi","title":"Hugging Face TGI","text":"Field Value Provider ID <code>tgi</code> Aliases <code>text-generation-inference</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:8080/v1</code> Env Var <code>TGI_BASE_URL</code> Docs HF TGI"},{"location":"integrations/llm-providers/#mlx","title":"MLX","text":"Field Value Provider ID <code>mlx</code> Aliases <code>mlx-lm</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:8080/v1</code> Env Var <code>MLX_BASE_URL</code> Docs mlx-lm <p>Apple Silicon</p> <p>MLX models run natively on Apple Silicon (M1/M2/M3/M4) GPUs. Quantized models from the <code>mlx-community</code> hub are recommended.</p>"},{"location":"integrations/llm-providers/#llamacpp-server","title":"llama.cpp Server","text":"Field Value Provider ID <code>llama-cpp</code> Aliases <code>llamacpp</code>, <code>llama_cpp</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:8080/v1</code> Env Var <code>LLAMACPP_BASE_URL</code> Docs llama.cpp"},{"location":"integrations/llm-providers/#openai-compatible-generic","title":"OpenAI-Compatible (Generic)","text":"Field Value Provider ID <code>openai-compatible</code> Aliases <code>openai_compatible</code>, <code>compatible</code> Adapter <code>openai_compatible</code> Default URL <code>http://localhost:8000/v1</code> Env Var <code>OPENAI_COMPATIBLE_BASE_URL</code> <p>Use this for any local server that exposes an OpenAI-compatible <code>/v1/chat/completions</code> endpoint.</p>"},{"location":"integrations/llm-providers/#byok-providers-cloud","title":"BYOK Providers (Cloud)","text":"<p>Bring Your Own Key providers require an API key, typically set via environment variable.</p>"},{"location":"integrations/llm-providers/#us-labs","title":"US Labs","text":""},{"location":"integrations/llm-providers/#openai","title":"OpenAI","text":"Field Value Provider ID <code>openai</code> Aliases <code>gpt</code> Adapter <code>openai</code> Env Vars <code>OPENAI_API_KEY</code> Docs platform.openai.com <p>Curated models:</p> <pre><code>gpt-5.3-codex, gpt-5.2, gpt-5.2-pro, gpt-5.2-codex,\ngpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini,\ngpt-4o-2024-11-20, gpt-4o, gpt-4o-mini, o1, o1-mini\n</code></pre> <p>API Key Format</p> <p>OpenAI keys must start with <code>sk-</code>. The connector validates this format before attempting a connection.</p>"},{"location":"integrations/llm-providers/#anthropic","title":"Anthropic","text":"Field Value Provider ID <code>anthropic</code> Aliases <code>claude</code> Adapter <code>anthropic</code> Env Vars <code>ANTHROPIC_API_KEY</code> Docs console.anthropic.com <p>Curated models:</p> <pre><code>claude-opus-4-6, claude-opus-4-5-20251101,\nclaude-sonnet-4-5-20250929, claude-haiku-4-5-20251001,\nclaude-sonnet-4-20250514, claude-opus-4-20250514, claude-haiku-4-20250514\n</code></pre> <p>API Key Format</p> <p>Anthropic keys must start with <code>sk-ant-</code>.</p>"},{"location":"integrations/llm-providers/#google-gemini","title":"Google Gemini","text":"Field Value Provider ID <code>gemini</code> Aliases <code>google</code> Adapter <code>gemini</code> Env Vars <code>GEMINI_API_KEY</code>, <code>GOOGLE_API_KEY</code> Docs aistudio.google.com <p>Curated models:</p> <pre><code>gemini-3-pro-preview, gemini-3-flash-preview,\ngemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash, gemini-flash-latest\n</code></pre> <p>API Key Format</p> <p>Gemini keys must start with <code>AIza</code>.</p>"},{"location":"integrations/llm-providers/#xai","title":"xAI","text":"Field Value Provider ID <code>xai</code> Adapter <code>openai_compatible</code> Env Vars <code>XAI_API_KEY</code> Default URL <code>https://api.x.ai/v1</code> Docs console.x.ai <p>Curated models: <code>grok-3</code>, <code>grok-3-mini</code>, <code>grok-2</code>, <code>grok-beta</code></p>"},{"location":"integrations/llm-providers/#perplexity","title":"Perplexity","text":"Field Value Provider ID <code>perplexity</code> Adapter <code>openai_compatible</code> Env Vars <code>PERPLEXITY_API_KEY</code>, <code>PPLX_API_KEY</code> Default URL <code>https://api.perplexity.ai</code> Docs docs.perplexity.ai <p>Curated models: <code>sonar-pro</code>, <code>sonar</code>, <code>sonar-reasoning</code></p>"},{"location":"integrations/llm-providers/#china-labs","title":"China Labs","text":""},{"location":"integrations/llm-providers/#deepseek","title":"DeepSeek","text":"Field Value Provider ID <code>deepseek</code> Adapter <code>openai_compatible</code> Env Vars <code>DEEPSEEK_API_KEY</code> Default URL <code>https://api.deepseek.com/v1</code> Docs platform.deepseek.com <p>Curated models:</p> <pre><code>deepseek-ai/DeepSeek-V3.2, deepseek-ai/DeepSeek-R1,\ndeepseek-chat, deepseek-coder, deepseek-reasoner\n</code></pre>"},{"location":"integrations/llm-providers/#moonshot-kimi","title":"Moonshot (Kimi)","text":"Field Value Provider ID <code>moonshot</code> Aliases <code>kimi</code> Adapter <code>openai_compatible</code> Env Vars <code>MOONSHOT_API_KEY</code>, <code>KIMI_API_KEY</code> Default URL <code>https://api.moonshot.ai/v1</code> Docs platform.moonshot.cn <p>Curated models: <code>moonshot-v1-128k</code>, <code>moonshot-v1-32k</code>, <code>moonshot-v1-8k</code>, <code>kimi-k2</code></p>"},{"location":"integrations/llm-providers/#alibaba-dashscope-qwen","title":"Alibaba (DashScope / Qwen)","text":"Field Value Provider ID <code>alibaba</code> Aliases <code>qwen</code>, <code>dashscope</code> Adapter <code>openai_compatible</code> Env Vars <code>DASHSCOPE_API_KEY</code>, <code>QWEN_API_KEY</code> Default URL <code>https://dashscope-intl.aliyuncs.com/compatible-mode/v1</code> Docs dashscope.aliyun.com <p>Curated models: <code>qwen-max</code>, <code>qwen-plus</code>, <code>qwen-turbo</code>, <code>qwen2.5-coder-32b-instruct</code>, <code>qwen2.5-72b-instruct</code></p>"},{"location":"integrations/llm-providers/#model-hosts","title":"Model Hosts","text":""},{"location":"integrations/llm-providers/#openrouter","title":"OpenRouter","text":"Field Value Provider ID <code>openrouter</code> Adapter <code>openai_compatible</code> Env Vars <code>OPENROUTER_API_KEY</code> Default URL <code>https://openrouter.ai/api/v1</code> Docs openrouter.ai <p>Curated models:</p> <pre><code>anthropic/claude-sonnet-4, openai/gpt-4o,\ngoogle/gemini-2.0-flash, meta-llama/llama-3.3-70b-instruct\n</code></pre> <p>Multi-Provider Access</p> <p>OpenRouter provides a single API key for accessing models from many providers. Use the <code>provider/model</code> format for model names.</p>"},{"location":"integrations/llm-providers/#opencode-zen","title":"OpenCode Zen","text":"Field Value Provider ID <code>opencode</code> Aliases <code>opencode-zen</code> Adapter <code>openai_compatible</code> Env Vars <code>OPENCODE_API_KEY</code> Default URL <code>https://api.opencode.ai/v1</code> Docs opencode.ai <p>Curated models: <code>glm-4.7-free</code>, <code>grok-code</code>, <code>kimi-k2.5-free</code>, <code>gpt-5-nano</code>, <code>minimax-m2.1-free</code>, <code>big-pickle</code></p> <p>Keyless Mode</p> <p>OpenCode supports keyless access for free-tier models. When no <code>OPENCODE_API_KEY</code> is set, the connector filters to free models and can route through the local <code>opencode</code> CLI.</p>"},{"location":"integrations/llm-providers/#groq","title":"Groq","text":"Field Value Provider ID <code>groq</code> Adapter <code>openai_compatible</code> Env Vars <code>GROQ_API_KEY</code> Default URL <code>https://api.groq.com/openai/v1</code> Docs console.groq.com <p>Curated models: <code>llama-3.3-70b-versatile</code>, <code>llama-3.1-8b-instant</code>, <code>mixtral-8x7b-32768</code>, <code>gemma2-9b-it</code></p>"},{"location":"integrations/llm-providers/#together-ai","title":"Together AI","text":"Field Value Provider ID <code>together</code> Aliases <code>togetherai</code> Adapter <code>openai_compatible</code> Env Vars <code>TOGETHER_API_KEY</code>, <code>TOGETHERAI_API_KEY</code> Default URL <code>https://api.together.xyz/v1</code> Docs api.together.xyz <p>Curated models:</p> <pre><code>meta-llama/Llama-3.3-70B-Instruct-Turbo,\nQwen/Qwen2.5-Coder-32B-Instruct,\ndeepseek-ai/DeepSeek-R1,\nmistralai/Mixtral-8x22B-Instruct-v0.1\n</code></pre>"},{"location":"integrations/llm-providers/#siliconflow","title":"SiliconFlow","text":"Field Value Provider ID <code>siliconflow</code> Aliases <code>silicon-flow</code> Adapter <code>openai_compatible</code> Env Vars <code>SILICONFLOW_API_KEY</code>, <code>SILICON_API_KEY</code> Default URL <code>https://api.siliconflow.cn/v1</code> Docs siliconflow.cn"},{"location":"integrations/llm-providers/#fireworks-ai","title":"Fireworks AI","text":"Field Value Provider ID <code>fireworks</code> Adapter <code>openai_compatible</code> Env Vars <code>FIREWORKS_API_KEY</code> Default URL <code>https://api.fireworks.ai/inference/v1</code> Docs fireworks.ai"},{"location":"integrations/llm-providers/#cerebras","title":"Cerebras","text":"Field Value Provider ID <code>cerebras</code> Adapter <code>openai_compatible</code> Env Vars <code>CEREBRAS_API_KEY</code> Default URL <code>https://api.cerebras.ai/v1</code> Docs cloud.cerebras.ai <p>Curated models: <code>llama-3.3-70b</code>, <code>qwen-3-coder-480b</code></p>"},{"location":"integrations/llm-providers/#other-labs","title":"Other Labs","text":""},{"location":"integrations/llm-providers/#mistral-ai","title":"Mistral AI","text":"Field Value Provider ID <code>mistral</code> Adapter <code>openai_compatible</code> Env Vars <code>MISTRAL_API_KEY</code> Default URL <code>https://api.mistral.ai/v1</code> Docs console.mistral.ai <p>Curated models: <code>mistral-large-2411</code>, <code>mistral-medium-2505</code>, <code>mistral-nemo</code>, <code>codestral-latest</code>, <code>mistral-small-latest</code></p>"},{"location":"integrations/llm-providers/#llm-connector","title":"LLM Connector","text":"<p>The <code>LLMConnector</code> class manages connections and response generation.</p>"},{"location":"integrations/llm-providers/#constructor","title":"Constructor","text":"<pre><code>from rlm_code.models.llm_connector import LLMConnector\nfrom rlm_code.core.config import ConfigManager\n\nconnector = LLMConnector(config_manager=ConfigManager())\n</code></pre>"},{"location":"integrations/llm-providers/#key-methods","title":"Key Methods","text":"Method Signature Description <code>connect_to_model()</code> <code>(model_name, model_type?, api_key?, base_url?) -&gt; bool</code> Connect to a model by provider and name <code>disconnect()</code> <code>() -&gt; None</code> Disconnect from current model <code>generate_response()</code> <code>(prompt, system_prompt?, context?) -&gt; str</code> Generate a response from the connected model <code>generate_response_with_model()</code> <code>(prompt, model_name, ...) -&gt; str</code> Generate using a temporary model connection <code>list_available_ollama_models()</code> <code>() -&gt; list[str]</code> Discover running Ollama models <code>discover_local_providers()</code> <code>(timeout?) -&gt; list[dict]</code> Scan for healthy local providers <code>discover_acp_agents()</code> <code>(timeout?) -&gt; list[dict]</code> Discover ACP-compatible local agents <code>list_provider_example_models()</code> <code>(provider_name, limit?) -&gt; list[str]</code> Get curated model list for a provider <code>get_connection_status()</code> <code>() -&gt; dict</code> Current connection info <code>get_supported_providers()</code> <code>() -&gt; list[dict]</code> All providers with runtime config hints <code>get_usage_summary()</code> <code>() -&gt; dict</code> Accumulated token usage by model <code>usage_snapshot()</code> <code>() -&gt; dict[str, int]</code> Compact counter snapshot for delta accounting"},{"location":"integrations/llm-providers/#usage-tracking","title":"Usage Tracking","text":"<p>The connector automatically tracks token usage across all provider calls:</p> <pre><code># Before a task\nbefore = connector.usage_snapshot()\n\n# Run task...\nresponse = connector.generate_response(\"Explain this code\")\n\n# After\nafter = connector.usage_snapshot()\ndelta = LLMConnector.usage_delta(before, after)\nprint(delta)\n# {\"total_calls\": 1, \"prompt_tokens\": 150, \"completion_tokens\": 300}\n</code></pre> <p>The full usage summary includes per-model breakdowns:</p> <pre><code>summary = connector.get_usage_summary()\n# {\n#   \"totals\": {\"total_calls\": 5, \"prompt_tokens\": 1200, \"completion_tokens\": 2400},\n#   \"by_model\": {\n#     \"anthropic/claude-sonnet-4-5-20250929\": {\"calls\": 3, ...},\n#     \"ollama/qwen2.5-coder:7b\": {\"calls\": 2, ...},\n#   },\n#   \"last_call\": {\"timestamp\": \"...\", \"model\": \"...\", ...},\n# }\n</code></pre>"},{"location":"integrations/llm-providers/#adapter-types","title":"Adapter Types","text":"<p>Each provider uses one of five adapter backends:</p> Adapter Type SDK / Protocol Providers Using It <code>ollama</code> Ollama REST API Ollama <code>openai</code> OpenAI SDK OpenAI <code>anthropic</code> Anthropic SDK Anthropic <code>gemini</code> Google GenAI SDK Google Gemini <code>openai_compatible</code> OpenAI-compatible HTTP All other providers (14+) <p>SDK Installation</p> <p>RLM Code does not bundle provider SDKs. Install only the SDKs you need:</p> <pre><code>pip install \"openai&gt;=2.8.1\"      # For OpenAI and compatible providers\npip install anthropic             # For Anthropic Claude\npip install \"google-genai&gt;=1.52.0\"  # For Google Gemini\n</code></pre> <p>Local providers (Ollama, vLLM, etc.) use raw HTTP and require no extra SDK.</p>"},{"location":"integrations/llm-providers/#model-catalog","title":"Model Catalog","text":"<p>The <code>SUPERQODE_MODEL_CATALOG</code> provides curated model lists for each provider, used by the TUI's interactive model picker:</p> <pre><code>from rlm_code.models.providers.model_catalog import get_superqode_models\n\nmodels = get_superqode_models(\"anthropic\")\n# [\"claude-opus-4-6\", \"claude-opus-4-5-20251101\", ...]\n</code></pre> <p>The catalog uses curated provider/model lists and takes priority over the <code>example_models</code> field in <code>ProviderSpec</code>.</p>"},{"location":"integrations/llm-providers/#provider-resolution","title":"Provider Resolution","text":"<p>The connector resolves providers through a multi-step process:</p> <ol> <li>Explicit provider -- If <code>model_type</code> is given, resolve via <code>ProviderRegistry.get()</code>.</li> <li>Model prefix inference -- If the model name contains <code>/</code> (e.g., <code>anthropic/claude-sonnet-4</code>), extract the provider prefix.</li> <li>Alias matching -- All aliases are checked case-insensitively (e.g., <code>claude</code> resolves to <code>anthropic</code>).</li> </ol> <pre><code># All of these resolve to the Anthropic provider:\nconnector.connect_to_model(\"claude-sonnet-4-5-20250929\", model_type=\"anthropic\")\nconnector.connect_to_model(\"anthropic/claude-sonnet-4-5-20250929\")\nconnector.connect_to_model(\"claude-sonnet-4-5-20250929\", model_type=\"claude\")\n</code></pre>"},{"location":"integrations/llm-providers/#environment-variables-reference","title":"Environment Variables Reference","text":"Variable Provider(s) Purpose <code>OPENAI_API_KEY</code> OpenAI API key <code>OPENAI_API_BASE</code> OpenAI Base URL override <code>ANTHROPIC_API_KEY</code> Anthropic API key <code>GEMINI_API_KEY</code> Gemini API key <code>GOOGLE_API_KEY</code> Gemini (fallback) API key <code>OLLAMA_HOST</code> Ollama Server URL <code>LMSTUDIO_BASE_URL</code> LM Studio Server URL <code>VLLM_BASE_URL</code> vLLM Server URL <code>SGLANG_BASE_URL</code> SGLang Server URL <code>TGI_BASE_URL</code> TGI Server URL <code>MLX_BASE_URL</code> MLX Server URL <code>LLAMACPP_BASE_URL</code> llama.cpp Server URL <code>XAI_API_KEY</code> xAI API key <code>DEEPSEEK_API_KEY</code> DeepSeek API key <code>OPENROUTER_API_KEY</code> OpenRouter API key <code>OPENCODE_API_KEY</code> OpenCode API key (optional) <code>GROQ_API_KEY</code> Groq API key <code>TOGETHER_API_KEY</code> Together AI API key <code>MISTRAL_API_KEY</code> Mistral API key <code>MOONSHOT_API_KEY</code> Moonshot API key <code>DASHSCOPE_API_KEY</code> Alibaba/Qwen API key <code>SILICONFLOW_API_KEY</code> SiliconFlow API key <code>FIREWORKS_API_KEY</code> Fireworks API key <code>PERPLEXITY_API_KEY</code> Perplexity API key <code>CEREBRAS_API_KEY</code> Cerebras API key <code>OLLAMA_HTTP_TIMEOUT</code> Ollama HTTP timeout (default 120s)"},{"location":"integrations/mcp/","title":"MCP Server","text":"<p>RLM Code includes a full Model Context Protocol (MCP) implementation with both a server (exposing RLM capabilities to external clients) and a client manager (connecting to external MCP servers).</p>"},{"location":"integrations/mcp/#module","title":"Module","text":"<pre><code>rlm_code.mcp.server          -- MCP server and tool definitions\nrlm_code.mcp.client_manager  -- Client connection manager\nrlm_code.mcp.session_wrapper -- Session lifecycle wrapper\nrlm_code.mcp.config          -- Configuration dataclasses\nrlm_code.mcp.transports      -- Transport factory (stdio, SSE, WebSocket)\nrlm_code.mcp.exceptions      -- MCP-specific error hierarchy\n</code></pre>"},{"location":"integrations/mcp/#rlm-mcp-server","title":"RLM MCP Server","text":"<p>The <code>RLMServer</code> exposes five tools over the MCP protocol, allowing Claude Desktop, VS Code, and other MCP clients to execute RLM tasks directly.</p>"},{"location":"integrations/mcp/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    Client[\"MCP Client&lt;br&gt;(Claude Desktop, VS Code)\"] --&gt;|JSON-RPC| Server[\"RLMServer\"]\n    Server --&gt;|initialize| Init[\"Protocol handshake\"]\n    Server --&gt;|tools/list| List[\"Return 5 tool schemas\"]\n    Server --&gt;|tools/call| Dispatch[\"Route to handler\"]\n    Dispatch --&gt; Execute[\"rlm_execute\"]\n    Dispatch --&gt; Query[\"rlm_query\"]\n    Dispatch --&gt; Compare[\"rlm_compare\"]\n    Dispatch --&gt; Benchmark[\"rlm_benchmark\"]\n    Dispatch --&gt; Trajectory[\"rlm_trajectory\"]\n    Execute --&gt; Runner[\"RLM Runner\"]\n    Query --&gt; Runner\n    Compare --&gt; Runner</code></pre>"},{"location":"integrations/mcp/#server-configuration","title":"Server Configuration","text":"<pre><code>from rlm_code.mcp.server import ServerConfig, create_rlm_server\n\nconfig = ServerConfig(\n    name=\"rlm-code\",\n    version=\"1.0.0\",\n    transport=\"stdio\",\n    host=\"127.0.0.1\",\n    port=8765,\n)\n\nserver = create_rlm_server(config)\n</code></pre> Field Type Default Description <code>name</code> <code>str</code> <code>\"rlm-code\"</code> Server name in MCP handshake <code>version</code> <code>str</code> <code>\"1.0.0\"</code> Server version <code>transport</code> <code>str</code> <code>\"stdio\"</code> Transport type: <code>stdio</code>, <code>http</code>, <code>websocket</code> <code>host</code> <code>str</code> <code>\"127.0.0.1\"</code> Host for HTTP/WebSocket <code>port</code> <code>int</code> <code>8765</code> Port for HTTP/WebSocket"},{"location":"integrations/mcp/#running-the-server","title":"Running the Server","text":"Stdio (default)Programmatic <pre><code>python -m rlm_code.mcp.server.rlm_server\n</code></pre> <pre><code>import asyncio\nfrom rlm_code.mcp.server import create_rlm_server\n\nasync def main():\n    server = create_rlm_server()\n    await server.run()\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/mcp/#claude-desktop-integration","title":"Claude Desktop Integration","text":"<p>Add to your Claude Desktop <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"rlm-code\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"rlm_code.mcp.server.rlm_server\"]\n    }\n  }\n}\n</code></pre>"},{"location":"integrations/mcp/#tools","title":"Tools","text":"<p>The server exposes five tools via the <code>RLMTools</code> class. Each tool is defined as a <code>ToolDefinition</code> with typed <code>ToolParameter</code> entries.</p>"},{"location":"integrations/mcp/#rlm_execute","title":"<code>rlm_execute</code>","text":"<p>Execute a task using the RLM paradigm.</p> Parameter Type Required Default Description <code>task</code> <code>string</code> Yes -- The task to execute <code>context</code> <code>string</code> No -- Context data (text, JSON, or file path) <code>paradigm</code> <code>string</code> No <code>\"pure_rlm\"</code> <code>pure_rlm</code>, <code>codeact</code>, or <code>traditional</code> <code>max_steps</code> <code>integer</code> No <code>6</code> Maximum REPL iterations <code>timeout</code> <code>integer</code> No <code>60</code> Execution timeout in seconds <code>max_depth</code> <code>integer</code> No <code>2</code> Maximum recursion depth for child agents <p>Response fields: <code>run_id</code>, <code>completed</code>, <code>answer</code>, <code>steps</code>, <code>paradigm</code>, <code>total_reward</code></p>"},{"location":"integrations/mcp/#rlm_query","title":"<code>rlm_query</code>","text":"<p>Query a large context efficiently using the RLM paradigm.</p> Parameter Type Required Default Description <code>question</code> <code>string</code> Yes -- The question to answer <code>context</code> <code>string</code> Yes -- Context data to query <code>max_steps</code> <code>integer</code> No <code>4</code> Maximum analysis iterations <p>Response fields: <code>question</code>, <code>answer</code>, <code>context_length</code>, <code>steps_used</code></p> <p>Token Efficiency</p> <p><code>rlm_query</code> stores context as a variable rather than in the token window, enabling efficient processing of contexts that would overflow standard prompt limits.</p>"},{"location":"integrations/mcp/#rlm_compare","title":"<code>rlm_compare</code>","text":"<p>Compare different RLM paradigms on the same task.</p> Parameter Type Required Default Description <code>task</code> <code>string</code> Yes -- Task to run across paradigms <code>context</code> <code>string</code> No -- Context data <code>paradigms</code> <code>string</code> No <code>\"pure_rlm,codeact\"</code> Comma-separated paradigm list <code>max_steps</code> <code>integer</code> No <code>5</code> Maximum steps per paradigm <p>Response fields: <code>task</code>, <code>comparison_id</code>, <code>results[]</code> (paradigm, success, answer, tokens, duration, iterations), <code>summary</code></p>"},{"location":"integrations/mcp/#rlm_benchmark","title":"<code>rlm_benchmark</code>","text":"<p>Run benchmark presets to evaluate paradigm performance.</p> Parameter Type Required Default Description <code>preset</code> <code>string</code> Yes -- Benchmark preset name <code>limit</code> <code>integer</code> No <code>3</code> Maximum cases to run <p>Available presets:</p> Preset Description <code>pure_rlm_smoke</code> Quick validation of the Pure RLM pipeline <code>pure_rlm_context</code> Context-handling test cases <code>oolong_style</code> OOLONG benchmark-compatible tests <code>browsecomp_style</code> BrowseComp benchmark-compatible tests <code>token_efficiency</code> Token usage comparison across paradigms <code>paradigm_comparison</code> Head-to-head paradigm comparison <code>deep_recursion</code> Multi-level recursive agent tests <p>Response fields: <code>preset</code>, <code>cases_run</code>, <code>cases_completed</code>, <code>completion_rate</code>, <code>avg_steps</code>, <code>avg_reward</code>, <code>total_time_seconds</code></p>"},{"location":"integrations/mcp/#rlm_trajectory","title":"<code>rlm_trajectory</code>","text":"<p>View or export an RLM execution trajectory.</p> Parameter Type Required Default Description <code>run_id</code> <code>string</code> No <code>\"latest\"</code> Run ID or <code>\"latest\"</code> <code>format</code> <code>string</code> No <code>\"summary\"</code> <code>tree</code>, <code>json</code>, <code>html</code>, or <code>summary</code> <p>Response fields: Varies by format -- <code>summary</code> returns stats, <code>tree</code> returns a tree string, <code>json</code> returns events array.</p>"},{"location":"integrations/mcp/#tool-call-result","title":"Tool Call Result","text":"<p>All tool handlers return a <code>ToolCallResult</code>:</p> <pre><code>from rlm_code.mcp.server import ToolCallResult\n\n# Success\nresult = ToolCallResult(success=True, content={\"answer\": \"42\"})\n\n# Error\nresult = ToolCallResult(success=False, content=None, error=\"Task is required\")\n</code></pre> <p>The <code>to_mcp_response()</code> method converts to the MCP response format:</p> <ul> <li>Success: <code>{\"content\": [{\"type\": \"text\", \"text\": \"...\"}]}</code></li> <li>Error: <code>{\"isError\": true, \"content\": [{\"type\": \"text\", \"text\": \"error message\"}]}</code></li> </ul>"},{"location":"integrations/mcp/#mcp-client-manager","title":"MCP Client Manager","text":"<p>The <code>MCPClientManager</code> connects to external MCP servers and manages their lifecycle.</p>"},{"location":"integrations/mcp/#constructor","title":"Constructor","text":"<pre><code>from rlm_code.mcp.client_manager import MCPClientManager\nfrom rlm_code.core.config import ConfigManager\n\nmanager = MCPClientManager(config_manager=ConfigManager())\n</code></pre>"},{"location":"integrations/mcp/#key-methods","title":"Key Methods","text":"Method Signature Description <code>add_server()</code> <code>async (config: MCPServerConfig) -&gt; None</code> Add a server configuration <code>remove_server()</code> <code>async (server_name: str) -&gt; None</code> Remove a server configuration <code>list_servers()</code> <code>async () -&gt; list[dict]</code> List servers with connection status <code>connect()</code> <code>async (server_name, use_retry?) -&gt; MCPSessionWrapper</code> Connect with optional retry <code>disconnect()</code> <code>async (server_name: str) -&gt; None</code> Disconnect from a server <code>get_session()</code> <code>async (server_name) -&gt; MCPSessionWrapper | None</code> Get active session <code>list_tools()</code> <code>async (server_name?) -&gt; dict[str, list[Tool]]</code> List tools from connected servers <code>call_tool()</code> <code>async (server, tool, args?) -&gt; CallToolResult</code> Invoke a tool on a server <code>list_resources()</code> <code>async (server_name?) -&gt; dict[str, list[Resource]]</code> List resources <code>read_resource()</code> <code>async (server, uri) -&gt; ReadResourceResult</code> Read a resource by URI <code>list_prompts()</code> <code>async (server_name?) -&gt; dict[str, list[Prompt]]</code> List prompts <code>get_prompt()</code> <code>async (server, name, args?) -&gt; GetPromptResult</code> Get a prompt with arguments <code>cleanup()</code> <code>async () -&gt; None</code> Close all connections <p>Auto-Reconnect</p> <p>When the client detects a closed connection during a tool call or listing operation, it automatically reconnects and retries the operation once.</p>"},{"location":"integrations/mcp/#connection-lifecycle","title":"Connection Lifecycle","text":"<pre><code>import asyncio\nfrom rlm_code.mcp.client_manager import MCPClientManager\nfrom rlm_code.mcp.config import MCPServerConfig, MCPTransportConfig\n\nasync def example():\n    manager = MCPClientManager(config_manager=config)\n\n    # Add server\n    server_config = MCPServerConfig(\n        name=\"my-server\",\n        transport=MCPTransportConfig(\n            type=\"stdio\",\n            command=\"npx\",\n            args=[\"@my-org/my-mcp-server\"],\n        ),\n    )\n    await manager.add_server(server_config)\n\n    # Connect\n    session = await manager.connect(\"my-server\")\n\n    # List and call tools\n    tools = await manager.list_tools(\"my-server\")\n    result = await manager.call_tool(\"my-server\", \"my_tool\", {\"arg\": \"value\"})\n\n    # Cleanup\n    await manager.cleanup()\n</code></pre>"},{"location":"integrations/mcp/#configuration","title":"Configuration","text":""},{"location":"integrations/mcp/#mcptransportconfig","title":"<code>MCPTransportConfig</code>","text":"<p>Transport-specific settings for MCP connections.</p> <pre><code>from rlm_code.mcp.config import MCPTransportConfig\n\n# Stdio transport\nstdio = MCPTransportConfig(\n    type=\"stdio\",\n    command=\"npx\",\n    args=[\"@anthropic-ai/mcp-filesystem\"],\n    env={\"HOME\": \"${HOME}\"},\n)\n\n# SSE transport\nsse = MCPTransportConfig(\n    type=\"sse\",\n    url=\"https://api.example.com/mcp\",\n    headers={\"Authorization\": \"Bearer ${API_TOKEN}\"},\n)\n\n# WebSocket transport\nws = MCPTransportConfig(\n    type=\"websocket\",\n    url=\"wss://api.example.com/mcp/ws\",\n)\n</code></pre> Field Type Description <code>type</code> <code>str</code> <code>\"stdio\"</code>, <code>\"sse\"</code>, or <code>\"websocket\"</code> <code>command</code> <code>str | None</code> Command for stdio transport <code>args</code> <code>list[str] | None</code> Arguments for stdio command <code>env</code> <code>dict[str, str] | None</code> Environment variables for stdio <code>url</code> <code>str | None</code> URL for SSE/WebSocket transport <code>headers</code> <code>dict[str, str] | None</code> HTTP headers for SSE/WebSocket <code>auth_type</code> <code>str | None</code> <code>\"bearer\"</code>, <code>\"basic\"</code>, or <code>\"oauth\"</code> <code>auth_token</code> <code>str | None</code> Authentication token <p>Environment Variable Resolution</p> <p><code>MCPTransportConfig.resolve_env_vars()</code> replaces <code>${VAR_NAME}</code> patterns in <code>env</code>, <code>headers</code>, <code>auth_token</code>, and <code>url</code> fields with actual environment variable values.</p>"},{"location":"integrations/mcp/#mcpserverconfig","title":"<code>MCPServerConfig</code>","text":"<p>Server-level configuration wrapping transport settings.</p> <pre><code>from rlm_code.mcp.config import MCPServerConfig\n\nconfig = MCPServerConfig(\n    name=\"filesystem\",\n    description=\"File system access\",\n    transport=MCPTransportConfig(type=\"stdio\", command=\"npx\", args=[\"@anthropic-ai/mcp-filesystem\"]),\n    enabled=True,\n    auto_connect=False,\n    timeout_seconds=30,\n    retry_attempts=3,\n)\n</code></pre> Field Type Default Description <code>name</code> <code>str</code> -- Server identifier (required) <code>description</code> <code>str | None</code> <code>None</code> Human-readable description <code>transport</code> <code>MCPTransportConfig</code> stdio Transport configuration <code>enabled</code> <code>bool</code> <code>True</code> Whether the server is active <code>auto_connect</code> <code>bool</code> <code>False</code> Connect automatically on startup <code>timeout_seconds</code> <code>int</code> <code>30</code> Connection timeout <code>retry_attempts</code> <code>int</code> <code>3</code> Number of retry attempts"},{"location":"integrations/mcp/#session-wrapper","title":"Session Wrapper","text":"<p><code>MCPSessionWrapper</code> wraps the MCP <code>ClientSession</code> with additional status tracking and operation delegation.</p> <pre><code>from rlm_code.mcp.session_wrapper import MCPSessionWrapper\n</code></pre> <p>Key capabilities:</p> <ul> <li>Status tracking -- <code>get_status()</code> returns connection state and metadata</li> <li>Tool operations -- <code>list_tools()</code>, <code>call_tool()</code></li> <li>Resource operations -- <code>list_resources()</code>, <code>read_resource()</code></li> <li>Prompt operations -- <code>list_prompts()</code>, <code>get_prompt()</code></li> <li>Lifecycle management -- <code>initialize()</code>, <code>close()</code></li> </ul>"},{"location":"integrations/mcp/#transport-factory","title":"Transport Factory","text":"<p>The transport factory creates the appropriate transport for a given configuration:</p> <pre><code>from rlm_code.mcp.transports.factory import MCPTransportFactory\n\ntransport_cm = MCPTransportFactory.create_transport(transport_config)\n</code></pre> Transport Type Module Description <code>stdio</code> <code>rlm_code.mcp.transports.stdio_transport</code> Subprocess stdin/stdout <code>sse</code> <code>rlm_code.mcp.transports.sse_transport</code> Server-Sent Events over HTTP <code>websocket</code> <code>rlm_code.mcp.transports.websocket_transport</code> WebSocket connection"},{"location":"integrations/mcp/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All MCP exceptions inherit from <code>MCPError</code> and include <code>details</code> dictionaries and troubleshooting messages.</p> <pre><code>MCPError (base)\n +-- MCPConnectionError      -- Connection failures (per transport type)\n +-- MCPConfigurationError   -- Invalid config, missing fields\n +-- MCPOperationError       -- Tool call or resource access failures\n +-- MCPTransportError       -- Transport-level errors\n +-- MCPTimeoutError         -- Operation timeouts\n</code></pre> <p>Each exception class provides a <code>get_troubleshooting_message()</code> method with context-specific guidance:</p> <pre><code>from rlm_code.mcp.exceptions import MCPConnectionError\n\ntry:\n    await manager.connect(\"my-server\")\nexcept MCPConnectionError as e:\n    print(e.get_troubleshooting_message())\n    # Troubleshooting:\n    #   1. Ensure the server command is installed and in your PATH\n    #   2. Verify the command and arguments in your configuration\n    #   ...\n</code></pre>"},{"location":"integrations/mcp/#yaml-configuration","title":"YAML Configuration","text":"<p>MCP servers can be configured in <code>rlm_config.yaml</code>:</p> <pre><code>mcp_servers:\n  filesystem:\n    name: filesystem\n    description: \"File system access\"\n    transport:\n      type: stdio\n      command: npx\n      args: [\"@anthropic-ai/mcp-filesystem\"]\n    enabled: true\n    auto_connect: false\n    timeout_seconds: 30\n    retry_attempts: 3\n\n  api-server:\n    name: api-server\n    description: \"Remote API server\"\n    transport:\n      type: sse\n      url: \"https://api.example.com/mcp\"\n      headers:\n        Authorization: \"Bearer ${API_TOKEN}\"\n    enabled: true\n    timeout_seconds: 60\n</code></pre>"},{"location":"integrations/mcp/#class-reference","title":"Class Reference","text":""},{"location":"integrations/mcp/#rlmserver","title":"RLMServer","text":"Method / Property Description <code>config</code> <code>ServerConfig</code> instance <code>handle_initialize()</code> Handle MCP initialize handshake <code>handle_tools_list()</code> Return tool schemas <code>handle_tools_call()</code> Dispatch tool call to handler <code>handle_message()</code> Route incoming MCP JSON-RPC message <code>run_stdio()</code> Run server over stdio transport <code>run()</code> Run server (dispatches to transport method)"},{"location":"integrations/mcp/#rlmtools","title":"RLMTools","text":"Method Description <code>rlm_execute()</code> <code>ToolDefinition</code> for task execution <code>rlm_query()</code> <code>ToolDefinition</code> for context querying <code>rlm_compare()</code> <code>ToolDefinition</code> for paradigm comparison <code>rlm_benchmark()</code> <code>ToolDefinition</code> for benchmark execution <code>rlm_trajectory()</code> <code>ToolDefinition</code> for trajectory viewing <code>all_tools()</code> List of all <code>ToolDefinition</code> instances <code>to_mcp_tools()</code> Convert all tools to MCP JSON schema format"},{"location":"observability/","title":"Observability","text":"<p>RLM Code ships with a pluggable, multi-sink observability architecture that captures every run start, step event, and run completion, then fans them out to one or more telemetry backends in real time. Whether you are debugging locally with JSONL files or shipping distributed traces to a production Jaeger cluster, the system adapts without code changes -- you only toggle environment variables.</p>"},{"location":"observability/#architecture-at-a-glance","title":"Architecture at a Glance","text":"<pre><code>flowchart LR\n    Runner[\"RLMRunner\"] --&gt;|events| Obs[\"RLMObservability\"]\n    Obs --&gt; S1[\"LocalJSONLSink\"]\n    Obs --&gt; S2[\"MLflowSink\"]\n    Obs --&gt; S3[\"OpenTelemetrySink\"]\n    Obs --&gt; S4[\"LangSmithSink\"]\n    Obs --&gt; S5[\"LangFuseSink\"]\n    Obs --&gt; S6[\"LogfireSink\"]\n    Obs --&gt; S7[\"CompositeSink\"]\n    S7 --&gt; S7a[\"Custom Sink A\"]\n    S7 --&gt; S7b[\"Custom Sink B\"]</code></pre> <p>The central coordinator, <code>RLMObservability</code>, iterates over its list of sinks and calls each one inside a <code>try/except</code> guard. A failing sink never crashes the run.</p>"},{"location":"observability/#the-rlmobservabilitysink-protocol","title":"The RLMObservabilitySink Protocol","text":"<p>Every sink -- built-in or custom -- must satisfy the <code>RLMObservabilitySink</code> structural protocol defined in <code>rlm_code.rlm.observability</code>:</p> <pre><code>class RLMObservabilitySink(Protocol):\n    \"\"\"Sink contract for RLM observability events.\"\"\"\n\n    name: str\n\n    def status(self) -&gt; dict[str, Any]:\n        \"\"\"Return sink status for CLI visibility.\"\"\"\n        ...\n\n    def on_run_start(\n        self,\n        run_id: str,\n        *,\n        task: str,\n        environment: str,\n        params: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Hook called at run start.\"\"\"\n        ...\n\n    def on_step(\n        self,\n        run_id: str,\n        *,\n        event: dict[str, Any],\n        cumulative_reward: float,\n    ) -&gt; None:\n        \"\"\"Hook called after each step event.\"\"\"\n        ...\n\n    def on_run_end(\n        self,\n        run_id: str,\n        *,\n        result: Any,\n        run_path: Path,\n    ) -&gt; None:\n        \"\"\"Hook called once at run completion.\"\"\"\n        ...\n</code></pre> Method When It Fires Key Arguments <code>on_run_start</code> Immediately before the first iteration <code>run_id</code>, <code>task</code>, <code>environment</code>, <code>params</code> <code>on_step</code> After every iteration completes <code>run_id</code>, step <code>event</code> dict, <code>cumulative_reward</code> <code>on_run_end</code> After the run finishes (success or failure) <code>run_id</code>, <code>result</code> object, <code>run_path</code> <code>status</code> Any time the CLI queries sink health Returns a dict with <code>name</code>, <code>enabled</code>, <code>available</code>, <code>detail</code>"},{"location":"observability/#available-sinks","title":"Available Sinks","text":"<p>RLM Code provides 7 sinks out of the box:</p> # Sink Class Always Active? Activation 1 Local JSONL <code>LocalJSONLSink</code> Yes (default) <code>DSPY_RLM_OBS_LOCAL_JSONL=true</code> 2 MLflow <code>MLflowSink</code> No <code>DSPY_RLM_MLFLOW_ENABLED=true</code> + <code>MLFLOW_TRACKING_URI</code> 3 OpenTelemetry <code>OpenTelemetrySink</code> No <code>DSPY_RLM_OTEL_ENABLED=true</code> + <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> 4 LangSmith <code>LangSmithSink</code> No <code>DSPY_RLM_LANGSMITH_ENABLED=true</code> + <code>LANGCHAIN_API_KEY</code> 5 LangFuse <code>LangFuseSink</code> No <code>DSPY_RLM_LANGFUSE_ENABLED=true</code> + <code>LANGFUSE_PUBLIC_KEY</code> + <code>LANGFUSE_SECRET_KEY</code> 6 Logfire <code>LogfireSink</code> No <code>DSPY_RLM_LOGFIRE_ENABLED=true</code> + <code>LOGFIRE_TOKEN</code> 7 Composite <code>CompositeSink</code> N/A (wrapper) Programmatic only <p>Master Switch</p> <p>Set <code>DSPY_RLM_OBS_ENABLED=false</code> to disable all observability sinks at once. The default is <code>true</code>.</p>"},{"location":"observability/#automatic-activation-from-environment-variables","title":"Automatic Activation from Environment Variables","text":"<p>When <code>RLMObservability.default()</code> is called (which happens automatically at the start of every RLM run), it reads the following environment variables and instantiates sinks accordingly:</p> Environment Variable Default Description <code>DSPY_RLM_OBS_ENABLED</code> <code>true</code> Master switch for all observability <code>DSPY_RLM_OBS_LOCAL_JSONL</code> <code>true</code> Enable the local JSONL file sink <code>DSPY_RLM_MLFLOW_ENABLED</code> <code>false</code> Enable MLflow experiment tracking <code>DSPY_RLM_MLFLOW_EXPERIMENT</code> <code>rlm-code-rlm</code> MLflow experiment name <code>MLFLOW_TRACKING_URI</code> (none) MLflow server URI <code>DSPY_RLM_OTEL_ENABLED</code> <code>false</code> Enable OpenTelemetry tracing <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> (none) OTLP gRPC endpoint <code>OTEL_SERVICE_NAME</code> <code>rlm-code</code> OTEL service name <code>DSPY_RLM_OTEL_METRICS_ENABLED</code> <code>true</code> Enable OTEL metrics alongside traces <code>DSPY_RLM_LANGSMITH_ENABLED</code> <code>false</code> Enable LangSmith tracing <code>LANGCHAIN_API_KEY</code> (none) LangSmith API key <code>LANGCHAIN_PROJECT</code> <code>rlm-code</code> LangSmith project name <code>DSPY_RLM_LANGFUSE_ENABLED</code> <code>false</code> Enable LangFuse observability <code>LANGFUSE_PUBLIC_KEY</code> (none) LangFuse public API key <code>LANGFUSE_SECRET_KEY</code> (none) LangFuse secret API key <code>LANGFUSE_HOST</code> <code>https://cloud.langfuse.com</code> LangFuse host URL <code>DSPY_RLM_LOGFIRE_ENABLED</code> <code>false</code> Enable Logfire (Pydantic) tracing <code>LOGFIRE_TOKEN</code> (none) Logfire API token <code>LOGFIRE_PROJECT_NAME</code> <code>rlm-code</code> Logfire project name <p>Multiple Sinks Simultaneously</p> <p>You can activate as many sinks as you like. For example, you might keep the local JSONL sink for offline analysis, MLflow for experiment tracking dashboards, and OpenTelemetry for production tracing -- all at the same time.</p>"},{"location":"observability/#runtime-sink-management","title":"Runtime Sink Management","text":"<p>The <code>RLMObservability</code> coordinator exposes three methods for runtime sink management:</p>"},{"location":"observability/#adding-a-sink","title":"Adding a Sink","text":"<pre><code>from rlm_code.rlm.observability import RLMObservability\n\nobs = RLMObservability.default(workdir=workdir, run_dir=run_dir)\n\n# Add a custom sink at runtime\nobs.add_sink(my_custom_sink)\n</code></pre>"},{"location":"observability/#removing-a-sink","title":"Removing a Sink","text":"<pre><code>removed = obs.remove_sink(\"mlflow\")  # Returns True if found and removed\n</code></pre>"},{"location":"observability/#retrieving-a-sink","title":"Retrieving a Sink","text":"<pre><code>otel_sink = obs.get_sink(\"opentelemetry\")\nif otel_sink:\n    trace_id = otel_sink.get_trace_id(run_id)\n</code></pre>"},{"location":"observability/#querying-sink-status","title":"Querying Sink Status","text":"<pre><code>for sink_status in obs.status():\n    print(f\"{sink_status['name']}: enabled={sink_status['enabled']}, \"\n          f\"available={sink_status['available']}, detail={sink_status['detail']}\")\n</code></pre> <p>Example output:</p> <pre><code>local-jsonl: enabled=True, available=True, detail=/home/user/.rlm_code/rlm/observability\nmlflow: enabled=True, available=True, detail=http://localhost:5000\nopentelemetry: enabled=False, available=False, detail=disabled\nlangsmith: enabled=False, available=False, detail=disabled\nlangfuse: enabled=False, available=False, detail=disabled\nlogfire: enabled=False, available=False, detail=disabled\n</code></pre>"},{"location":"observability/#event-flow","title":"Event Flow","text":"<p>Every RLM run follows this lifecycle through the observability system:</p> <pre><code>sequenceDiagram\n    participant R as RLMRunner\n    participant O as RLMObservability\n    participant S as Sink (any)\n\n    R-&gt;&gt;O: on_run_start(run_id, task, env, params)\n    O-&gt;&gt;S: on_run_start(...)\n\n    loop Each Iteration\n        R-&gt;&gt;O: on_step(run_id, event, cumulative_reward)\n        O-&gt;&gt;S: on_step(...)\n    end\n\n    R-&gt;&gt;O: on_run_end(run_id, result, run_path)\n    O-&gt;&gt;S: on_run_end(...)</code></pre> <p>Error Isolation</p> <p>If any sink raises an exception during any hook, the <code>RLMObservability</code> coordinator catches it, logs a warning, and proceeds to the next sink. Your run is never interrupted by a sink failure.</p>"},{"location":"observability/#quick-start","title":"Quick Start","text":"<p>Enable MLflow and OpenTelemetry alongside the default local sink:</p> <pre><code>export DSPY_RLM_MLFLOW_ENABLED=true\nexport MLFLOW_TRACKING_URI=http://localhost:5000\nexport DSPY_RLM_OTEL_ENABLED=true\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\n\nrlm-code run --task \"Build a DSPy signature\" --environment dspy\n</code></pre> <p>All three sinks will receive the same events in parallel.</p>"},{"location":"observability/#whats-next","title":"What's Next","text":"Page Description Sink Architecture Deep dive into the sink protocol, the <code>CompositeSink</code>, custom sink creation, and factory functions MLflow MLflow experiment tracking integration OpenTelemetry Distributed tracing with OTEL, Jaeger, and Zipkin LangSmith LangChain's LLM observability platform LangFuse Open-source LLM observability Logfire Pydantic's structured observability platform"},{"location":"observability/langfuse/","title":"LangFuse Integration","text":"<p>The <code>LangFuseSink</code> sends RLM traces to LangFuse, an open-source LLM observability platform that provides trace visualization, cost tracking, prompt management, and evaluation tools.</p>"},{"location":"observability/langfuse/#overview","title":"Overview","text":"Property Value Class <code>rlm_code.rlm.observability_sinks.LangFuseSink</code> Sink name <code>langfuse</code> Activation <code>DSPY_RLM_LANGFUSE_ENABLED=true</code> Primary env vars <code>LANGFUSE_PUBLIC_KEY</code> + <code>LANGFUSE_SECRET_KEY</code> Optional dependency <code>pip install langfuse</code>"},{"location":"observability/langfuse/#activation","title":"Activation","text":"<pre><code>export DSPY_RLM_LANGFUSE_ENABLED=true\nexport LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nexport LANGFUSE_SECRET_KEY=sk-lf-your-secret-key\nexport LANGFUSE_HOST=https://cloud.langfuse.com  # optional, default\n</code></pre> <p>Self-Hosted LangFuse</p> <p>If you are running a self-hosted LangFuse instance, set <code>LANGFUSE_HOST</code> to your instance URL (e.g., <code>http://localhost:3000</code>).</p>"},{"location":"observability/langfuse/#features","title":"Features","text":""},{"location":"observability/langfuse/#open-source-llm-observability","title":"Open-Source LLM Observability","text":"<p>LangFuse is fully open-source and can be self-hosted. The RLM sink provides:</p> <ul> <li>Trace-level visibility into every RLM run</li> <li>Span-level detail for each step</li> <li>Automatic scoring of traces based on reward and completion</li> <li>Tag-based organization for filtering by environment</li> </ul>"},{"location":"observability/langfuse/#trace-visualization","title":"Trace Visualization","text":"<p>Each RLM run creates a trace in LangFuse:</p> Trace Field Value <code>id</code> The RLM <code>run_id</code> <code>name</code> <code>rlm-run</code> <code>input</code> Task text <code>metadata</code> Environment, params dict <code>tags</code> <code>[\"rlm\", \"&lt;environment&gt;\"]</code> <p>At run end, the trace is updated with output data:</p> Output Field Description <code>completed</code> Whether the run completed <code>steps</code> Total steps taken <code>total_reward</code> Final reward <code>final_answer</code> The final answer (first 500 chars)"},{"location":"observability/langfuse/#step-spans","title":"Step Spans","text":"<p>Each step creates a span nested under the trace:</p> Span Field Value <code>name</code> <code>step-&lt;n&gt;</code> <code>input</code> Action type and code (first 500 chars) <code>metadata</code> Step number, reward, cumulative reward <code>output</code> Success flag and output (first 500 chars) <code>level</code> <code>ERROR</code> if the step failed, <code>DEFAULT</code> otherwise <code>status_message</code> Error message if the step failed"},{"location":"observability/langfuse/#cost-tracking","title":"Cost Tracking","text":"<p>LangFuse automatically tracks token usage and cost when using its LLM integrations. The RLM sink provides per-step and per-run metrics that LangFuse uses to aggregate cost data across your project.</p>"},{"location":"observability/langfuse/#automatic-scoring","title":"Automatic Scoring","text":"<p>At run end, the sink creates two scores on the trace:</p> Score Name Value Description <code>reward</code> <code>float</code> The total cumulative reward for the run <code>completed</code> <code>1.0</code> or <code>0.0</code> Whether the run completed successfully <p>These scores are visible in the LangFuse dashboard and can be used for filtering, aggregation, and evaluation.</p> <pre><code># Scores are created automatically at run end\nself._langfuse.score(\n    trace_id=run_id,\n    name=\"reward\",\n    value=float(getattr(result, \"total_reward\", 0.0)),\n)\nself._langfuse.score(\n    trace_id=run_id,\n    name=\"completed\",\n    value=1.0 if getattr(result, \"completed\", False) else 0.0,\n)\n</code></pre>"},{"location":"observability/langfuse/#automatic-flush","title":"Automatic Flush","text":"<p>The sink calls <code>self._langfuse.flush()</code> at the end of each run to ensure all data is sent to the LangFuse backend before the process exits.</p>"},{"location":"observability/langfuse/#setup-guide","title":"Setup Guide","text":""},{"location":"observability/langfuse/#option-a-langfuse-cloud","title":"Option A: LangFuse Cloud","text":"<ol> <li>Sign up at langfuse.com</li> <li>Create a project and obtain your API keys</li> <li>Configure environment variables:</li> </ol> <pre><code>export DSPY_RLM_LANGFUSE_ENABLED=true\nexport LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key\nexport LANGFUSE_SECRET_KEY=sk-lf-your-secret-key\n</code></pre>"},{"location":"observability/langfuse/#option-b-self-hosted-langfuse","title":"Option B: Self-Hosted LangFuse","text":"<ol> <li>Deploy LangFuse using Docker:</li> </ol> <pre><code>docker compose up -d\n</code></pre> <p>Refer to the LangFuse self-hosting guide for the full <code>docker-compose.yml</code>.</p> <ol> <li>Configure with your local instance:</li> </ol> <pre><code>export DSPY_RLM_LANGFUSE_ENABLED=true\nexport LANGFUSE_PUBLIC_KEY=pk-lf-your-local-key\nexport LANGFUSE_SECRET_KEY=sk-lf-your-local-key\nexport LANGFUSE_HOST=http://localhost:3000\n</code></pre>"},{"location":"observability/langfuse/#install-the-sdk","title":"Install the SDK","text":"<pre><code>pip install langfuse\n</code></pre>"},{"location":"observability/langfuse/#run-a-task","title":"Run a Task","text":"<pre><code>rlm-code run --task \"Analyze context with llm_query\" --environment pure_rlm\n</code></pre>"},{"location":"observability/langfuse/#view-traces","title":"View Traces","text":"<p>Open the LangFuse dashboard (cloud or self-hosted). Navigate to Traces and find the run by its <code>run_id</code>. The trace view shows:</p> <ul> <li>Timeline: Visual span hierarchy for each step</li> <li>Input/Output: Full task and result data</li> <li>Scores: Reward and completion scores</li> <li>Tags: Environment-based tags for filtering</li> </ul>"},{"location":"observability/langfuse/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Env Var Description <code>enabled</code> <code>bool</code> <code>False</code> <code>DSPY_RLM_LANGFUSE_ENABLED</code> Enable/disable the sink <code>host</code> <code>str | None</code> <code>None</code> <code>LANGFUSE_HOST</code> LangFuse host URL <p>API Keys</p> <p><code>LANGFUSE_PUBLIC_KEY</code> and <code>LANGFUSE_SECRET_KEY</code> are read directly by the <code>langfuse</code> Python SDK, not by the sink constructor. They must be set as environment variables.</p>"},{"location":"observability/langfuse/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from rlm_code.rlm.observability_sinks import LangFuseSink\n\nsink = LangFuseSink(\n    enabled=True,\n    host=\"http://localhost:3000\",\n)\n\nprint(sink.status())\n# {'name': 'langfuse', 'enabled': True, 'available': True,\n#  'detail': 'http://localhost:3000'}\n</code></pre>"},{"location":"observability/langfuse/#factory-function","title":"Factory Function","text":"<pre><code>from rlm_code.rlm.observability_sinks import create_langfuse_sink_from_env\n\n# Reads DSPY_RLM_LANGFUSE_ENABLED and LANGFUSE_HOST\nsink = create_langfuse_sink_from_env()\n</code></pre>"},{"location":"observability/langfuse/#connection-validation","title":"Connection Validation","text":"<p>During initialization, the sink validates the connection by calling <code>self._langfuse.auth_check()</code>:</p> <pre><code>try:\n    from langfuse import Langfuse\n\n    self._langfuse = Langfuse(host=self.host) if self.host else Langfuse()\n    self._langfuse.auth_check()\n    self._available = True\n    self._detail = self.host or \"https://cloud.langfuse.com\"\nexcept Exception as exc:\n    self._available = False\n    self._detail = f\"connection failed: {exc}\"\n</code></pre> <p>If the auth check fails, the sink becomes inactive and all subsequent hook calls return immediately.</p>"},{"location":"observability/langfuse/#trace-structure","title":"Trace Structure","text":"<p>A typical 3-step RLM run creates this structure in LangFuse:</p> <pre><code>Trace: rlm-run (id: abc12345)\n  Tags: [rlm, dspy]\n  Input: { task: \"Create a DSPy signature...\" }\n  |\n  +-- Span: step-1\n  |    Input: { action: \"run_python\", code: \"...\" }\n  |    Output: { success: true, output: \"...\" }\n  |    Metadata: { step: 1, reward: 0.5, cumulative_reward: 0.5 }\n  |\n  +-- Span: step-2\n  |    Input: { action: \"run_python\", code: \"...\" }\n  |    Output: { success: true, output: \"...\" }\n  |    Metadata: { step: 2, reward: 0.5, cumulative_reward: 1.0 }\n  |\n  +-- Span: step-3\n       Input: { action: \"submit\", code: \"\" }\n       Output: { success: true, output: \"...\" }\n       Metadata: { step: 3, reward: 0.5, cumulative_reward: 1.5 }\n  |\n  Output: { completed: true, steps: 3, total_reward: 1.5 }\n  Scores: reward=1.5, completed=1.0\n</code></pre>"},{"location":"observability/langfuse/#troubleshooting","title":"Troubleshooting","text":"Symptom Cause Solution <code>available: False</code>, <code>langfuse not installed</code> SDK not installed <code>pip install langfuse</code> <code>available: False</code>, <code>connection failed</code> Bad API keys or network Verify <code>LANGFUSE_PUBLIC_KEY</code> and <code>LANGFUSE_SECRET_KEY</code> Traces not appearing Sink not enabled Set <code>DSPY_RLM_LANGFUSE_ENABLED=true</code> Traces show in wrong host <code>LANGFUSE_HOST</code> mismatch Set <code>LANGFUSE_HOST</code> to the correct URL Missing scores Run did not complete <code>on_run_end</code> Check for run errors; scores are created at run end"},{"location":"observability/langsmith/","title":"LangSmith Integration","text":"<p>The <code>LangSmithSink</code> sends RLM run traces to LangSmith, LangChain's observability platform for LLM application debugging, testing, and monitoring.</p>"},{"location":"observability/langsmith/#overview","title":"Overview","text":"Property Value Class <code>rlm_code.rlm.observability_sinks.LangSmithSink</code> Sink name <code>langsmith</code> Activation <code>DSPY_RLM_LANGSMITH_ENABLED=true</code> Primary env var <code>LANGCHAIN_API_KEY</code> Optional dependency <code>pip install langsmith</code>"},{"location":"observability/langsmith/#activation","title":"Activation","text":"<pre><code>export DSPY_RLM_LANGSMITH_ENABLED=true\nexport LANGCHAIN_API_KEY=ls-your-api-key-here\nexport LANGCHAIN_PROJECT=rlm-code              # optional, default: rlm-code\nexport LANGCHAIN_TRACING_V2=true               # auto-set by the sink if not present\n</code></pre> <p>LANGCHAIN_TRACING_V2</p> <p>The sink automatically calls <code>os.environ.setdefault(\"LANGCHAIN_TRACING_V2\", \"true\")</code> during initialization. You do not need to set this variable manually unless you want to ensure it is set before any other LangChain code runs.</p>"},{"location":"observability/langsmith/#features","title":"Features","text":""},{"location":"observability/langsmith/#run-tracing","title":"Run Tracing","text":"<p>Each RLM run is represented as a root RunTree in LangSmith:</p> <ul> <li>Name: <code>rlm-run-&lt;first 8 chars of run_id&gt;</code></li> <li>Run type: <code>chain</code></li> <li>Project: Configurable via <code>LANGCHAIN_PROJECT</code> (default: <code>rlm-code</code>)</li> <li>Inputs: Task text, environment name, and full parameters dict</li> <li>Metadata: <code>run_id</code> and <code>environment</code></li> </ul>"},{"location":"observability/langsmith/#step-level-child-runs","title":"Step-Level Child Runs","text":"<p>Each step is created as a child run under the root:</p> Field Source Name <code>step-&lt;n&gt;</code> Run type <code>tool</code> Inputs Action type and code (first 500 chars) Outputs Success flag, output (first 500 chars), reward, cumulative reward Error Set if the step did not succeed"},{"location":"observability/langsmith/#run-completion","title":"Run Completion","text":"<p>At run end, the root <code>RunTree</code> is updated with outputs:</p> Output Field Description <code>completed</code> Whether the run completed successfully <code>steps</code> Total number of steps taken <code>total_reward</code> Final cumulative reward <code>final_answer</code> The final answer text (first 500 chars) <p>If the run did not complete, an error message is attached.</p>"},{"location":"observability/langsmith/#feedback-collection","title":"Feedback Collection","text":"<p>LangSmith supports feedback/evaluation annotations on runs. While the sink does not automatically create feedback, you can add it via the LangSmith SDK using the <code>run_id</code> logged in the trace.</p>"},{"location":"observability/langsmith/#dataset-creation","title":"Dataset Creation","text":"<p>You can use LangSmith's dataset features to create evaluation datasets from RLM benchmark results. Export runs from the LangSmith UI or use the SDK to query by project name.</p>"},{"location":"observability/langsmith/#setup-guide","title":"Setup Guide","text":""},{"location":"observability/langsmith/#1-create-a-langsmith-account","title":"1. Create a LangSmith Account","text":"<p>Sign up at smith.langchain.com and obtain an API key.</p>"},{"location":"observability/langsmith/#2-install-the-sdk","title":"2. Install the SDK","text":"<pre><code>pip install langsmith\n</code></pre>"},{"location":"observability/langsmith/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export DSPY_RLM_LANGSMITH_ENABLED=true\nexport LANGCHAIN_API_KEY=ls-your-api-key-here\nexport LANGCHAIN_PROJECT=rlm-code\n</code></pre>"},{"location":"observability/langsmith/#4-run-a-task","title":"4. Run a Task","text":"<pre><code>rlm-code run --task \"Create a DSPy module\" --environment dspy\n</code></pre>"},{"location":"observability/langsmith/#5-view-traces","title":"5. View Traces","text":"<p>Open smith.langchain.com, navigate to your project (<code>rlm-code</code>), and view the run traces. Each trace shows:</p> <ul> <li>The root run with inputs and outputs</li> <li>Child runs for each step with timing</li> <li>Input/output data for debugging</li> <li>Error details for failed steps</li> </ul>"},{"location":"observability/langsmith/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Env Var Description <code>enabled</code> <code>bool</code> <code>False</code> <code>DSPY_RLM_LANGSMITH_ENABLED</code> Enable/disable the sink <code>project</code> <code>str</code> <code>\"rlm-code\"</code> <code>LANGCHAIN_PROJECT</code> LangSmith project name"},{"location":"observability/langsmith/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from rlm_code.rlm.observability_sinks import LangSmithSink\n\nsink = LangSmithSink(\n    enabled=True,\n    project=\"my-rlm-project\",\n)\n\nprint(sink.status())\n# {'name': 'langsmith', 'enabled': True, 'available': True,\n#  'detail': 'project: my-rlm-project', 'project': 'my-rlm-project'}\n</code></pre>"},{"location":"observability/langsmith/#factory-function","title":"Factory Function","text":"<pre><code>from rlm_code.rlm.observability_sinks import create_langsmith_sink_from_env\n\n# Reads DSPY_RLM_LANGSMITH_ENABLED and LANGCHAIN_PROJECT\nsink = create_langsmith_sink_from_env()\n</code></pre>"},{"location":"observability/langsmith/#trace-structure","title":"Trace Structure","text":"<p>A typical RLM run creates this hierarchy in LangSmith:</p> <pre><code>rlm-run-abc12345 (chain)\n  |-- Inputs: { task: \"...\", environment: \"dspy\", params: {...} }\n  |-- Metadata: { run_id: \"abc12345\", environment: \"dspy\" }\n  |\n  +-- step-1 (tool)\n  |    |-- Inputs: { action: \"run_python\", code: \"import dspy...\" }\n  |    |-- Outputs: { success: true, output: \"...\", reward: 0.5 }\n  |\n  +-- step-2 (tool)\n  |    |-- Inputs: { action: \"run_python\", code: \"class Module...\" }\n  |    |-- Outputs: { success: true, output: \"...\", reward: 0.5 }\n  |\n  +-- step-3 (tool)\n       |-- Inputs: { action: \"submit\", code: \"\" }\n       |-- Outputs: { success: true, reward: 0.5 }\n  |\n  |-- Outputs: { completed: true, steps: 3, total_reward: 1.5 }\n</code></pre>"},{"location":"observability/langsmith/#connection-validation","title":"Connection Validation","text":"<p>During initialization, the sink tests the connection to LangSmith by calling <code>self._client.list_projects(limit=1)</code>. If this call fails (invalid API key, network error, etc.), the sink sets <code>_available=False</code> and records the error:</p> <pre><code>try:\n    self._client = Client()\n    self._client.list_projects(limit=1)\n    self._available = True\n    self._detail = f\"project: {self.project}\"\nexcept Exception as exc:\n    self._available = False\n    self._detail = f\"connection failed: {exc}\"\n</code></pre> <p>Check Status</p> <p>After initialization, call <code>sink.status()</code> to verify the connection. The <code>available</code> field tells you whether the sink is live.</p>"},{"location":"observability/langsmith/#troubleshooting","title":"Troubleshooting","text":"Symptom Cause Solution <code>available: False</code>, detail mentions <code>ImportError</code> <code>langsmith</code> package not installed <code>pip install langsmith</code> <code>available: False</code>, detail mentions <code>connection failed</code> Invalid API key or network issue Verify <code>LANGCHAIN_API_KEY</code> and network connectivity Traces not appearing in UI Sink not enabled Ensure <code>DSPY_RLM_LANGSMITH_ENABLED=true</code> Traces in wrong project <code>LANGCHAIN_PROJECT</code> mismatch Set <code>LANGCHAIN_PROJECT</code> to the correct project name Missing step details Step truncation Code and output are truncated to 500 chars in LangSmith"},{"location":"observability/logfire/","title":"Logfire Integration","text":"<p>The <code>LogfireSink</code> sends RLM traces to Logfire, Pydantic's observability platform built on OpenTelemetry. Logfire provides structured logging, trace visualization, and dashboards tailored for Python applications.</p>"},{"location":"observability/logfire/#overview","title":"Overview","text":"Property Value Class <code>rlm_code.rlm.observability_sinks.LogfireSink</code> Sink name <code>logfire</code> Activation <code>DSPY_RLM_LOGFIRE_ENABLED=true</code> Primary env var <code>LOGFIRE_TOKEN</code> Optional dependency <code>pip install logfire</code>"},{"location":"observability/logfire/#activation","title":"Activation","text":"<pre><code>export DSPY_RLM_LOGFIRE_ENABLED=true\nexport LOGFIRE_TOKEN=your-logfire-token\nexport LOGFIRE_PROJECT_NAME=rlm-code  # optional, default: rlm-code\n</code></pre>"},{"location":"observability/logfire/#features","title":"Features","text":""},{"location":"observability/logfire/#pydantic-observability-platform","title":"Pydantic Observability Platform","text":"<p>Logfire is built by the Pydantic team and integrates natively with Pydantic models and Python applications. It provides:</p> <ul> <li>OTEL-compatible tracing: Logfire is built on OpenTelemetry under the hood</li> <li>Structured logging: Rich, queryable log entries with typed attributes</li> <li>Dashboard visualization: Web-based UI for exploring traces and logs</li> <li>Python-native experience: Designed specifically for Python applications</li> </ul>"},{"location":"observability/logfire/#structured-logging","title":"Structured Logging","text":"<p>The sink uses Logfire's structured logging API to emit rich log entries at each step:</p> Successful stepFailed step <pre><code>self._logfire.info(\n    \"RLM step {step} completed\",\n    step=step,\n    run_id=run_id,\n    action=action.get(\"action\", \"unknown\"),\n    reward=event.get(\"reward\", 0.0),\n    cumulative_reward=cumulative_reward,\n)\n</code></pre> <pre><code>self._logfire.warn(\n    \"RLM step {step} failed\",\n    step=step,\n    run_id=run_id,\n    action=action.get(\"action\", \"unknown\"),\n    error=observation.get(\"error\", \"\")[:200],\n    reward=event.get(\"reward\", 0.0),\n)\n</code></pre>"},{"location":"observability/logfire/#trace-visualization","title":"Trace Visualization","text":"<p>Each RLM run is wrapped in a Logfire span that captures the entire execution duration:</p> <pre><code>span = self._logfire.span(\n    \"rlm.run {run_id}\",\n    run_id=run_id,\n    task=task[:200],\n    environment=environment,\n    max_steps=params.get(\"max_steps\", 0),\n)\n</code></pre> <p>The span is entered at run start and exited at run end. All structured log entries emitted during the run appear as children of this span in the Logfire dashboard.</p>"},{"location":"observability/logfire/#run-lifecycle-logging","title":"Run Lifecycle Logging","text":"Event Log Level Message Attributes Run start <code>info</code> <code>RLM run started</code> <code>run_id</code>, <code>environment</code> Step success <code>info</code> <code>RLM step {step} completed</code> <code>step</code>, <code>run_id</code>, <code>action</code>, <code>reward</code>, <code>cumulative_reward</code> Step failure <code>warn</code> <code>RLM step {step} failed</code> <code>step</code>, <code>run_id</code>, <code>action</code>, <code>error</code>, <code>reward</code> Run completed <code>info</code> <code>RLM run completed</code> <code>run_id</code>, <code>steps</code>, <code>total_reward</code> Run incomplete <code>warn</code> <code>RLM run did not complete</code> <code>run_id</code>, <code>steps</code>, <code>total_reward</code>"},{"location":"observability/logfire/#setup-guide","title":"Setup Guide","text":""},{"location":"observability/logfire/#1-create-a-logfire-account","title":"1. Create a Logfire Account","text":"<p>Sign up at logfire.pydantic.dev and create a project.</p>"},{"location":"observability/logfire/#2-install-the-sdk","title":"2. Install the SDK","text":"<pre><code>pip install logfire\n</code></pre>"},{"location":"observability/logfire/#3-authenticate","title":"3. Authenticate","text":"Via environment variableVia CLI <pre><code>export LOGFIRE_TOKEN=your-logfire-token\n</code></pre> <pre><code>logfire auth\n</code></pre> <p>This stores credentials locally so you don't need to set <code>LOGFIRE_TOKEN</code> manually.</p>"},{"location":"observability/logfire/#4-configure-environment","title":"4. Configure Environment","text":"<pre><code>export DSPY_RLM_LOGFIRE_ENABLED=true\nexport LOGFIRE_TOKEN=your-logfire-token\nexport LOGFIRE_PROJECT_NAME=rlm-code\n</code></pre>"},{"location":"observability/logfire/#5-run-a-task","title":"5. Run a Task","text":"<pre><code>rlm-code run --task \"Process context variable\" --environment pure_rlm\n</code></pre>"},{"location":"observability/logfire/#6-view-in-dashboard","title":"6. View in Dashboard","text":"<p>Open the Logfire dashboard at logfire.pydantic.dev. You will see:</p> <ul> <li>Traces: The <code>rlm.run</code> span with its duration and status</li> <li>Logs: Structured log entries for each step, searchable by attributes</li> <li>Metrics: Aggregated views of run performance</li> </ul>"},{"location":"observability/logfire/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Env Var Description <code>enabled</code> <code>bool</code> <code>False</code> <code>DSPY_RLM_LOGFIRE_ENABLED</code> Enable/disable the sink <code>project_name</code> <code>str</code> <code>\"rlm-code\"</code> <code>LOGFIRE_PROJECT_NAME</code> Logfire project name"},{"location":"observability/logfire/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from rlm_code.rlm.observability_sinks import LogfireSink\n\nsink = LogfireSink(\n    enabled=True,\n    project_name=\"my-rlm-project\",\n)\n\nprint(sink.status())\n# {'name': 'logfire', 'enabled': True, 'available': True,\n#  'detail': 'project: my-rlm-project', 'project_name': 'my-rlm-project'}\n</code></pre>"},{"location":"observability/logfire/#factory-function","title":"Factory Function","text":"<pre><code>from rlm_code.rlm.observability_sinks import create_logfire_sink_from_env\n\n# Reads DSPY_RLM_LOGFIRE_ENABLED and LOGFIRE_PROJECT_NAME\nsink = create_logfire_sink_from_env()\n</code></pre>"},{"location":"observability/logfire/#span-management","title":"Span Management","text":"<p>The sink manages span lifecycle manually using context manager enter/exit:</p> <pre><code># On run start\nspan = self._logfire.span(\"rlm.run {run_id}\", ...)\nspan.__enter__()\nself._active_spans[run_id] = {\"span\": span, \"steps\": []}\n\n# On run end\nspan_data = self._active_spans.pop(run_id)\nspan = span_data[\"span\"]\nspan.__exit__(None, None, None)\n</code></pre> <p>Manual Context Management</p> <p>The sink enters the span context at run start and exits it at run end, rather than using <code>with</code> blocks. This is necessary because run start and run end are separate method calls.</p>"},{"location":"observability/logfire/#troubleshooting","title":"Troubleshooting","text":"Symptom Cause Solution <code>available: False</code>, <code>logfire not installed</code> SDK not installed <code>pip install logfire</code> <code>available: False</code>, <code>setup failed</code> Invalid token or configuration Verify <code>LOGFIRE_TOKEN</code>; try <code>logfire auth</code> Traces not appearing Sink not enabled Set <code>DSPY_RLM_LOGFIRE_ENABLED=true</code> Wrong project <code>LOGFIRE_PROJECT_NAME</code> mismatch Set to the correct project name Span not closed Run crashed before <code>on_run_end</code> Spans may appear as incomplete in the dashboard"},{"location":"observability/logfire/#logfire-vs-opentelemetry-sink","title":"Logfire vs OpenTelemetry Sink","text":"<p>Both sinks use OpenTelemetry under the hood. Here is when to choose each:</p> Consideration Logfire OpenTelemetry Backend Logfire cloud or self-hosted Any OTEL-compatible backend (Jaeger, Zipkin, Grafana, etc.) Setup complexity Low (token-based auth) Medium (requires collector configuration) Python-native logging Yes (structured <code>info</code>/<code>warn</code>/<code>error</code>) No (spans and events only) Pydantic integration Native Manual Cost Logfire pricing Depends on backend Self-hosting Optional Standard OTEL infrastructure <p>You can enable both sinks simultaneously if you want Logfire's structured logging alongside a separate OTEL trace backend.</p>"},{"location":"observability/mlflow/","title":"MLflow Integration","text":"<p>The <code>MLflowSink</code> sends RLM run data to an MLflow tracking server for experiment management, metric visualization, and artifact storage.</p>"},{"location":"observability/mlflow/#overview","title":"Overview","text":"Property Value Class <code>rlm_code.rlm.observability.MLflowSink</code> Sink name <code>mlflow</code> Activation <code>DSPY_RLM_MLFLOW_ENABLED=true</code> Primary env var <code>MLFLOW_TRACKING_URI</code> Optional dependency <code>pip install mlflow</code>"},{"location":"observability/mlflow/#activation","title":"Activation","text":"<p>Set the following environment variables to enable the MLflow sink:</p> <pre><code>export DSPY_RLM_MLFLOW_ENABLED=true\nexport MLFLOW_TRACKING_URI=http://localhost:5000\nexport DSPY_RLM_MLFLOW_EXPERIMENT=rlm-code-rlm  # optional, default: rlm-code-rlm\n</code></pre> <p>Dependency Required</p> <p>The <code>mlflow</code> Python package must be installed. If it is missing, the sink will initialize with <code>_available=False</code> and log the import error in its status <code>detail</code> field.</p>"},{"location":"observability/mlflow/#features","title":"Features","text":""},{"location":"observability/mlflow/#experiment-tracking","title":"Experiment Tracking","text":"<p>Each RLM benchmark or run maps to an MLflow experiment. The experiment name defaults to <code>rlm-code-rlm</code> and can be overridden with <code>DSPY_RLM_MLFLOW_EXPERIMENT</code>.</p>"},{"location":"observability/mlflow/#run-logging","title":"Run Logging","text":"<p>For every RLM run, the sink:</p> <ol> <li>Calls <code>mlflow.start_run(run_name=run_id)</code> at the start</li> <li>Logs parameters (environment, task length, max_steps, model, and any scalar params)</li> <li>Sets tags: <code>run_id</code> and <code>component=rlm-code-rlm</code></li> <li>Calls <code>mlflow.end_run()</code> on completion</li> </ol>"},{"location":"observability/mlflow/#metric-logging","title":"Metric Logging","text":"<p>Two metrics are logged per step:</p> Metric Description Logged Per Step <code>step_reward</code> Reward for this individual step Yes <code>cumulative_reward</code> Running total reward Yes <p>Three summary metrics are logged at run end:</p> Metric Description <code>completed</code> <code>1.0</code> if the run completed, <code>0.0</code> otherwise <code>steps</code> Total number of steps taken <code>total_reward</code> Final cumulative reward"},{"location":"observability/mlflow/#artifact-storage","title":"Artifact Storage","text":"<p>If the run's artifact directory exists, it is uploaded to MLflow as an artifact:</p> <pre><code>if run_path.exists():\n    self._mlflow.log_artifact(str(run_path))\n</code></pre> <p>This means your full trajectory JSONL, code files, and any outputs are preserved alongside the MLflow run.</p>"},{"location":"observability/mlflow/#setup-guide","title":"Setup Guide","text":""},{"location":"observability/mlflow/#1-install-mlflow","title":"1. Install MLflow","text":"<pre><code>pip install mlflow\n</code></pre>"},{"location":"observability/mlflow/#2-start-the-mlflow-tracking-server","title":"2. Start the MLflow Tracking Server","text":"Local file backendSQLite backendPostgreSQL backend <pre><code>mlflow server --host 0.0.0.0 --port 5000\n</code></pre> <pre><code>mlflow server \\\n    --backend-store-uri sqlite:///mlflow.db \\\n    --default-artifact-root ./mlflow-artifacts \\\n    --host 0.0.0.0 --port 5000\n</code></pre> <pre><code>mlflow server \\\n    --backend-store-uri postgresql://user:pass@localhost/mlflow \\\n    --default-artifact-root s3://my-bucket/mlflow-artifacts \\\n    --host 0.0.0.0 --port 5000\n</code></pre>"},{"location":"observability/mlflow/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export DSPY_RLM_MLFLOW_ENABLED=true\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre>"},{"location":"observability/mlflow/#4-run-a-benchmark","title":"4. Run a Benchmark","text":"<pre><code>rlm-code run --task \"Create a DSPy signature\" --environment dspy\n</code></pre>"},{"location":"observability/mlflow/#5-view-results","title":"5. View Results","text":"<p>Open <code>http://localhost:5000</code> in your browser. You will see:</p> <ul> <li>The rlm-code-rlm experiment</li> <li>Individual runs with parameters, metrics, and artifacts</li> <li>Step-by-step reward curves via the metrics tab</li> </ul>"},{"location":"observability/mlflow/#configuration-options","title":"Configuration Options","text":"<p>The <code>MLflowSink</code> accepts the following parameters:</p> Parameter Type Default Env Var Description <code>enabled</code> <code>bool</code> <code>False</code> <code>DSPY_RLM_MLFLOW_ENABLED</code> Enable or disable the sink <code>experiment</code> <code>str</code> <code>\"rlm-code-rlm\"</code> <code>DSPY_RLM_MLFLOW_EXPERIMENT</code> MLflow experiment name <code>tracking_uri</code> <code>str | None</code> <code>None</code> <code>MLFLOW_TRACKING_URI</code> MLflow tracking server URI"},{"location":"observability/mlflow/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from rlm_code.rlm.observability import MLflowSink\n\nsink = MLflowSink(\n    enabled=True,\n    experiment=\"my-custom-experiment\",\n    tracking_uri=\"http://mlflow.internal:5000\",\n)\n\n# Check status\nprint(sink.status())\n# {'name': 'mlflow', 'enabled': True, 'available': True,\n#  'detail': 'http://mlflow.internal:5000', 'experiment': 'my-custom-experiment'}\n</code></pre>"},{"location":"observability/mlflow/#logged-parameters","title":"Logged Parameters","text":"<p>When <code>on_run_start</code> fires, the sink logs the following as MLflow parameters:</p> Parameter Source <code>environment</code> The RLM environment name <code>task_chars</code> Length of the task string (integer) Any scalar param Any key in <code>params</code> whose value is <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, or <code>None</code> <p>Non-Scalar Parameters</p> <p>Parameters with non-scalar values (lists, dicts, objects) are silently skipped to avoid MLflow serialization errors.</p>"},{"location":"observability/mlflow/#error-handling","title":"Error Handling","text":"<p>The <code>MLflowSink</code> is resilient to failures at every stage:</p> <ul> <li>Import failure: If <code>mlflow</code> is not installed, <code>_available</code> is set to <code>False</code> and all hooks return immediately.</li> <li><code>on_run_start</code> failure: Logged as a warning; the run continues without MLflow tracking.</li> <li><code>on_step</code> failure: Logged as a warning; subsequent steps still attempt logging.</li> <li><code>on_run_end</code> failure: The sink ensures <code>mlflow.end_run()</code> is called even if metric logging fails, preventing orphaned MLflow runs.</li> </ul> <pre><code>def on_run_end(self, run_id, *, result, run_path):\n    if not self._available:\n        return\n    try:\n        if run_id in self._active_runs:\n            self._mlflow.log_metrics({...})\n            self._mlflow.log_artifact(str(run_path))\n            self._mlflow.end_run()\n            self._active_runs.remove(run_id)\n    except Exception as exc:\n        logger.warning(f\"MLflow on_run_end failed: {exc}\")\n        try:\n            self._mlflow.end_run()\n        except Exception:\n            pass\n        self._active_runs.discard(run_id)\n</code></pre>"},{"location":"observability/mlflow/#viewing-results-in-mlflow-ui","title":"Viewing Results in MLflow UI","text":"<p>After running benchmarks, the MLflow UI provides:</p> Feature Where to Find Run list Experiments page, sorted by start time Parameters Run detail &gt; Parameters tab Step-by-step reward Run detail &gt; Metrics &gt; <code>step_reward</code> or <code>cumulative_reward</code> Summary metrics Run detail &gt; Metrics &gt; <code>completed</code>, <code>steps</code>, <code>total_reward</code> Artifacts Run detail &gt; Artifacts tab (trajectory files, code) Compare runs Select multiple runs &gt; Compare"},{"location":"observability/otel/","title":"OpenTelemetry Integration","text":"<p>The <code>OpenTelemetrySink</code> provides distributed tracing and metrics export via the OpenTelemetry standard. Traces are exported over OTLP (gRPC) and can be visualized in Jaeger, Zipkin, Grafana Tempo, or any OTEL-compatible backend.</p>"},{"location":"observability/otel/#overview","title":"Overview","text":"Property Value Class <code>rlm_code.rlm.observability_sinks.OpenTelemetrySink</code> Sink name <code>opentelemetry</code> Activation <code>DSPY_RLM_OTEL_ENABLED=true</code> Primary env var <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> Optional dependencies <code>pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-grpc</code>"},{"location":"observability/otel/#activation","title":"Activation","text":"<pre><code>export DSPY_RLM_OTEL_ENABLED=true\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\nexport OTEL_SERVICE_NAME=rlm-code              # optional, default: rlm-code\nexport DSPY_RLM_OTEL_METRICS_ENABLED=true      # optional, default: true\n</code></pre> <p>Dependencies Required</p> <p>The following packages must be installed:</p> <pre><code>pip install \\\n    opentelemetry-api \\\n    opentelemetry-sdk \\\n    opentelemetry-exporter-otlp-proto-grpc\n</code></pre> <p>If any are missing, the sink will set <code>_available=False</code> and include the <code>ImportError</code> in its status detail.</p>"},{"location":"observability/otel/#features","title":"Features","text":""},{"location":"observability/otel/#distributed-tracing-with-span-linking","title":"Distributed Tracing with Span Linking","text":"<p>The sink creates a span hierarchy that mirrors the RLM execution structure:</p> <pre><code>rlm.run (root span)\n  |-- rlm.step (step 1)\n  |-- rlm.step (step 2)\n  |-- rlm.step (step 3)\n  ...\n</code></pre> <p>Each <code>rlm.run</code> root span persists for the entire run duration. Individual <code>rlm.step</code> spans are created as children of the root span, establishing proper parent-child relationships for trace visualization.</p>"},{"location":"observability/otel/#trace-ids","title":"Trace IDs","text":"<p>Every run receives a unique trace ID. You can retrieve it programmatically:</p> <pre><code>otel_sink = obs.get_sink(\"opentelemetry\")\nif otel_sink:\n    trace_id = otel_sink.get_trace_id(run_id)\n    print(f\"View trace: http://localhost:16686/trace/{trace_id}\")\n</code></pre>"},{"location":"observability/otel/#nested-spans-for-iterations","title":"Nested Spans for Iterations","text":"<p>For each step, the sink creates a child span with rich attributes:</p> Span Attribute Description <code>rlm.run_id</code> Unique run identifier <code>rlm.step</code> Step number <code>rlm.action_type</code> Action type (e.g., <code>run_python</code>, <code>submit</code>) <code>rlm.reward</code> Step reward <code>rlm.cumulative_reward</code> Running total reward <code>rlm.success</code> Whether the step succeeded"},{"location":"observability/otel/#span-events","title":"Span Events","text":"<p>Each step span carries two OTEL events:</p> Event Name When Attached Content <code>code_execution</code> When the step includes code First 1000 characters of executed code <code>output</code> When the step produces output First 1000 characters of output"},{"location":"observability/otel/#span-status","title":"Span Status","text":"<p>The span status is set based on execution outcome:</p> <ul> <li>OK: Step succeeded (<code>observation.success == True</code>)</li> <li>ERROR: Step failed (includes first 200 chars of error message)</li> </ul>"},{"location":"observability/otel/#root-span-attributes","title":"Root Span Attributes","text":"<p>The root span (<code>rlm.run</code>) carries these attributes:</p> Attribute Description <code>rlm.run_id</code> Run identifier <code>rlm.task</code> Task description (first 500 chars) <code>rlm.environment</code> Environment name <code>rlm.max_steps</code> Maximum allowed steps <code>rlm.model</code> Model identifier <p>At run end, additional attributes are added:</p> Attribute Description <code>rlm.completed</code> Whether the run completed successfully <code>rlm.total_steps</code> Actual number of steps taken <code>rlm.total_reward</code> Final cumulative reward <code>rlm.run_path</code> Path to run artifacts <p>A <code>final_answer</code> event is attached if the run produced a final answer.</p>"},{"location":"observability/otel/#metrics","title":"Metrics","text":"<p>When <code>metrics_enabled</code> is <code>true</code> (the default), the sink creates four OTEL metric instruments:</p> Instrument Type Name Unit Description Run counter Counter <code>rlm.runs</code> <code>1</code> Total number of RLM runs, labeled by environment Step counter Counter <code>rlm.steps</code> <code>1</code> Total number of steps, labeled by run_id Reward histogram Histogram <code>rlm.reward</code> <code>1</code> Distribution of per-step rewards Duration histogram Histogram <code>rlm.run_duration</code> <code>s</code> Distribution of run durations in seconds <p>Metrics are exported via the same OTLP endpoint as traces.</p>"},{"location":"observability/otel/#setup-guide","title":"Setup Guide","text":""},{"location":"observability/otel/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code>pip install \\\n    opentelemetry-api \\\n    opentelemetry-sdk \\\n    opentelemetry-exporter-otlp-proto-grpc\n</code></pre>"},{"location":"observability/otel/#2-start-an-otel-collector","title":"2. Start an OTEL Collector","text":"Jaeger (all-in-one)Zipkin + OTEL CollectorGrafana Tempo <pre><code>docker run -d --name jaeger \\\n    -e COLLECTOR_OTLP_ENABLED=true \\\n    -p 4317:4317 \\\n    -p 16686:16686 \\\n    jaegertracing/all-in-one:latest\n</code></pre> <ul> <li>OTLP gRPC: <code>http://localhost:4317</code></li> <li>Jaeger UI: <code>http://localhost:16686</code></li> </ul> <p>Create <code>otel-collector-config.yaml</code>:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n\nexporters:\n  zipkin:\n    endpoint: \"http://zipkin:9411/api/v2/spans\"\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [zipkin]\n</code></pre> <pre><code>docker run -d --name zipkin -p 9411:9411 openzipkin/zipkin\ndocker run -d --name otel-collector \\\n    -v $(pwd)/otel-collector-config.yaml:/etc/otelcol/config.yaml \\\n    -p 4317:4317 \\\n    otel/opentelemetry-collector:latest\n</code></pre> <pre><code>docker run -d --name tempo \\\n    -p 4317:4317 \\\n    -p 3200:3200 \\\n    grafana/tempo:latest \\\n    -config.file=/etc/tempo.yaml\n</code></pre>"},{"location":"observability/otel/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export DSPY_RLM_OTEL_ENABLED=true\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\nexport OTEL_SERVICE_NAME=rlm-code\n</code></pre>"},{"location":"observability/otel/#4-run-a-task","title":"4. Run a Task","text":"<pre><code>rlm-code run --task \"Build a DSPy signature\" --environment dspy\n</code></pre>"},{"location":"observability/otel/#5-visualize","title":"5. Visualize","text":"<p>Open the trace UI for your backend:</p> <ul> <li>Jaeger: <code>http://localhost:16686</code> -- Search for service <code>rlm-code</code></li> <li>Zipkin: <code>http://localhost:9411</code> -- Search for serviceName <code>rlm-code</code></li> <li>Grafana: Connect Tempo as a data source, explore traces</li> </ul>"},{"location":"observability/otel/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Env Var Description <code>enabled</code> <code>bool</code> <code>False</code> <code>DSPY_RLM_OTEL_ENABLED</code> Enable/disable the sink <code>service_name</code> <code>str</code> <code>\"rlm-code\"</code> <code>OTEL_SERVICE_NAME</code> OTEL service name <code>endpoint</code> <code>str | None</code> <code>None</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> OTLP gRPC endpoint URL <code>metrics_enabled</code> <code>bool</code> <code>True</code> <code>DSPY_RLM_OTEL_METRICS_ENABLED</code> Whether to export metrics alongside traces"},{"location":"observability/otel/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from rlm_code.rlm.observability_sinks import OpenTelemetrySink\n\nsink = OpenTelemetrySink(\n    enabled=True,\n    service_name=\"my-rlm-service\",\n    endpoint=\"http://otel-collector.internal:4317\",\n    metrics_enabled=True,\n)\n\nprint(sink.status())\n# {'name': 'opentelemetry', 'enabled': True, 'available': True,\n#  'detail': 'http://otel-collector.internal:4317',\n#  'service_name': 'my-rlm-service', 'metrics_enabled': True}\n</code></pre>"},{"location":"observability/otel/#factory-function","title":"Factory Function","text":"<pre><code>from rlm_code.rlm.observability_sinks import create_otel_sink_from_env\n\n# Reads DSPY_RLM_OTEL_ENABLED, OTEL_EXPORTER_OTLP_ENDPOINT,\n# OTEL_SERVICE_NAME, DSPY_RLM_OTEL_METRICS_ENABLED\nsink = create_otel_sink_from_env()\n</code></pre>"},{"location":"observability/otel/#trace-structure-example","title":"Trace Structure Example","text":"<p>A typical 3-step RLM run produces the following trace:</p> <pre><code>Trace: 00000000000000000000abcdef123456\n  |\n  +-- rlm.run [600ms]\n       | rlm.run_id = \"abc12345\"\n       | rlm.environment = \"dspy\"\n       | rlm.completed = true\n       | rlm.total_steps = 3\n       | rlm.total_reward = 1.5\n       |\n       +-- rlm.step [150ms]\n       |    | rlm.step = 1\n       |    | rlm.action_type = \"run_python\"\n       |    | rlm.reward = 0.5\n       |    | rlm.success = true\n       |    | Event: code_execution { code: \"import dspy...\" }\n       |    | Event: output { output: \"Signature created\" }\n       |\n       +-- rlm.step [200ms]\n       |    | rlm.step = 2\n       |    | rlm.action_type = \"run_python\"\n       |    | rlm.reward = 0.5\n       |    | rlm.success = true\n       |\n       +-- rlm.step [100ms]\n            | rlm.step = 3\n            | rlm.action_type = \"submit\"\n            | rlm.reward = 0.5\n            | rlm.success = true\n            | Event: final_answer { answer: \"...\" }\n</code></pre>"},{"location":"observability/otel/#jaeger-visualization-tips","title":"Jaeger Visualization Tips","text":"<ol> <li>Service filter: Search for <code>rlm-code</code> (or your custom <code>OTEL_SERVICE_NAME</code>)</li> <li>Operation filter: Use <code>rlm.run</code> to find root spans, or <code>rlm.step</code> for individual steps</li> <li>Tag search: Filter by <code>rlm.environment=dspy</code> or <code>rlm.completed=true</code></li> <li>Compare traces: Select two traces to diff their timelines side by side</li> <li>Flame graph: Switch to the flame graph view to see time distribution across steps</li> </ol>"},{"location":"observability/otel/#disabling-metrics","title":"Disabling Metrics","text":"<p>If you only want traces and not metrics, set:</p> <pre><code>export DSPY_RLM_OTEL_METRICS_ENABLED=false\n</code></pre> <p>This prevents the sink from creating a <code>MeterProvider</code> and the four metric instruments, reducing overhead.</p>"},{"location":"observability/sinks/","title":"Sink Architecture","text":"<p>This page covers the internal architecture of the observability system: the sink protocol, the central <code>RLMObservability</code> manager, the always-on <code>LocalJSONLSink</code>, the <code>CompositeSink</code> wrapper, factory functions, and how to build your own custom sink.</p>"},{"location":"observability/sinks/#module-reference","title":"Module Reference","text":"Module Purpose <code>rlm_code.rlm.observability</code> <code>RLMObservabilitySink</code> protocol, <code>LocalJSONLSink</code>, <code>MLflowSink</code>, <code>RLMObservability</code> coordinator <code>rlm_code.rlm.observability_sinks</code> <code>OpenTelemetrySink</code>, <code>LangSmithSink</code>, <code>LangFuseSink</code>, <code>LogfireSink</code>, <code>CompositeSink</code>, factory functions"},{"location":"observability/sinks/#the-rlmobservabilitysink-protocol","title":"The RLMObservabilitySink Protocol","text":"<p>The protocol is defined as a Python <code>Protocol</code> (structural typing). Any class that exposes the required attributes and methods is a valid sink -- no inheritance needed.</p> <pre><code>from typing import Any, Protocol\nfrom pathlib import Path\n\n\nclass RLMObservabilitySink(Protocol):\n    \"\"\"Sink contract for RLM observability events.\"\"\"\n\n    name: str\n\n    def status(self) -&gt; dict[str, Any]:\n        \"\"\"Return sink status for CLI visibility.\"\"\"\n        ...\n\n    def on_run_start(\n        self,\n        run_id: str,\n        *,\n        task: str,\n        environment: str,\n        params: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Hook called at run start.\"\"\"\n        ...\n\n    def on_step(\n        self,\n        run_id: str,\n        *,\n        event: dict[str, Any],\n        cumulative_reward: float,\n    ) -&gt; None:\n        \"\"\"Hook called after each step event.\"\"\"\n        ...\n\n    def on_run_end(\n        self,\n        run_id: str,\n        *,\n        result: Any,\n        run_path: Path,\n    ) -&gt; None:\n        \"\"\"Hook called once at run completion.\"\"\"\n        ...\n</code></pre>"},{"location":"observability/sinks/#protocol-methods-in-detail","title":"Protocol Methods in Detail","text":""},{"location":"observability/sinks/#status-dictstr-any","title":"<code>status() -&gt; dict[str, Any]</code>","text":"<p>Returns a dictionary that the CLI uses to display sink health. Standard keys:</p> Key Type Description <code>name</code> <code>str</code> Unique sink name (e.g., <code>\"mlflow\"</code>, <code>\"opentelemetry\"</code>) <code>enabled</code> <code>bool</code> Whether the user requested this sink <code>available</code> <code>bool</code> Whether the sink could actually initialize (dependencies installed, connection OK) <code>detail</code> <code>str</code> Human-readable detail string (URI, error message, etc.)"},{"location":"observability/sinks/#on_run_startrun_id-task-environment-params","title":"<code>on_run_start(run_id, *, task, environment, params)</code>","text":"<p>Called once before the first iteration. The <code>params</code> dictionary contains runner configuration such as <code>max_steps</code>, <code>model</code>, <code>exec_timeout</code>, and any custom parameters.</p>"},{"location":"observability/sinks/#on_steprun_id-event-cumulative_reward","title":"<code>on_step(run_id, *, event, cumulative_reward)</code>","text":"<p>Called after every iteration. The <code>event</code> dictionary follows this shape:</p> <pre><code>{\n    \"step\": 1,\n    \"action\": {\n        \"action\": \"run_python\",\n        \"code\": \"print('hello')\",\n        \"rationale\": \"Testing basic output\"\n    },\n    \"observation\": {\n        \"success\": True,\n        \"output\": \"hello\\n\",\n        \"error\": \"\"\n    },\n    \"reward\": 0.5\n}\n</code></pre>"},{"location":"observability/sinks/#on_run_endrun_id-result-run_path","title":"<code>on_run_end(run_id, *, result, run_path)</code>","text":"<p>Called once at run completion. The <code>result</code> object carries attributes like <code>completed</code>, <code>steps</code>, <code>total_reward</code>, <code>final_answer</code>, <code>started_at</code>, and <code>finished_at</code>. The <code>run_path</code> is the filesystem path to the run's artifact directory.</p>"},{"location":"observability/sinks/#rlmobservability-the-central-manager","title":"RLMObservability -- The Central Manager","text":"<p><code>RLMObservability</code> is a dataclass that holds a list of sinks and broadcasts every event to all of them.</p> <pre><code>@dataclass(slots=True)\nclass RLMObservability:\n    sinks: list[RLMObservabilitySink]\n</code></pre>"},{"location":"observability/sinks/#construction","title":"Construction","text":"Default (from environment)Explicit sink list <pre><code>from rlm_code.rlm.observability import RLMObservability\n\nobs = RLMObservability.default(workdir=workdir, run_dir=run_dir)\n</code></pre> <p>This reads all <code>DSPY_RLM_*</code> environment variables and creates the appropriate sinks.</p> <pre><code>from rlm_code.rlm.observability import RLMObservability, LocalJSONLSink, MLflowSink\n\nobs = RLMObservability.with_sinks([\n    LocalJSONLSink(base_dir=Path(\".rlm_trajectories\"), enabled=True),\n    MLflowSink(enabled=True, experiment=\"my-experiment\"),\n])\n</code></pre>"},{"location":"observability/sinks/#runtime-management-api","title":"Runtime Management API","text":"Method Signature Returns Description <code>add_sink</code> <code>(sink: RLMObservabilitySink) -&gt; None</code> <code>None</code> Append a sink to the live list <code>remove_sink</code> <code>(name: str) -&gt; bool</code> <code>True</code> if removed Remove a sink by its <code>name</code> attribute <code>get_sink</code> <code>(name: str) -&gt; RLMObservabilitySink | None</code> Sink or <code>None</code> Retrieve a sink by name <code>status</code> <code>() -&gt; list[dict[str, Any]]</code> List of status dicts Query all sink statuses"},{"location":"observability/sinks/#error-handling","title":"Error Handling","text":"<p>Every call to a sink is wrapped in <code>try/except</code>:</p> <pre><code>def on_step(self, run_id, *, event, cumulative_reward):\n    for sink in self.sinks:\n        try:\n            sink.on_step(run_id, event=event, cumulative_reward=cumulative_reward)\n        except Exception as exc:\n            logger.warning(f\"Observability sink '{sink.name}' on_step failed: {exc}\")\n</code></pre> <p>Fault Tolerance</p> <p>A single sink failure never propagates. The coordinator logs a warning and continues to the next sink. This is critical for production stability -- you never want a telemetry issue to halt an agent run.</p>"},{"location":"observability/sinks/#localjsonlsink","title":"LocalJSONLSink","text":"<p>The <code>LocalJSONLSink</code> is always created (even if disabled) and serves as the foundation of offline analysis. It writes two types of files:</p>"},{"location":"observability/sinks/#file-layout","title":"File Layout","text":"<pre><code>.rlm_code/rlm/observability/\n    runs.jsonl              # One line per completed run\n    steps/\n        &lt;run_id&gt;.jsonl      # One line per step for each run\n</code></pre>"},{"location":"observability/sinks/#runsjsonl-format","title":"runs.jsonl Format","text":"<p>Each line is a JSON object:</p> <pre><code>{\n  \"timestamp\": \"2025-05-15T10:30:00+00:00\",\n  \"run_id\": \"abc12345\",\n  \"started_at\": \"2025-05-15T10:29:00+00:00\",\n  \"finished_at\": \"2025-05-15T10:30:00+00:00\",\n  \"task\": \"Create a DSPy signature...\",\n  \"environment\": \"dspy\",\n  \"params\": {\"max_steps\": 4, \"model\": \"gpt-4o\"},\n  \"completed\": true,\n  \"steps\": 3,\n  \"step_count_observed\": 3,\n  \"total_reward\": 1.5,\n  \"run_path\": \"/home/user/.rlm_code/rlm/runs/abc12345\"\n}\n</code></pre>"},{"location":"observability/sinks/#stepsrun_idjsonl-format","title":"steps/\\&lt;run_id&gt;.jsonl Format","text":"<p>Each line records one step:</p> <pre><code>{\n  \"timestamp\": \"2025-05-15T10:29:15+00:00\",\n  \"run_id\": \"abc12345\",\n  \"step\": 1,\n  \"action\": \"run_python\",\n  \"reward\": 0.5,\n  \"cumulative_reward\": 0.5,\n  \"success\": true\n}\n</code></pre>"},{"location":"observability/sinks/#configuration","title":"Configuration","text":"<pre><code>LocalJSONLSink(\n    base_dir=Path(\".rlm_code/rlm/observability\"),\n    enabled=True,   # Set to False to suppress file writes\n)\n</code></pre> Parameter Type Default Description <code>base_dir</code> <code>Path</code> (required) Root directory for JSONL files <code>enabled</code> <code>bool</code> <code>True</code> Whether to write files"},{"location":"observability/sinks/#compositesink","title":"CompositeSink","text":"<p>The <code>CompositeSink</code> wraps multiple sinks into a single sink object. This is useful for advanced composition patterns where you want to treat a bundle of sinks as one unit.</p> <pre><code>from rlm_code.rlm.observability_sinks import CompositeSink\n\ncomposite = CompositeSink(\n    sinks=[sink_a, sink_b, sink_c],\n    name=\"my-composite\"\n)\n</code></pre> <p>The <code>CompositeSink</code> itself satisfies the <code>RLMObservabilitySink</code> protocol, so it can be nested inside another <code>CompositeSink</code> or added directly to <code>RLMObservability</code>.</p>"},{"location":"observability/sinks/#error-handling_1","title":"Error Handling","text":"<p>Like the <code>RLMObservability</code> coordinator, <code>CompositeSink</code> wraps each child sink call in a <code>try/except</code>, ensuring one child failure does not block the others.</p>"},{"location":"observability/sinks/#creating-a-custom-sink","title":"Creating a Custom Sink","text":"<p>To create your own sink, implement the four required methods and set a <code>name</code> attribute.</p>"},{"location":"observability/sinks/#minimal-example","title":"Minimal Example","text":"<pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\n\n@dataclass\nclass WebhookSink:\n    \"\"\"Send events to a webhook URL.\"\"\"\n\n    url: str\n    name: str = \"webhook\"\n\n    def status(self) -&gt; dict[str, Any]:\n        return {\n            \"name\": self.name,\n            \"enabled\": True,\n            \"available\": True,\n            \"detail\": self.url,\n        }\n\n    def on_run_start(\n        self,\n        run_id: str,\n        *,\n        task: str,\n        environment: str,\n        params: dict[str, Any],\n    ) -&gt; None:\n        import requests\n        requests.post(self.url, json={\n            \"event\": \"run_start\",\n            \"run_id\": run_id,\n            \"task\": task[:200],\n            \"environment\": environment,\n        })\n\n    def on_step(\n        self,\n        run_id: str,\n        *,\n        event: dict[str, Any],\n        cumulative_reward: float,\n    ) -&gt; None:\n        import requests\n        requests.post(self.url, json={\n            \"event\": \"step\",\n            \"run_id\": run_id,\n            \"step\": event.get(\"step\"),\n            \"reward\": event.get(\"reward\"),\n            \"cumulative_reward\": cumulative_reward,\n        })\n\n    def on_run_end(\n        self,\n        run_id: str,\n        *,\n        result: Any,\n        run_path: Path,\n    ) -&gt; None:\n        import requests\n        requests.post(self.url, json={\n            \"event\": \"run_end\",\n            \"run_id\": run_id,\n            \"completed\": bool(getattr(result, \"completed\", False)),\n            \"total_reward\": float(getattr(result, \"total_reward\", 0.0)),\n        })\n</code></pre>"},{"location":"observability/sinks/#registering-the-custom-sink","title":"Registering the Custom Sink","text":"<pre><code>from rlm_code.rlm.observability import RLMObservability\n\nobs = RLMObservability.default(workdir=workdir, run_dir=run_dir)\nobs.add_sink(WebhookSink(url=\"https://hooks.example.com/rlm\"))\n</code></pre> <p>Lazy Imports</p> <p>If your sink depends on optional packages (like <code>requests</code> above), import them inside the method body so that the sink can be instantiated even if the package is not installed. All built-in sinks follow this pattern.</p>"},{"location":"observability/sinks/#sink-lifecycle","title":"Sink Lifecycle","text":"<p>Every sink follows this lifecycle during an RLM run:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Initialized: __init__ / __post_init__\n    Initialized --&gt; Active: on_run_start()\n    Active --&gt; Active: on_step() [repeated]\n    Active --&gt; Completed: on_run_end()\n    Completed --&gt; [*]\n\n    note right of Initialized\n        Sink tests its availability\n        (e.g., tries to import mlflow,\n        connect to OTEL endpoint)\n    end note\n\n    note right of Active\n        Each step event is forwarded.\n        Errors are caught and logged.\n    end note</code></pre>"},{"location":"observability/sinks/#phase-1-initialization-__post_init__","title":"Phase 1: Initialization (<code>__post_init__</code>)","text":"<p>During initialization, each sink:</p> <ol> <li>Checks whether it is <code>enabled</code> (via its constructor parameter)</li> <li>Attempts to import required dependencies</li> <li>Establishes connections to external backends</li> <li>Sets <code>_available = True</code> on success, or captures the error in <code>_detail</code></li> </ol>"},{"location":"observability/sinks/#phase-2-event-processing-on_run_start-on_step","title":"Phase 2: Event Processing (<code>on_run_start</code>, <code>on_step</code>)","text":"<p>If both <code>enabled</code> and <code>_available</code> are true, the sink processes events. If not, methods return immediately (early exit).</p>"},{"location":"observability/sinks/#phase-3-run-completion-on_run_end","title":"Phase 3: Run Completion (<code>on_run_end</code>)","text":"<p>The sink records final metrics, closes active spans/runs, and flushes any buffered data.</p>"},{"location":"observability/sinks/#factory-functions","title":"Factory Functions","text":"<p>Each external sink has a corresponding factory function that reads environment variables:</p> Factory Reads Creates <code>create_otel_sink_from_env()</code> <code>DSPY_RLM_OTEL_ENABLED</code>, <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>, <code>OTEL_SERVICE_NAME</code>, <code>DSPY_RLM_OTEL_METRICS_ENABLED</code> <code>OpenTelemetrySink</code> <code>create_langsmith_sink_from_env()</code> <code>DSPY_RLM_LANGSMITH_ENABLED</code>, <code>LANGCHAIN_PROJECT</code> <code>LangSmithSink</code> <code>create_langfuse_sink_from_env()</code> <code>DSPY_RLM_LANGFUSE_ENABLED</code>, <code>LANGFUSE_HOST</code> <code>LangFuseSink</code> <code>create_logfire_sink_from_env()</code> <code>DSPY_RLM_LOGFIRE_ENABLED</code>, <code>LOGFIRE_PROJECT_NAME</code> <code>LogfireSink</code> <code>create_all_sinks_from_env()</code> All of the above <code>list</code> of all four sinks"},{"location":"observability/sinks/#usage","title":"Usage","text":"<pre><code>from rlm_code.rlm.observability_sinks import (\n    create_otel_sink_from_env,\n    create_all_sinks_from_env,\n)\n\n# Create a single sink\notel = create_otel_sink_from_env()\n\n# Create all external sinks at once\nall_sinks = create_all_sinks_from_env()\n</code></pre> <p>Automatic Usage</p> <p>You rarely need to call factory functions directly. <code>RLMObservability.default()</code> calls them internally when constructing the default sink list.</p>"},{"location":"observability/sinks/#boolean-environment-variable-parsing","title":"Boolean Environment Variable Parsing","text":"<p>Both <code>observability.py</code> and <code>observability_sinks.py</code> use a shared helper for parsing boolean environment variables:</p> <pre><code>def _as_bool_env(value: str | None, default: bool) -&gt; bool:\n    if value is None:\n        return default\n    return value.strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n</code></pre> <p>Recognized truthy values: <code>\"1\"</code>, <code>\"true\"</code>, <code>\"yes\"</code>, <code>\"on\"</code> (case-insensitive). Everything else evaluates to <code>False</code>.</p>"},{"location":"policies/","title":"Policy Lab","text":""},{"location":"policies/#overview","title":"Overview","text":"<p>The Policy Lab is RLM Code's hot-swappable policy system for customizing every aspect of agent behavior at runtime. Rather than hard-coding decision logic into the execution engine, RLM Code delegates all critical behavioral decisions to pluggable policies that can be registered, swapped, and configured without modifying core code.</p> <p>Policies govern how the agent learns (reward), what it does next (action selection), how it manages memory (compaction), and when it stops (termination). Each category has multiple built-in implementations and supports custom user-defined policies through a decorator-based registration system.</p> <pre><code>from rlm_code.rlm.policies import (\n    PolicyRegistry,\n    RewardPolicy,\n    ActionSelectionPolicy,\n    CompactionPolicy,\n    TerminationPolicy,\n)\n\n# Create a full policy suite from config\npolicies = PolicyRegistry.create_from_config({\n    \"reward\": {\"name\": \"research\", \"config\": {\"base_success\": 0.4}},\n    \"action\": {\"name\": \"sampling\", \"config\": {\"temperature\": 0.8}},\n    \"compaction\": {\"name\": \"hierarchical\"},\n    \"termination\": {\"name\": \"final_pattern\"},\n})\n</code></pre>"},{"location":"policies/#the-four-policy-categories","title":"The Four Policy Categories","text":"<p>RLM Code organizes policies into four distinct categories, each controlling a different aspect of agent execution:</p> Category Base Class Purpose Default Reward <code>RewardPolicy</code> Calculate reward signals from action results <code>default</code> Action Selection <code>ActionSelectionPolicy</code> Choose the next action from candidates <code>greedy</code> Compaction <code>CompactionPolicy</code> Compress and summarize execution history <code>sliding_window</code> Termination <code>TerminationPolicy</code> Decide when to stop executing <code>final_pattern</code> <pre><code>graph LR\n    A[Agent Step] --&gt; B{Action Selection Policy}\n    B --&gt; C[Execute Action]\n    C --&gt; D{Reward Policy}\n    D --&gt; E{Compaction Policy}\n    E --&gt; F{Termination Policy}\n    F --&gt;|Continue| A\n    F --&gt;|Stop| G[Final Answer]</code></pre>"},{"location":"policies/#reward-policies","title":"Reward Policies","text":"<p>Reward policies compute a <code>RewardSignal</code> from an action and its result. The signal includes a scalar value (clamped to [-1, 1]) and a component-by-component breakdown for interpretability. Different reward policies trade off between strictness, exploration, and analytical detail.</p> <p>Built-in implementations: <code>default</code>, <code>strict</code>, <code>lenient</code>, <code>research</code></p> <p>See Reward Policies for full documentation.</p>"},{"location":"policies/#action-selection-policies","title":"Action Selection Policies","text":"<p>Action selection policies determine which action to execute next from a set of candidates. Strategies range from simple deterministic selection to sophisticated tree-search methods.</p> <p>Built-in implementations: <code>greedy</code>, <code>sampling</code>, <code>beam_search</code>, <code>mcts</code></p> <p>See Action Selection Policies for full documentation.</p>"},{"location":"policies/#compaction-policies","title":"Compaction Policies","text":"<p>Compaction policies manage the agent's execution history, compressing older entries to stay within token budgets while preserving important context. Strategies range from simple windowing to LLM-powered summarization.</p> <p>Built-in implementations: <code>llm</code>, <code>deterministic</code>, <code>sliding_window</code>, <code>hierarchical</code></p> <p>See Compaction Policies for full documentation.</p>"},{"location":"policies/#termination-policies","title":"Termination Policies","text":"<p>Termination policies determine when the agent should stop executing and return a final answer. They detect completion signals, reward thresholds, and confidence levels.</p> <p>Built-in implementations: <code>final_pattern</code>, <code>reward_threshold</code>, <code>confidence</code>, <code>composite</code></p> <p>See Termination Policies for full documentation.</p>"},{"location":"policies/#policyregistry","title":"PolicyRegistry","text":"<p>The <code>PolicyRegistry</code> is the central management hub for all policies. It provides:</p> <ul> <li>Decorator-based registration via <code>@PolicyRegistry.register_reward(name)</code>, etc.</li> <li>Lookup by name with <code>PolicyRegistry.get_reward(name, config)</code></li> <li>Configuration-based instantiation via <code>PolicyRegistry.create_from_config(config_dict)</code></li> <li>Discovery with <code>PolicyRegistry.list_all()</code> to enumerate all registered policies</li> </ul> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\n# List everything registered\nall_policies = PolicyRegistry.list_all()\n# {\n#     \"reward\": [{\"name\": \"default\", \"description\": \"Balanced reward for general use\"}, ...],\n#     \"action\": [{\"name\": \"greedy\", \"description\": \"Always select highest-scored action\"}, ...],\n#     \"compaction\": [...],\n#     \"termination\": [...],\n# }\n\n# Get a specific policy instance\nreward = PolicyRegistry.get_reward(\"strict\", config={\"failure_penalty\": 0.8})\n\n# Change defaults\nPolicyRegistry.set_default_reward(\"research\")\nPolicyRegistry.set_default_action(\"sampling\")\n</code></pre> <p>See Policy Registry for full documentation.</p>"},{"location":"policies/#policycontext","title":"PolicyContext","text":"<p>Every policy method receives a <code>PolicyContext</code> dataclass that carries the full execution state. This provides policies with the information they need to make context-aware decisions.</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\n@dataclass\nclass PolicyContext:\n    \"\"\"Context passed to policy methods.\"\"\"\n\n    task: str = \"\"                                    # The current task description\n    step: int = 0                                     # Current step number (0-indexed)\n    max_steps: int = 10                               # Maximum allowed steps\n    history: list[dict[str, Any]] = field(default_factory=list)   # Execution history entries\n    variables: dict[str, Any] = field(default_factory=dict)       # Named variables from execution\n    metrics: dict[str, float] = field(default_factory=dict)       # Runtime metrics (rewards, etc.)\n    config: dict[str, Any] = field(default_factory=dict)          # Additional config\n</code></pre> Field Type Description <code>task</code> <code>str</code> The task description the agent is working on <code>step</code> <code>int</code> Current step number (0-indexed) <code>max_steps</code> <code>int</code> Maximum number of steps allowed <code>history</code> <code>list[dict]</code> Full execution history (actions, outputs, rewards) <code>variables</code> <code>dict[str, Any]</code> Named variables accumulated during execution <code>metrics</code> <code>dict[str, float]</code> Runtime metrics such as <code>last_reward</code>, cumulative totals <code>config</code> <code>dict[str, Any]</code> Additional configuration passed through to policies <p>Using PolicyContext in custom policies</p> <p>The <code>context.history</code> field is particularly useful for policies that need to analyze past behavior. For example, a reward policy might penalize repeated failed actions, or a termination policy might detect convergence by examining recent reward trends.</p>"},{"location":"policies/#base-policy-class","title":"Base Policy Class","text":"<p>All policies inherit from the <code>Policy</code> base class, which provides:</p> <pre><code>class Policy(ABC):\n    \"\"\"Base class for all policies.\"\"\"\n\n    name: str = \"base\"\n    description: str = \"Base policy\"\n\n    def __init__(self, config: dict[str, Any] | None = None):\n        self.config = config or {}\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        \"\"\"Get default configuration for this policy.\"\"\"\n        return {}\n\n    def validate_config(self) -&gt; list[str]:\n        \"\"\"Validate configuration, return list of errors.\"\"\"\n        return []\n</code></pre> <p>Configuration merging</p> <p>All built-in policies merge user-provided config with defaults using the pattern: <pre><code>config = {**self.get_default_config(), **self.config}\n</code></pre> This means you only need to override the specific parameters you want to change. Any unspecified parameters fall back to their defaults.</p>"},{"location":"policies/#quick-start","title":"Quick Start","text":""},{"location":"policies/#using-built-in-policies","title":"Using built-in policies","text":"<pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\n# Get policy instances (uses defaults if no name given)\nreward_policy = PolicyRegistry.get_reward()          # DefaultRewardPolicy\naction_policy = PolicyRegistry.get_action(\"sampling\", config={\"temperature\": 0.5})\ncompaction_policy = PolicyRegistry.get_compaction(\"hierarchical\")\ntermination_policy = PolicyRegistry.get_termination(\"final_pattern\")\n</code></pre>"},{"location":"policies/#creating-a-custom-policy","title":"Creating a custom policy","text":"<pre><code>from rlm_code.rlm.policies import RewardPolicy, PolicyRegistry, RewardSignal\n\n@PolicyRegistry.register_reward(\"my_custom_reward\")\nclass MyCustomRewardPolicy(RewardPolicy):\n    name = \"my_custom_reward\"\n    description = \"Domain-specific reward for my application\"\n\n    @classmethod\n    def get_default_config(cls):\n        return {\"bonus_multiplier\": 2.0}\n\n    def calculate(self, action, result, context):\n        config = {**self.get_default_config(), **self.config}\n        value = 0.5 if result.success else -0.3\n        value *= config[\"bonus_multiplier\"]\n        return RewardSignal(\n            value=max(-1.0, min(1.0, value)),\n            components={\"base\": value},\n            explanation=f\"Custom reward: {value:.2f}\",\n        )\n</code></pre>"},{"location":"policies/#configuration-driven-setup","title":"Configuration-driven setup","text":"<pre><code># rlm_config.yaml\npolicies:\n  reward:\n    name: research\n    config:\n      base_success: 0.4\n      fast_execution_bonus: 0.1\n  action:\n    name: sampling\n    config:\n      temperature: 0.7\n      min_probability: 0.05\n  compaction:\n    name: llm\n    config:\n      max_entries_before_compact: 15\n      preserve_last_n: 3\n  termination:\n    name: composite\n    config:\n      policies:\n        - final_pattern\n        - reward_threshold\n</code></pre> <pre><code>import yaml\nfrom rlm_code.rlm.policies import PolicyRegistry\n\nwith open(\"rlm_config.yaml\") as f:\n    config = yaml.safe_load(f)\n\npolicies = PolicyRegistry.create_from_config(config[\"policies\"])\n</code></pre>"},{"location":"policies/action-selection/","title":"Action Selection Policies","text":""},{"location":"policies/action-selection/#overview","title":"Overview","text":"<p>Action selection policies determine which action the agent should execute next, given a set of candidate actions. The choice of selection strategy directly affects the exploration-exploitation trade-off: deterministic strategies exploit known-good actions, while stochastic strategies explore alternatives that might yield better long-term outcomes.</p> <p>All action selection policies inherit from <code>ActionSelectionPolicy</code> and implement the <code>select()</code> method. They optionally override <code>rank()</code> to provide scored orderings of candidates.</p>"},{"location":"policies/action-selection/#base-class","title":"Base Class","text":""},{"location":"policies/action-selection/#actionselectionpolicy","title":"ActionSelectionPolicy","text":"<pre><code>class ActionSelectionPolicy(Policy):\n    \"\"\"Policy for selecting actions from candidates.\"\"\"\n\n    name = \"action_base\"\n    description = \"Base action selection policy\"\n\n    @abstractmethod\n    def select(\n        self,\n        candidates: list[dict[str, Any]],\n        context: PolicyContext,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Select an action from candidates.\n\n        Args:\n            candidates: List of candidate actions\n            context: Current execution context\n\n        Returns:\n            Selected action\n        \"\"\"\n        ...\n\n    def rank(\n        self,\n        candidates: list[dict[str, Any]],\n        context: PolicyContext,\n    ) -&gt; list[tuple[dict[str, Any], float]]:\n        \"\"\"\n        Rank candidates by score.\n\n        Returns list of (action, score) tuples sorted by score descending.\n        \"\"\"\n        # Default: equal scores\n        return [(c, 1.0) for c in candidates]\n</code></pre> Method Description <code>select(candidates, context)</code> Required. Choose one action from the candidate list <code>rank(candidates, context)</code> Optional. Return <code>(action, score)</code> tuples sorted by descending score"},{"location":"policies/action-selection/#actionresult","title":"ActionResult","text":"<p>Action selection policies use scores embedded in candidate dictionaries. Candidates typically include a <code>confidence</code> or <code>score</code> field:</p> <pre><code>candidates = [\n    {\"action\": \"code\", \"code\": \"print(42)\", \"confidence\": 0.9},\n    {\"action\": \"code\", \"code\": \"print(41)\", \"confidence\": 0.6},\n    {\"action\": \"final\", \"code\": \"FINAL('42')\", \"confidence\": 0.3},\n]\n</code></pre>"},{"location":"policies/action-selection/#built-in-implementations","title":"Built-in Implementations","text":""},{"location":"policies/action-selection/#greedyactionpolicy","title":"GreedyActionPolicy","text":"<p>Registration name: <code>\"greedy\"</code></p> <p>The simplest and most deterministic action selection strategy. Always picks the candidate with the highest <code>confidence</code> or <code>score</code> value. When these fields are absent, defaults to 0.5.</p> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\npolicy = PolicyRegistry.get_action(\"greedy\")\n</code></pre>"},{"location":"policies/action-selection/#configuration","title":"Configuration","text":"<p>The GreedyActionPolicy has no configurable parameters. It relies entirely on the scores present in the candidate actions.</p>"},{"location":"policies/action-selection/#behavior","title":"Behavior","text":"<pre><code>candidates = [\n    {\"action\": \"code\", \"code\": \"approach_a()\", \"confidence\": 0.7},\n    {\"action\": \"code\", \"code\": \"approach_b()\", \"confidence\": 0.9},\n    {\"action\": \"code\", \"code\": \"approach_c()\", \"confidence\": 0.4},\n]\n\nselected = policy.select(candidates, context)\n# Always returns approach_b (confidence 0.9)\n</code></pre> <p>The <code>rank()</code> method sorts candidates by their <code>confidence</code> (or <code>score</code>) field in descending order:</p> <pre><code>ranked = policy.rank(candidates, context)\n# [\n#     ({\"action\": \"code\", \"code\": \"approach_b()\", \"confidence\": 0.9}, 0.9),\n#     ({\"action\": \"code\", \"code\": \"approach_a()\", \"confidence\": 0.7}, 0.7),\n#     ({\"action\": \"code\", \"code\": \"approach_c()\", \"confidence\": 0.4}, 0.4),\n# ]\n</code></pre> <p>When to use Greedy</p> <p>Greedy selection is ideal for production environments where deterministic, reproducible behavior is important. It always exploits the model's best-guess action without any randomness. The downside is that it never explores alternatives that might be globally better.</p>"},{"location":"policies/action-selection/#samplingactionpolicy","title":"SamplingActionPolicy","text":"<p>Registration name: <code>\"sampling\"</code></p> <p>Samples from candidates using a probability distribution weighted by their scores. A temperature parameter controls the sharpness of the distribution: lower temperatures concentrate probability on higher-scored actions, while higher temperatures spread probability more evenly.</p> <pre><code>policy = PolicyRegistry.get_action(\"sampling\", config={\"temperature\": 0.5})\n</code></pre>"},{"location":"policies/action-selection/#default-configuration","title":"Default Configuration","text":"Parameter Default Description <code>temperature</code> <code>1.0</code> Controls distribution sharpness. Lower = more deterministic, higher = more random <code>min_probability</code> <code>0.01</code> Minimum selection probability for any candidate (prevents zero probability)"},{"location":"policies/action-selection/#behavior_1","title":"Behavior","text":"<p>The sampling process works as follows:</p> <ol> <li>Score extraction: Extract <code>confidence</code> or <code>score</code> from each candidate (minimum 0.01)</li> <li>Temperature scaling: Apply <code>score^(1/temperature)</code> to each score</li> <li>Normalization: Convert to a probability distribution</li> <li>Floor enforcement: Ensure every candidate has at least <code>min_probability</code></li> <li>Renormalization: Normalize again after floor enforcement</li> <li>Sampling: Draw from the resulting distribution</li> </ol> <pre><code>candidates = [\n    {\"action\": \"code\", \"code\": \"approach_a()\", \"confidence\": 0.9},\n    {\"action\": \"code\", \"code\": \"approach_b()\", \"confidence\": 0.6},\n    {\"action\": \"code\", \"code\": \"approach_c()\", \"confidence\": 0.1},\n]\n\n# With temperature=1.0 (default): probabilities roughly proportional to scores\n# With temperature=0.1: almost always picks approach_a (near-greedy)\n# With temperature=5.0: nearly uniform distribution (maximum exploration)\n</code></pre> <p>Temperature guide</p> Temperature Behavior <code>0.1 - 0.3</code> Near-greedy, strong exploitation <code>0.5 - 0.8</code> Moderate exploration with exploitation bias <code>1.0</code> Probabilities proportional to scores <code>2.0 - 5.0</code> Heavy exploration, nearly uniform sampling <p>When to use Sampling</p> <p>Sampling is ideal for research and experimentation where you want the agent to explore diverse strategies. It is also useful for ensembling -- running multiple episodes with sampling can reveal alternative solution paths that greedy selection would miss.</p>"},{"location":"policies/action-selection/#beamsearchactionpolicy","title":"BeamSearchActionPolicy","text":"<p>Registration name: <code>\"beam_search\"</code></p> <p>Maintains multiple hypotheses (beams) simultaneously and selects actions considering both immediate score and long-term diversity. Applies a length penalty to discourage running too many steps and a diversity penalty to avoid repeating the same action types.</p> <pre><code>policy = PolicyRegistry.get_action(\"beam_search\", config={\"beam_width\": 5})\n</code></pre>"},{"location":"policies/action-selection/#default-configuration_1","title":"Default Configuration","text":"Parameter Default Description <code>beam_width</code> <code>3</code> Number of hypotheses to maintain <code>length_penalty</code> <code>0.6</code> Penalty factor for longer sequences (higher = stronger penalty) <code>diversity_penalty</code> <code>0.2</code> Score reduction for repeated action types"},{"location":"policies/action-selection/#behavior_2","title":"Behavior","text":"<p>The beam search policy modifies candidate scores using two adjustments:</p> <p>Length penalty (based on current step):</p> <pre><code>length_factor = ((5 + step) / 6) ^ length_penalty\nadjusted_score = base_score / length_factor\n</code></pre> <p>This progressively reduces scores as the episode gets longer, favoring actions that lead to earlier termination.</p> <p>Diversity penalty:</p> <p>When multiple candidates share the same <code>action</code> type, subsequent occurrences receive a score reduction of <code>diversity_penalty</code>. This encourages the agent to try different approaches.</p> <pre><code>candidates = [\n    {\"action\": \"code\", \"code\": \"approach_a()\", \"confidence\": 0.8},\n    {\"action\": \"code\", \"code\": \"approach_b()\", \"confidence\": 0.75},  # -0.2 diversity\n    {\"action\": \"final\", \"code\": \"FINAL('x')\", \"confidence\": 0.7},\n]\n\n# At step 0, length_factor ~ 0.87:\n#   approach_a: 0.8 / 0.87 = 0.92\n#   approach_b: 0.75 / 0.87 - 0.2 = 0.66  (diversity penalty for repeated \"code\")\n#   FINAL:      0.7 / 0.87 = 0.80\n</code></pre> <p>The policy maintains internal beam state across calls. Call <code>reset()</code> to clear it:</p> <pre><code>policy.reset()\n</code></pre> <p>When to use Beam Search</p> <p>Beam search is ideal for complex multi-step reasoning tasks where maintaining multiple hypotheses improves solution quality. It naturally balances exploration (through diversity penalties) with exploitation (through score-based ranking) and encourages efficient solutions (through length penalties).</p>"},{"location":"policies/action-selection/#mctsactionpolicy","title":"MCTSActionPolicy","text":"<p>Registration name: <code>\"mcts\"</code></p> <p>Monte Carlo Tree Search (MCTS) action selection using the UCB1 (Upper Confidence Bound) formula to balance exploration of unvisited actions with exploitation of historically rewarding ones. Tracks visit counts and cumulative values across calls, building a progressively better model of action quality.</p> <pre><code>policy = PolicyRegistry.get_action(\"mcts\", config={\"exploration_constant\": 2.0})\n</code></pre>"},{"location":"policies/action-selection/#default-configuration_2","title":"Default Configuration","text":"Parameter Default Description <code>exploration_constant</code> <code>1.41</code> UCB1 exploration constant (sqrt(2) is theoretically optimal) <code>num_simulations</code> <code>10</code> Number of simulations per selection (for future use) <code>simulation_depth</code> <code>3</code> Depth of rollout simulations (for future use)"},{"location":"policies/action-selection/#behavior_3","title":"Behavior","text":"<p>The MCTS policy uses the UCB1 formula to compute a score for each candidate:</p> <pre><code>UCB1(action) = exploitation + exploration\n             = (total_value / visits) + c * sqrt(ln(total_visits + 1) / visits)\n</code></pre> <p>Where:</p> <ul> <li><code>total_value</code>: Cumulative reward received from this action type</li> <li><code>visits</code>: Number of times this action type has been selected</li> <li><code>total_visits</code>: Total number of selections across all actions</li> <li><code>c</code>: The <code>exploration_constant</code> parameter</li> </ul> <p>Unvisited actions receive infinite UCB1 score, ensuring every action type is tried at least once before exploitation begins.</p> <pre><code># First call: approach_a selected (all have infinite UCB, picks first)\nselected = policy.select(candidates, context)\n\n# Provide feedback after execution\npolicy.update(selected, reward=0.8)\n\n# Second call: approach_b selected (unvisited = infinite UCB)\nselected = policy.select(candidates, context)\npolicy.update(selected, reward=0.3)\n\n# Third call: approach_a likely selected again (0.8 value &gt; 0.3 value)\n# unless exploration term favors trying approach_c\nselected = policy.select(candidates, context)\n</code></pre> <p>The <code>update()</code> method must be called after each action to provide reward feedback:</p> <pre><code>policy.update(action, reward)  # Update visit count and value for this action\n</code></pre> <p>The <code>rank()</code> method returns candidates ranked by their average reward:</p> <pre><code>ranked = policy.rank(candidates, context)\n# Sorted by average value (total_value / visits), defaulting to 0.5 for unvisited\n</code></pre> <p>Call <code>reset()</code> to clear all learned statistics:</p> <pre><code>policy.reset()\n</code></pre> <p>When to use MCTS</p> <p>MCTS is ideal for complex decision trees where the same action types recur across steps and historical performance is predictive of future performance. It excels in tasks where the agent needs to learn which tool or approach works best through trial and error. The exploration constant controls the balance: higher values explore more, lower values exploit learned knowledge more aggressively.</p> <p>Stateful policy</p> <p>Unlike Greedy and Sampling, MCTS maintains internal state (visit counts and values) across calls. This state must be managed carefully: call <code>reset()</code> between independent episodes, and always call <code>update()</code> after each selection to provide reward feedback.</p>"},{"location":"policies/action-selection/#comparison","title":"Comparison","text":"Policy Deterministic Stateful Exploration Best For Greedy Yes No None Production, reproducibility Sampling No No Temperature-controlled Research, diversity Beam Search Yes Yes (beams) Diversity penalty Multi-step reasoning MCTS No Yes (UCB1) UCB1-driven Complex decision trees"},{"location":"policies/action-selection/#decision-guide","title":"Decision Guide","text":"<pre><code>Is reproducibility critical?\n  YES --&gt; Greedy\n  NO  --&gt; Do you need multi-hypothesis reasoning?\n            YES --&gt; Beam Search\n            NO  --&gt; Do action types repeat across steps?\n                      YES --&gt; MCTS (learns which actions work)\n                      NO  --&gt; Sampling (explores broadly)\n</code></pre>"},{"location":"policies/action-selection/#creating-a-custom-action-selection-policy","title":"Creating a Custom Action Selection Policy","text":"<pre><code>from rlm_code.rlm.policies import (\n    ActionSelectionPolicy,\n    PolicyRegistry,\n    PolicyContext,\n)\nfrom typing import Any\n\n\n@PolicyRegistry.register_action(\"epsilon_greedy\")\nclass EpsilonGreedyActionPolicy(ActionSelectionPolicy):\n    \"\"\"\n    Epsilon-greedy: pick best action with probability (1 - epsilon),\n    random action with probability epsilon.\n    \"\"\"\n\n    name = \"epsilon_greedy\"\n    description = \"Epsilon-greedy exploration strategy\"\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        return {\n            \"epsilon\": 0.1,\n            \"epsilon_decay\": 0.99,\n            \"min_epsilon\": 0.01,\n        }\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        cfg = {**self.get_default_config(), **self.config}\n        self._epsilon = cfg[\"epsilon\"]\n\n    def select(self, candidates, context):\n        import random\n\n        if not candidates:\n            raise ValueError(\"No candidates to select from\")\n\n        cfg = {**self.get_default_config(), **self.config}\n\n        if random.random() &lt; self._epsilon:\n            # Explore: random selection\n            selected = random.choice(candidates)\n        else:\n            # Exploit: pick best\n            ranked = self.rank(candidates, context)\n            selected = ranked[0][0]\n\n        # Decay epsilon\n        self._epsilon = max(\n            cfg[\"min_epsilon\"],\n            self._epsilon * cfg[\"epsilon_decay\"],\n        )\n        return selected\n\n    def rank(self, candidates, context):\n        scored = []\n        for c in candidates:\n            score = c.get(\"confidence\", c.get(\"score\", 0.5))\n            scored.append((c, float(score)))\n        return sorted(scored, key=lambda x: x[1], reverse=True)\n\n\n# Use it\npolicy = PolicyRegistry.get_action(\"epsilon_greedy\", config={\"epsilon\": 0.2})\n</code></pre>"},{"location":"policies/compaction/","title":"Compaction Policies","text":""},{"location":"policies/compaction/#overview","title":"Overview","text":"<p>Compaction policies manage the agent's execution history, compressing older entries to keep context within token budgets while preserving the information the agent needs to make good decisions. As sessions grow longer, uncompacted history can exceed model context windows and slow down inference. Compaction policies solve this by summarizing or discarding older entries according to configurable strategies.</p> <p>All compaction policies inherit from <code>CompactionPolicy</code> and implement two methods: <code>should_compact()</code> to detect when compaction is needed, and <code>compact()</code> to perform the compression.</p>"},{"location":"policies/compaction/#base-class","title":"Base Class","text":""},{"location":"policies/compaction/#compactionpolicy","title":"CompactionPolicy","text":"<pre><code>class CompactionPolicy(Policy):\n    \"\"\"Policy for compacting memory/history.\"\"\"\n\n    name = \"compaction_base\"\n    description = \"Base compaction policy\"\n\n    @abstractmethod\n    def should_compact(self, context: PolicyContext) -&gt; bool:\n        \"\"\"Check if compaction should be triggered.\"\"\"\n        ...\n\n    @abstractmethod\n    def compact(\n        self,\n        history: list[dict[str, Any]],\n        context: PolicyContext,\n    ) -&gt; tuple[list[dict[str, Any]], str]:\n        \"\"\"\n        Compact history.\n\n        Args:\n            history: Current history entries\n            context: Execution context\n\n        Returns:\n            Tuple of (compacted_history, summary_text)\n        \"\"\"\n        ...\n</code></pre> Method Return Type Description <code>should_compact(context)</code> <code>bool</code> Returns <code>True</code> when the history is long enough to warrant compaction <code>compact(history, context)</code> <code>(list, str)</code> Returns the compacted history and a human-readable summary of what was compressed <p>The <code>compact()</code> method returns a tuple:</p> <ul> <li>First element: The new, shorter history list. Typically includes a summary entry followed by preserved recent entries.</li> <li>Second element: A human-readable summary string describing what was compacted.</li> </ul>"},{"location":"policies/compaction/#summary-entry-format","title":"Summary Entry Format","text":"<p>All built-in compaction policies produce summary entries in this format:</p> <pre><code>{\n    \"type\": \"summary\",\n    \"content\": \"...\",              # The summary text\n    \"entries_summarized\": 8,       # Number of original entries compressed\n}\n</code></pre>"},{"location":"policies/compaction/#built-in-implementations","title":"Built-in Implementations","text":""},{"location":"policies/compaction/#llmcompactionpolicy","title":"LLMCompactionPolicy","text":"<p>Registration name: <code>\"llm\"</code></p> <p>Uses an LLM to intelligently summarize execution history. Produces the highest-quality summaries because the LLM can identify key findings, successful approaches, and important context. Falls back to deterministic summarization if no LLM connector is available.</p> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\npolicy = PolicyRegistry.get_compaction(\"llm\")\n</code></pre>"},{"location":"policies/compaction/#default-configuration","title":"Default Configuration","text":"Parameter Default Description <code>min_entries_to_compact</code> <code>5</code> Minimum history length before compaction is considered <code>max_entries_before_compact</code> <code>10</code> Trigger compaction when history reaches this length <code>preserve_last_n</code> <code>2</code> Number of most recent entries to keep in full detail <code>summary_max_tokens</code> <code>200</code> Maximum token count for the LLM summary <code>include_key_findings</code> <code>True</code> Instruct the LLM to highlight key findings"},{"location":"policies/compaction/#behavior","title":"Behavior","text":"<ol> <li>Trigger check: Compaction triggers when <code>len(history) &gt;= max_entries_before_compact</code></li> <li>Split: History is divided into entries to summarize (older) and entries to preserve (most recent <code>preserve_last_n</code>)</li> <li>Summarize: If an LLM connector is set, uses it to generate a context-aware summary. Otherwise, falls back to deterministic summarization (action counts + key outputs)</li> <li>Result: Returns <code>[summary_entry] + preserved_entries</code></li> </ol> <pre><code># Set up with LLM connector for best quality\npolicy.set_llm_connector(my_llm_connector)\n</code></pre> <p>The LLM receives a prompt that includes the task description and a formatted version of each history entry:</p> <pre><code>Summarize the following RLM execution history in 200 tokens or less.\nFocus on key findings, successful approaches, and important context.\n\nTask: Solve the optimization problem\n\nHistory:\nStep 1: code - Loaded dataset with 1000 rows...\nStep 2: code - Defined objective function...\n...\n</code></pre> <p>LLM connector setup</p> <p>Call <code>policy.set_llm_connector(connector)</code> to provide an LLM for summarization. The connector must have a <code>generate(prompt)</code> method that returns a string. Without a connector, the policy automatically falls back to deterministic summarization.</p> <p>Cost considerations</p> <p>LLM-based compaction produces the best summaries but incurs additional API costs for each compaction. For cost-sensitive applications, consider using <code>deterministic</code> or <code>sliding_window</code> instead, or increase <code>max_entries_before_compact</code> to compact less frequently.</p>"},{"location":"policies/compaction/#deterministiccompactionpolicy","title":"DeterministicCompactionPolicy","text":"<p>Registration name: <code>\"deterministic\"</code></p> <p>Uses fixed, rule-based compression without any LLM calls. Produces summaries by counting action types and extracting key non-error outputs. Fast, predictable, and free of external dependencies.</p> <pre><code>policy = PolicyRegistry.get_compaction(\"deterministic\")\n</code></pre>"},{"location":"policies/compaction/#default-configuration_1","title":"Default Configuration","text":"Parameter Default Description <code>max_entries</code> <code>8</code> Trigger compaction when history exceeds this <code>preserve_last_n</code> <code>2</code> Number of recent entries to keep in full <code>max_output_chars</code> <code>200</code> Maximum characters per extracted output <code>include_action_counts</code> <code>True</code> Include action type counts in summary"},{"location":"policies/compaction/#behavior_1","title":"Behavior","text":"<ol> <li>Trigger check: Compaction triggers when <code>len(history) &gt; max_entries</code></li> <li>Split: Divides into entries to summarize and entries to preserve</li> <li>Action counts: Counts occurrences of each action type (e.g., <code>code(5), search(2)</code>)</li> <li>Key outputs: Extracts the first 3 non-error outputs, truncated to <code>max_output_chars</code></li> <li>Result: Joins components with <code>|</code> separator</li> </ol> <p>Example summary output:</p> <pre><code>Previous 6 steps: code(4), search(2) | Key outputs: Found 42 matching results; Dataset loaded with 1000 rows; Computed optimal value 3.14\n</code></pre> <pre><code># Compaction result\ncompacted_history, summary = policy.compact(history, context)\n# compacted_history = [\n#     {\"type\": \"summary\", \"content\": \"Previous 6 steps: code(4), search(2) | ...\", \"entries_summarized\": 6},\n#     history[-2],  # second-to-last entry (preserved)\n#     history[-1],  # last entry (preserved)\n# ]\n</code></pre> <p>When to use Deterministic</p> <p>Deterministic compaction is the best choice when you need zero LLM cost, deterministic behavior (same input always produces same output), and fast execution. The quality of summaries is lower than LLM-based compaction but sufficient for most applications.</p>"},{"location":"policies/compaction/#slidingwindowcompactionpolicy","title":"SlidingWindowCompactionPolicy","text":"<p>Registration name: <code>\"sliding_window\"</code></p> <p>The simplest compaction strategy: keeps only the <code>N</code> most recent history entries and discards everything else. Optionally prepends a marker entry noting how many entries were discarded.</p> <pre><code>policy = PolicyRegistry.get_compaction(\"sliding_window\")\n</code></pre>"},{"location":"policies/compaction/#default-configuration_2","title":"Default Configuration","text":"Parameter Default Description <code>window_size</code> <code>5</code> Number of recent entries to keep <code>include_summary_marker</code> <code>True</code> Add a marker entry noting discarded count"},{"location":"policies/compaction/#behavior_2","title":"Behavior","text":"<ol> <li>Trigger check: Compaction triggers when <code>len(history) &gt; window_size</code></li> <li>Discard: All entries older than the window are discarded</li> <li>Marker (optional): If <code>include_summary_marker</code> is True, prepends a summary entry</li> </ol> <pre><code># 12 entries in history, window_size=5\ncompacted_history, summary = policy.compact(history, context)\n# compacted_history = [\n#     {\"type\": \"summary\", \"content\": \"[7 earlier entries discarded]\", \"entries_summarized\": 7},\n#     history[7],   # 5th from end\n#     history[8],\n#     history[9],\n#     history[10],\n#     history[11],  # most recent\n# ]\n# summary = \"[7 earlier entries discarded]\"\n</code></pre> <p>Information loss</p> <p>Sliding window compaction permanently discards older history without summarization. Important early findings, variable definitions, or failed approaches are lost. For tasks where early context matters, consider <code>deterministic</code>, <code>llm</code>, or <code>hierarchical</code> compaction instead.</p> <p>When to use Sliding Window</p> <p>Sliding window is ideal for simple, short-horizon tasks where only recent actions matter, token-constrained environments where you need guaranteed maximum history size, and high-throughput scenarios where compaction speed matters.</p>"},{"location":"policies/compaction/#hierarchicalcompactionpolicy","title":"HierarchicalCompactionPolicy","text":"<p>Registration name: <code>\"hierarchical\"</code></p> <p>Multi-level compaction that maintains history at different granularities. Recent entries are kept in full detail, medium-age entries receive partial summarization, and old entries are compressed to minimal summaries. Additionally, maintains a rolling list of historical summaries for very long sessions.</p> <pre><code>policy = PolicyRegistry.get_compaction(\"hierarchical\")\n</code></pre>"},{"location":"policies/compaction/#default-configuration_3","title":"Default Configuration","text":"Parameter Default Description <code>recent_window</code> <code>3</code> Number of entries kept at full detail <code>medium_window</code> <code>5</code> Number of entries kept at partial detail <code>compress_threshold</code> <code>10</code> Total entries (including summaries) before compaction triggers <code>summary_detail_levels</code> <code>3</code> Number of detail levels (informational)"},{"location":"policies/compaction/#behavior_3","title":"Behavior","text":"<p>History is divided into three tiers:</p> Tier Detail Level Format Example Old Level 1 (minimal) Action types only <code>[4 steps: code, search]</code> Medium Level 2 (partial) Action + truncated output <code>code: Found 42... \\| search: Results...</code> Recent Level 3 (full) Complete entries Original history entries unchanged <p>The compaction process:</p> <ol> <li>Trigger check: Compaction triggers when <code>len(history) + len(historical_summaries) &gt; compress_threshold</code></li> <li>Tier split:<ul> <li>Recent: Last <code>recent_window</code> entries (full detail)</li> <li>Medium: Next <code>medium_window</code> entries before recent (partial detail)</li> <li>Old: Everything else (minimal compression)</li> </ul> </li> <li>Historical summaries: Old tier summaries are accumulated across compaction cycles. The last 3 are retained.</li> <li>Assembly: The compacted history includes old summary, historical summaries, medium summary, and recent entries.</li> </ol> <pre><code># After several compaction cycles, the result might look like:\ncompacted_history = [\n    {\"type\": \"summary\", \"tier\": \"old\", \"content\": \"[4 steps: code, search]\", \"entries_summarized\": 4},\n    {\"type\": \"historical_summary\", \"content\": \"Previous cycle summary 1 | Previous cycle summary 2\"},\n    {\"type\": \"summary\", \"tier\": \"medium\", \"content\": \"code: Loaded data... | search: Found results...\", \"entries_summarized\": 3},\n    # ... 3 recent entries in full detail ...\n]\n</code></pre> <p>Call <code>reset()</code> to clear accumulated historical summaries:</p> <pre><code>policy.reset()\n</code></pre> <p>Long-running sessions</p> <p>Hierarchical compaction is specifically designed for very long sessions (50+ steps) where maintaining some awareness of early history is important. The multi-level approach means the agent always has access to a high-level view of its entire history while keeping recent context in full detail.</p>"},{"location":"policies/compaction/#comparison","title":"Comparison","text":"Policy LLM Required Summary Quality Speed Token Cost Best For LLM Optional (fallback available) Highest Slow High Quality-critical tasks Deterministic No Medium Fast Zero General use, cost-sensitive Sliding Window No None (discard) Fastest Zero Simple/short tasks Hierarchical No Medium (multi-level) Fast Zero Long sessions"},{"location":"policies/compaction/#decision-guide","title":"Decision Guide","text":"<pre><code>How long are your sessions?\n  Short (&lt; 10 steps):\n    Is token budget tight? --&gt; Sliding Window\n    Otherwise             --&gt; Deterministic\n  Medium (10-30 steps):\n    Need best summaries?  --&gt; LLM\n    Otherwise             --&gt; Deterministic\n  Long (30+ steps):\n    Need multi-level context? --&gt; Hierarchical\n    Need best summaries?      --&gt; LLM (with higher max_entries_before_compact)\n</code></pre>"},{"location":"policies/compaction/#creating-a-custom-compaction-policy","title":"Creating a Custom Compaction Policy","text":"<pre><code>from rlm_code.rlm.policies import (\n    CompactionPolicy,\n    PolicyRegistry,\n    PolicyContext,\n)\nfrom typing import Any\n\n\n@PolicyRegistry.register_compaction(\"importance_weighted\")\nclass ImportanceWeightedCompactionPolicy(CompactionPolicy):\n    \"\"\"\n    Keep entries based on their importance score.\n    High-reward and error entries are preserved;\n    routine successful actions are summarized.\n    \"\"\"\n\n    name = \"importance_weighted\"\n    description = \"Preserve high-importance entries, summarize routine ones\"\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        return {\n            \"max_entries\": 8,\n            \"preserve_errors\": True,\n            \"preserve_high_reward_threshold\": 0.7,\n            \"max_preserved\": 4,\n        }\n\n    def should_compact(self, context: PolicyContext) -&gt; bool:\n        config = {**self.get_default_config(), **self.config}\n        return len(context.history) &gt; config[\"max_entries\"]\n\n    def compact(self, history, context):\n        config = {**self.get_default_config(), **self.config}\n        threshold = config[\"preserve_high_reward_threshold\"]\n        max_preserved = config[\"max_preserved\"]\n\n        # Score each entry by importance\n        important = []\n        routine = []\n        for entry in history:\n            reward = entry.get(\"reward\", 0.0)\n            has_error = bool(entry.get(\"error\"))\n\n            if (config[\"preserve_errors\"] and has_error) or reward &gt;= threshold:\n                important.append(entry)\n            else:\n                routine.append(entry)\n\n        # Limit important entries\n        important = important[-max_preserved:]\n\n        # Summarize routine entries\n        summary_text = f\"Summarized {len(routine)} routine steps\"\n        summary_entry = {\n            \"type\": \"summary\",\n            \"content\": summary_text,\n            \"entries_summarized\": len(routine),\n        }\n\n        return [summary_entry] + important, summary_text\n\n\n# Use it\npolicy = PolicyRegistry.get_compaction(\"importance_weighted\")\n</code></pre>"},{"location":"policies/registry/","title":"Policy Registry","text":""},{"location":"policies/registry/#overview","title":"Overview","text":"<p>The <code>PolicyRegistry</code> is the central management hub for all policies in RLM Code. It provides decorator-based registration, lookup by name, configuration-driven instantiation, and discovery of all available policies. The registry uses class-level dictionaries, so registered policies are available globally without needing to pass registry instances around.</p> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n</code></pre>"},{"location":"policies/registry/#class-reference","title":"Class Reference","text":""},{"location":"policies/registry/#policyregistry","title":"PolicyRegistry","text":"<pre><code>class PolicyRegistry:\n    \"\"\"Central registry for all policy types.\"\"\"\n\n    # Internal registries (class-level)\n    _reward_policies: dict[str, Type[RewardPolicy]] = {}\n    _action_policies: dict[str, Type[ActionSelectionPolicy]] = {}\n    _compaction_policies: dict[str, Type[CompactionPolicy]] = {}\n    _termination_policies: dict[str, Type[TerminationPolicy]] = {}\n\n    # Default policy names\n    _default_reward: str = \"default\"\n    _default_action: str = \"greedy\"\n    _default_compaction: str = \"sliding_window\"\n    _default_termination: str = \"final_pattern\"\n</code></pre>"},{"location":"policies/registry/#default-policies","title":"Default Policies","text":"<p>When no policy name is specified, the registry returns these defaults:</p> Category Default Policy Description Reward <code>\"default\"</code> <code>DefaultRewardPolicy</code> -- balanced reward Action <code>\"greedy\"</code> <code>GreedyActionPolicy</code> -- deterministic best-action Compaction <code>\"sliding_window\"</code> <code>SlidingWindowCompactionPolicy</code> -- keep last N entries Termination <code>\"final_pattern\"</code> <code>FinalPatternTerminationPolicy</code> -- detect FINAL() patterns"},{"location":"policies/registry/#registration","title":"Registration","text":""},{"location":"policies/registry/#decorator-based-registration","title":"Decorator-Based Registration","text":"<p>The primary way to register policies is through class decorators. Each policy category has its own registration decorator:</p> <pre><code>from rlm_code.rlm.policies import (\n    PolicyRegistry,\n    RewardPolicy,\n    ActionSelectionPolicy,\n    CompactionPolicy,\n    TerminationPolicy,\n)\n\n@PolicyRegistry.register_reward(\"my_reward\")\nclass MyRewardPolicy(RewardPolicy):\n    name = \"my_reward\"\n    description = \"My custom reward policy\"\n\n    def calculate(self, action, result, context):\n        ...\n\n@PolicyRegistry.register_action(\"my_action\")\nclass MyActionPolicy(ActionSelectionPolicy):\n    name = \"my_action\"\n    description = \"My custom action policy\"\n\n    def select(self, candidates, context):\n        ...\n\n@PolicyRegistry.register_compaction(\"my_compaction\")\nclass MyCompactionPolicy(CompactionPolicy):\n    name = \"my_compaction\"\n    description = \"My custom compaction policy\"\n\n    def should_compact(self, context):\n        ...\n    def compact(self, history, context):\n        ...\n\n@PolicyRegistry.register_termination(\"my_termination\")\nclass MyTerminationPolicy(TerminationPolicy):\n    name = \"my_termination\"\n    description = \"My custom termination policy\"\n\n    def should_terminate(self, result, context):\n        ...\n</code></pre>"},{"location":"policies/registry/#registration-methods","title":"Registration Methods","text":"Method Decorator Syntax Registers <code>register_reward(name)</code> <code>@PolicyRegistry.register_reward(\"name\")</code> Reward policy class <code>register_action(name)</code> <code>@PolicyRegistry.register_action(\"name\")</code> Action selection policy class <code>register_compaction(name)</code> <code>@PolicyRegistry.register_compaction(\"name\")</code> Compaction policy class <code>register_termination(name)</code> <code>@PolicyRegistry.register_termination(\"name\")</code> Termination policy class <p>Name resolution</p> <p>If the <code>name</code> argument is <code>None</code>, the decorator falls back to the class's <code>name</code> attribute. It is recommended to always provide an explicit name to avoid ambiguity: <pre><code># Explicit name (recommended)\n@PolicyRegistry.register_reward(\"my_reward\")\nclass MyRewardPolicy(RewardPolicy):\n    name = \"my_reward\"\n    ...\n\n# Implicit name (uses class attribute)\n@PolicyRegistry.register_reward()\nclass MyRewardPolicy(RewardPolicy):\n    name = \"my_reward\"  # This name is used\n    ...\n</code></pre></p>"},{"location":"policies/registry/#lookup-and-instantiation","title":"Lookup and Instantiation","text":""},{"location":"policies/registry/#getting-policy-instances","title":"Getting Policy Instances","text":"<p>Use the <code>get_*</code> methods to retrieve instantiated policy objects by name:</p> <pre><code># Get with default name\nreward = PolicyRegistry.get_reward()           # DefaultRewardPolicy()\naction = PolicyRegistry.get_action()           # GreedyActionPolicy()\ncompaction = PolicyRegistry.get_compaction()    # SlidingWindowCompactionPolicy()\ntermination = PolicyRegistry.get_termination() # FinalPatternTerminationPolicy()\n\n# Get by name\nreward = PolicyRegistry.get_reward(\"strict\")\naction = PolicyRegistry.get_action(\"mcts\")\ncompaction = PolicyRegistry.get_compaction(\"llm\")\ntermination = PolicyRegistry.get_termination(\"confidence\")\n\n# Get with custom configuration\nreward = PolicyRegistry.get_reward(\"research\", config={\n    \"base_success\": 0.4,\n    \"fast_execution_bonus\": 0.1,\n})\naction = PolicyRegistry.get_action(\"sampling\", config={\n    \"temperature\": 0.7,\n    \"min_probability\": 0.05,\n})\n</code></pre>"},{"location":"policies/registry/#lookup-methods","title":"Lookup Methods","text":"Method Signature Returns <code>get_reward</code> <code>(name: str \\| None, config: dict \\| None) -&gt; RewardPolicy</code> Instantiated reward policy <code>get_action</code> <code>(name: str \\| None, config: dict \\| None) -&gt; ActionSelectionPolicy</code> Instantiated action policy <code>get_compaction</code> <code>(name: str \\| None, config: dict \\| None) -&gt; CompactionPolicy</code> Instantiated compaction policy <code>get_termination</code> <code>(name: str \\| None, config: dict \\| None) -&gt; TerminationPolicy</code> Instantiated termination policy <p>Unknown policy names</p> <p>If you request a policy name that is not registered, a <code>ValueError</code> is raised with a helpful message listing all available policies: <pre><code>PolicyRegistry.get_reward(\"nonexistent\")\n# ValueError: Unknown reward policy 'nonexistent'. Available: default, strict, lenient, research\n</code></pre></p>"},{"location":"policies/registry/#configuration-based-instantiation","title":"Configuration-Based Instantiation","text":""},{"location":"policies/registry/#create_from_config","title":"create_from_config","text":"<p>The <code>create_from_config()</code> class method creates a complete set of policy instances from a configuration dictionary. This is the preferred way to set up policies from configuration files.</p> <pre><code>policies = PolicyRegistry.create_from_config({\n    \"reward\": {\n        \"name\": \"research\",\n        \"config\": {\n            \"base_success\": 0.4,\n            \"fast_execution_bonus\": 0.1,\n        },\n    },\n    \"action\": {\n        \"name\": \"sampling\",\n        \"config\": {\n            \"temperature\": 0.7,\n        },\n    },\n    \"compaction\": {\n        \"name\": \"hierarchical\",\n        \"config\": {\n            \"recent_window\": 5,\n        },\n    },\n    \"termination\": {\n        \"name\": \"composite\",\n        \"config\": {\n            \"policies\": [\"final_pattern\", \"reward_threshold\"],\n            \"require_all\": False,\n        },\n    },\n})\n</code></pre> <p>The returned dictionary contains instantiated policy objects:</p> <pre><code>policies[\"reward\"]       # ResearchRewardPolicy instance\npolicies[\"action\"]       # SamplingActionPolicy instance\npolicies[\"compaction\"]   # HierarchicalCompactionPolicy instance\npolicies[\"termination\"]  # CompositeTerminationPolicy instance\n</code></pre> <p>Partial configuration</p> <p>You can include only the categories you want to customize. Omitted categories are simply absent from the returned dictionary: <pre><code># Only customize reward and action\npolicies = PolicyRegistry.create_from_config({\n    \"reward\": {\"name\": \"strict\"},\n    \"action\": {\"name\": \"mcts\"},\n})\n# policies = {\"reward\": StrictRewardPolicy(), \"action\": MCTSActionPolicy()}\n# No \"compaction\" or \"termination\" keys\n</code></pre></p>"},{"location":"policies/registry/#configuration-format","title":"Configuration Format","text":"<p>The configuration dictionary follows this schema:</p> <pre><code>{\n    \"&lt;category&gt;\": {          # \"reward\", \"action\", \"compaction\", or \"termination\"\n        \"name\": str,         # Registered policy name\n        \"config\": dict,      # Optional: policy-specific configuration\n    },\n    ...\n}\n</code></pre>"},{"location":"policies/registry/#yaml-configuration-example","title":"YAML Configuration Example","text":"<pre><code># rlm_policies.yaml\nreward:\n  name: research\n  config:\n    base_success: 0.4\n    base_failure: 0.3\n    fast_execution_bonus: 0.1\n    step_penalty_per_step: 0.02\n\naction:\n  name: sampling\n  config:\n    temperature: 0.7\n    min_probability: 0.05\n\ncompaction:\n  name: llm\n  config:\n    max_entries_before_compact: 15\n    preserve_last_n: 3\n    summary_max_tokens: 300\n\ntermination:\n  name: final_pattern\n  config:\n    case_sensitive: false\n    extract_answer: true\n</code></pre> <pre><code>import yaml\nfrom rlm_code.rlm.policies import PolicyRegistry\n\nwith open(\"rlm_policies.yaml\") as f:\n    config = yaml.safe_load(f)\n\npolicies = PolicyRegistry.create_from_config(config)\n</code></pre>"},{"location":"policies/registry/#discovery-and-listing","title":"Discovery and Listing","text":""},{"location":"policies/registry/#listing-registered-policies","title":"Listing Registered Policies","text":"<p>The registry provides methods to enumerate all registered policies:</p> <pre><code># List all policies across all categories\nall_policies = PolicyRegistry.list_all()\n# Returns:\n# {\n#     \"reward\": [\n#         {\"name\": \"default\", \"description\": \"Balanced reward for general use\"},\n#         {\"name\": \"strict\", \"description\": \"Heavy penalties for errors...\"},\n#         {\"name\": \"lenient\", \"description\": \"Forgiving rewards, encourages exploration\"},\n#         {\"name\": \"research\", \"description\": \"Detailed breakdowns for reward function research\"},\n#     ],\n#     \"action\": [...],\n#     \"compaction\": [...],\n#     \"termination\": [...],\n# }\n\n# List by category\nreward_list = PolicyRegistry.list_reward_policies()\naction_list = PolicyRegistry.list_action_policies()\ncompaction_list = PolicyRegistry.list_compaction_policies()\ntermination_list = PolicyRegistry.list_termination_policies()\n</code></pre>"},{"location":"policies/registry/#listing-methods","title":"Listing Methods","text":"Method Returns <code>list_all()</code> <code>dict[str, list[dict[str, str]]]</code> -- all policies grouped by category <code>list_reward_policies()</code> <code>list[dict[str, str]]</code> -- reward policies with name and description <code>list_action_policies()</code> <code>list[dict[str, str]]</code> -- action policies with name and description <code>list_compaction_policies()</code> <code>list[dict[str, str]]</code> -- compaction policies with name and description <code>list_termination_policies()</code> <code>list[dict[str, str]]</code> -- termination policies with name and description <p>Each entry in the list is a dictionary with <code>\"name\"</code> and <code>\"description\"</code> keys.</p>"},{"location":"policies/registry/#changing-defaults","title":"Changing Defaults","text":"<p>You can change the default policy for each category at runtime:</p> <pre><code>PolicyRegistry.set_default_reward(\"research\")\nPolicyRegistry.set_default_action(\"sampling\")\nPolicyRegistry.set_default_compaction(\"hierarchical\")\nPolicyRegistry.set_default_termination(\"confidence\")\n\n# Now get_reward() returns ResearchRewardPolicy\nreward = PolicyRegistry.get_reward()\n</code></pre> Method Description <code>set_default_reward(name)</code> Set the default reward policy <code>set_default_action(name)</code> Set the default action selection policy <code>set_default_compaction(name)</code> Set the default compaction policy <code>set_default_termination(name)</code> Set the default termination policy <p>Validation</p> <p>All <code>set_default_*</code> methods raise <code>ValueError</code> if the specified name is not registered: <pre><code>PolicyRegistry.set_default_reward(\"nonexistent\")\n# ValueError: Unknown reward policy: nonexistent\n</code></pre></p>"},{"location":"policies/registry/#complete-example-custom-policy-lifecycle","title":"Complete Example: Custom Policy Lifecycle","text":"<p>This example shows the full lifecycle of creating, registering, configuring, and using a custom policy:</p> <pre><code>from rlm_code.rlm.policies import (\n    PolicyRegistry,\n    RewardPolicy,\n    RewardSignal,\n    ActionResult,\n    PolicyContext,\n)\nfrom typing import Any\n\n\n# Step 1: Define and register the policy\n@PolicyRegistry.register_reward(\"weighted_components\")\nclass WeightedComponentsRewardPolicy(RewardPolicy):\n    \"\"\"\n    Reward policy with user-defined component weights.\n    Allows fine-grained control over which factors\n    matter most for a specific task.\n    \"\"\"\n\n    name = \"weighted_components\"\n    description = \"User-defined weighted reward components\"\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        return {\n            \"weights\": {\n                \"success\": 1.0,\n                \"speed\": 0.5,\n                \"brevity\": 0.3,\n            },\n            \"success_value\": 0.5,\n            \"speed_threshold_ms\": 1000,\n            \"brevity_threshold_chars\": 500,\n        }\n\n    def validate_config(self) -&gt; list[str]:\n        \"\"\"Validate that weights are non-negative.\"\"\"\n        errors = []\n        config = {**self.get_default_config(), **self.config}\n        weights = config.get(\"weights\", {})\n        for name, weight in weights.items():\n            if weight &lt; 0:\n                errors.append(f\"Weight '{name}' must be non-negative, got {weight}\")\n        return errors\n\n    def calculate(self, action, result, context):\n        config = {**self.get_default_config(), **self.config}\n        weights = config[\"weights\"]\n        components = {}\n        total = 0.0\n\n        # Success component\n        if \"success\" in weights:\n            val = config[\"success_value\"] if result.success else -config[\"success_value\"]\n            weighted = val * weights[\"success\"]\n            components[\"success\"] = weighted\n            total += weighted\n\n        # Speed component\n        if \"speed\" in weights and result.duration_ms &gt; 0:\n            fast = result.duration_ms &lt; config[\"speed_threshold_ms\"]\n            val = 0.2 if fast else -0.1\n            weighted = val * weights[\"speed\"]\n            components[\"speed\"] = weighted\n            total += weighted\n\n        # Brevity component\n        if \"brevity\" in weights:\n            brief = len(result.output or \"\") &lt; config[\"brevity_threshold_chars\"]\n            val = 0.1 if brief else -0.05\n            weighted = val * weights[\"brevity\"]\n            components[\"brevity\"] = weighted\n            total += weighted\n\n        return RewardSignal(\n            value=max(-1.0, min(1.0, total)),\n            components=components,\n            explanation=f\"Weighted reward: {total:.3f}\",\n        )\n\n\n# Step 2: Verify registration\nall_rewards = PolicyRegistry.list_reward_policies()\nprint([p[\"name\"] for p in all_rewards])\n# ['default', 'strict', 'lenient', 'research', 'weighted_components']\n\n\n# Step 3: Validate configuration\npolicy = PolicyRegistry.get_reward(\"weighted_components\", config={\n    \"weights\": {\"success\": 2.0, \"speed\": -0.5},  # Invalid: negative weight\n})\nerrors = policy.validate_config()\n# [\"Weight 'speed' must be non-negative, got -0.5\"]\n\n\n# Step 4: Use with valid configuration\npolicy = PolicyRegistry.get_reward(\"weighted_components\", config={\n    \"weights\": {\"success\": 2.0, \"speed\": 1.0, \"brevity\": 0.0},\n})\n\n\n# Step 5: Set as default\nPolicyRegistry.set_default_reward(\"weighted_components\")\ndefault_policy = PolicyRegistry.get_reward()  # Returns WeightedComponentsRewardPolicy\n\n\n# Step 6: Use in configuration-driven setup\npolicies = PolicyRegistry.create_from_config({\n    \"reward\": {\n        \"name\": \"weighted_components\",\n        \"config\": {\n            \"weights\": {\"success\": 1.5, \"speed\": 0.8},\n            \"speed_threshold_ms\": 500,\n        },\n    },\n    \"action\": {\"name\": \"greedy\"},\n    \"termination\": {\"name\": \"final_pattern\"},\n})\n</code></pre>"},{"location":"policies/reward/","title":"Reward Policies","text":""},{"location":"policies/reward/#overview","title":"Overview","text":"<p>Reward policies calculate scalar reward signals from action results, providing the feedback mechanism that drives the RLM agent's learning and decision-making. Each reward policy produces a <code>RewardSignal</code> containing both a numerical value and a detailed component breakdown, enabling transparency and debugging of the reward computation.</p> <p>All reward policies inherit from <code>RewardPolicy</code> and implement the <code>calculate()</code> method.</p>"},{"location":"policies/reward/#base-classes","title":"Base Classes","text":""},{"location":"policies/reward/#rewardpolicy","title":"RewardPolicy","text":"<pre><code>class RewardPolicy(Policy):\n    \"\"\"Policy for calculating rewards from action results.\"\"\"\n\n    name = \"reward_base\"\n    description = \"Base reward policy\"\n\n    @abstractmethod\n    def calculate(\n        self,\n        action: dict[str, Any],\n        result: ActionResult,\n        context: PolicyContext,\n    ) -&gt; RewardSignal:\n        \"\"\"Calculate reward for an action.\"\"\"\n        ...\n\n    def on_episode_start(self, context: PolicyContext) -&gt; None:\n        \"\"\"Called when an episode starts.\"\"\"\n        pass\n\n    def on_episode_end(self, context: PolicyContext, total_reward: float) -&gt; None:\n        \"\"\"Called when an episode ends.\"\"\"\n        pass\n</code></pre> Method Description <code>calculate(action, result, context)</code> Required. Compute reward from action, its result, and current context <code>on_episode_start(context)</code> Optional hook called at the beginning of an execution episode <code>on_episode_end(context, total_reward)</code> Optional hook called at the end of an execution episode"},{"location":"policies/reward/#rewardsignal","title":"RewardSignal","text":"<p>The <code>RewardSignal</code> dataclass is the return type of every reward calculation:</p> <pre><code>@dataclass\nclass RewardSignal:\n    \"\"\"Reward signal from a reward policy.\"\"\"\n\n    value: float                                      # Scalar reward value\n    components: dict[str, float] = field(default_factory=dict)  # Named component breakdown\n    explanation: str = \"\"                              # Human-readable explanation\n\n    @property\n    def clamped(self) -&gt; float:\n        \"\"\"Return value clamped to [-1, 1].\"\"\"\n        return max(-1.0, min(1.0, self.value))\n</code></pre> Field Type Description <code>value</code> <code>float</code> The total reward value. All built-in policies clamp this to [-1, 1] <code>components</code> <code>dict[str, float]</code> Named breakdown of reward components (e.g., <code>{\"success\": 0.7, \"error\": -0.1}</code>) <code>explanation</code> <code>str</code> Human-readable explanation of the reward computation <p>Component breakdowns</p> <p>The <code>components</code> dictionary is invaluable for debugging and research. It lets you see exactly which factors contributed to the final reward value and by how much. The <code>ResearchRewardPolicy</code> produces the most granular breakdowns.</p>"},{"location":"policies/reward/#actionresult","title":"ActionResult","text":"<p>The <code>ActionResult</code> dataclass represents the outcome of executing an action:</p> <pre><code>@dataclass\nclass ActionResult:\n    \"\"\"Result of an action execution.\"\"\"\n\n    action_type: str                               # Type of action (e.g., \"code\", \"final\")\n    success: bool                                  # Whether the action succeeded\n    output: str = \"\"                               # Stdout/output from the action\n    error: str | None = None                       # Error message if any\n    duration_ms: float = 0.0                       # Execution time in milliseconds\n    tokens_used: int = 0                           # Tokens consumed\n    metadata: dict[str, Any] = field(default_factory=dict)  # Additional metadata\n</code></pre>"},{"location":"policies/reward/#built-in-implementations","title":"Built-in Implementations","text":""},{"location":"policies/reward/#defaultrewardpolicy","title":"DefaultRewardPolicy","text":"<p>Registration name: <code>\"default\"</code></p> <p>A balanced reward policy suitable for general-purpose use. Provides moderate rewards for success and moderate penalties for failure, with bonuses for completing the task.</p> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\npolicy = PolicyRegistry.get_reward(\"default\")\n</code></pre>"},{"location":"policies/reward/#default-configuration","title":"Default Configuration","text":"Parameter Default Description <code>success_bonus</code> <code>0.7</code> Reward for a successful action <code>failure_penalty</code> <code>0.3</code> Penalty for a failed action <code>partial_success_base</code> <code>0.3</code> Base reward for partial success <code>stderr_penalty</code> <code>0.1</code> Penalty when stderr/error output is present <code>final_bonus</code> <code>0.5</code> Bonus for a successful final action"},{"location":"policies/reward/#reward-calculation","title":"Reward Calculation","text":"<p>The DefaultRewardPolicy computes reward as a sum of components:</p> <pre><code>total = base (0.1)\n      + success_bonus (if success)  OR  - failure_penalty (if failure)\n      - stderr_penalty (if error present)\n      + final_bonus (if final action AND success)\n</code></pre> <pre><code># Example: successful code execution\nsignal = policy.calculate(\n    action={\"action\": \"code\", \"code\": \"print(42)\"},\n    result=ActionResult(action_type=\"code\", success=True, output=\"42\"),\n    context=PolicyContext(task=\"compute answer\"),\n)\n# signal.value = 0.8  (0.1 base + 0.7 success)\n# signal.components = {\"base\": 0.1, \"success\": 0.7}\n\n# Example: failed action with error\nsignal = policy.calculate(\n    action={\"action\": \"code\", \"code\": \"1/0\"},\n    result=ActionResult(action_type=\"code\", success=False, error=\"ZeroDivisionError\"),\n    context=PolicyContext(task=\"compute answer\"),\n)\n# signal.value = -0.3  (0.1 base - 0.3 failure - 0.1 error)\n# signal.components = {\"base\": 0.1, \"failure\": -0.3, \"error\": -0.1}\n</code></pre>"},{"location":"policies/reward/#custom-configuration","title":"Custom Configuration","text":"<pre><code>policy = PolicyRegistry.get_reward(\"default\", config={\n    \"success_bonus\": 0.9,\n    \"failure_penalty\": 0.5,\n    \"final_bonus\": 0.8,\n})\n</code></pre>"},{"location":"policies/reward/#strictrewardpolicy","title":"StrictRewardPolicy","text":"<p>Registration name: <code>\"strict\"</code></p> <p>A reward policy with heavy penalties for errors, designed for production environments where correctness is critical and errors must be strongly discouraged. The final bonus is only awarded when the action succeeds and has no errors.</p> <pre><code>policy = PolicyRegistry.get_reward(\"strict\")\n</code></pre>"},{"location":"policies/reward/#default-configuration_1","title":"Default Configuration","text":"Parameter Default Description <code>success_bonus</code> <code>0.5</code> Reward for a successful action <code>failure_penalty</code> <code>0.6</code> Heavy penalty for failure <code>error_penalty</code> <code>0.3</code> Additional penalty when error output is present <code>timeout_penalty</code> <code>0.4</code> Extra penalty for timeout errors <code>final_bonus</code> <code>0.3</code> Bonus for successful final action (only if error-free)"},{"location":"policies/reward/#reward-calculation_1","title":"Reward Calculation","text":"<pre><code>total = + success_bonus (if success)  OR  - failure_penalty (if failure)\n        - error_penalty (if error present)\n        - timeout_penalty (if error contains \"timeout\")\n        + final_bonus (if final action AND success AND no errors)\n</code></pre> <p>Strict penalties stack</p> <p>A failed action with a timeout error receives both the <code>failure_penalty</code> and the <code>error_penalty</code> and the <code>timeout_penalty</code>, potentially reaching -1.0 (the minimum clamped value). This aggressive penalization is intentional for production-critical workloads.</p> <pre><code># Example: failed action with timeout\nsignal = policy.calculate(\n    action={\"action\": \"code\", \"code\": \"time.sleep(300)\"},\n    result=ActionResult(\n        action_type=\"code\",\n        success=False,\n        error=\"Execution timeout after 60s\",\n    ),\n    context=PolicyContext(task=\"compute answer\"),\n)\n# signal.value = -1.0  (clamped from -1.3: -0.6 failure - 0.3 error - 0.4 timeout)\n# signal.components = {\"failure\": -0.6, \"error\": -0.3, \"timeout\": -0.4}\n</code></pre>"},{"location":"policies/reward/#lenientrewardpolicy","title":"LenientRewardPolicy","text":"<p>Registration name: <code>\"lenient\"</code></p> <p>A forgiving reward policy that encourages exploration. Every action attempt is rewarded, failures receive only mild penalties, and producing substantial output earns a progress bonus. Ideal for research, experimentation, and early-stage development where exploration is more valuable than immediate correctness.</p> <pre><code>policy = PolicyRegistry.get_reward(\"lenient\")\n</code></pre>"},{"location":"policies/reward/#default-configuration_2","title":"Default Configuration","text":"Parameter Default Description <code>attempt_bonus</code> <code>0.2</code> Reward just for attempting any action <code>success_bonus</code> <code>0.5</code> Reward for a successful action <code>failure_penalty</code> <code>0.1</code> Mild penalty for failure <code>progress_bonus</code> <code>0.15</code> Bonus when output exceeds 50 characters (suggests learning) <code>final_bonus</code> <code>0.4</code> Bonus for any final action (success not required)"},{"location":"policies/reward/#reward-calculation_2","title":"Reward Calculation","text":"<pre><code>total = + attempt_bonus (always)\n        + success_bonus (if success)  OR  - failure_penalty (if failure)\n        + progress_bonus (if output length &gt; 50 chars)\n        + final_bonus (if final action, regardless of success)\n</code></pre> <p>Exploration-friendly design</p> <p>Unlike DefaultRewardPolicy and StrictRewardPolicy, the LenientRewardPolicy awards the <code>final_bonus</code> even when the final action fails. This encourages the agent to attempt answers rather than endlessly exploring. The <code>progress_bonus</code> rewards producing verbose output, which often correlates with the agent making progress on the task.</p> <pre><code># Example: failed action with substantial output\nsignal = policy.calculate(\n    action={\"action\": \"code\", \"code\": \"complex_analysis()\"},\n    result=ActionResult(\n        action_type=\"code\",\n        success=False,\n        output=\"Partial results: computed 3 of 5 components...\" * 3,\n    ),\n    context=PolicyContext(task=\"analyze data\"),\n)\n# signal.value = 0.25  (0.2 attempt - 0.1 failure + 0.15 progress)\n# signal.components = {\"attempt\": 0.2, \"failure\": -0.1, \"progress\": 0.15}\n</code></pre>"},{"location":"policies/reward/#researchrewardpolicy","title":"ResearchRewardPolicy","text":"<p>Registration name: <code>\"research\"</code></p> <p>A research-focused reward policy that produces granular, multi-dimensional reward breakdowns. Designed for reward function research, paper analysis, and detailed performance profiling. Tracks code quality, output quality, execution efficiency, step efficiency, and more.</p> <pre><code>policy = PolicyRegistry.get_reward(\"research\")\n</code></pre>"},{"location":"policies/reward/#default-configuration_3","title":"Default Configuration","text":"Parameter Default Description Base components <code>base_attempt</code> <code>0.05</code> Small reward for every attempt <code>base_success</code> <code>0.3</code> Reward for success <code>base_failure</code> <code>0.2</code> Penalty for failure Code quality <code>code_length_bonus_per_100_chars</code> <code>0.02</code> Bonus per 100 characters of code <code>code_length_cap</code> <code>0.1</code> Maximum code length bonus <code>code_complexity_penalty_per_nest</code> <code>0.01</code> Penalty per nesting level above 10 Output quality <code>output_length_bonus_per_100_chars</code> <code>0.01</code> Bonus per 100 characters of output <code>output_length_cap</code> <code>0.05</code> Maximum output length bonus <code>error_keyword_penalty</code> <code>0.05</code> Penalty for error keywords in output Efficiency <code>fast_execution_bonus</code> <code>0.05</code> Bonus for execution under 1 second <code>slow_execution_penalty</code> <code>0.05</code> Penalty for execution over 10 seconds Progress <code>step_penalty_per_step</code> <code>0.01</code> Penalty per step taken (encourages efficiency) <code>early_termination_bonus</code> <code>0.1</code> Bonus for finishing before half of max_steps Final <code>final_success_bonus</code> <code>0.3</code> Bonus for successful final action <code>final_failure_penalty</code> <code>0.1</code> Penalty for failed final action"},{"location":"policies/reward/#reward-calculation_3","title":"Reward Calculation","text":"<p>The ResearchRewardPolicy computes up to 12 distinct components:</p> <pre><code>total = base_attempt (always)\n      + base_success (if success)        OR  - base_failure (if failure)\n      + code_length (capped bonus based on code size)\n      - code_complexity (penalty if nesting &gt; 10)\n      + output_length (capped bonus based on output size)\n      - error_keyword (if output contains \"error\", \"exception\", \"traceback\", \"failed\")\n      + fast_execution (if duration &lt; 1s)  OR  - slow_execution (if duration &gt; 10s)\n      - step_penalty (proportional to current step number)\n      + final_success (if final and success)\n      + early_termination (if final and success and step &lt; max_steps/2)\n      OR - final_failure (if final and not success)\n</code></pre> <pre><code># Example: fast, successful code execution at step 2\nsignal = policy.calculate(\n    action={\"action\": \"code\", \"code\": \"result = sum(range(100))\"},\n    result=ActionResult(\n        action_type=\"code\",\n        success=True,\n        output=\"4950\",\n        duration_ms=50.0,\n    ),\n    context=PolicyContext(task=\"compute sum\", step=2, max_steps=10),\n)\n# signal.components might include:\n# {\n#     \"base_attempt\": 0.05,\n#     \"base_success\": 0.3,\n#     \"code_length\": 0.006,       # ~30 chars / 100 * 0.02\n#     \"output_length\": 0.0004,    # 4 chars / 100 * 0.01\n#     \"fast_execution\": 0.05,     # 50ms &lt; 1000ms\n#     \"step_penalty\": -0.02,      # step 2 * 0.01\n# }\n</code></pre> <p>Research applications</p> <p>The granular component breakdown is particularly useful for:</p> <ul> <li>Ablation studies: Disable individual components to measure their impact</li> <li>Reward shaping research: Adjust component weights to study learning dynamics</li> <li>Performance profiling: Identify which reward dimensions are driving agent behavior</li> <li>Paper reproduction: Match reward functions from published RLM research</li> </ul>"},{"location":"policies/reward/#custom-configuration-example","title":"Custom Configuration Example","text":"<pre><code># Emphasize code quality and efficiency\npolicy = PolicyRegistry.get_reward(\"research\", config={\n    \"code_length_bonus_per_100_chars\": 0.05,\n    \"code_length_cap\": 0.2,\n    \"fast_execution_bonus\": 0.15,\n    \"slow_execution_penalty\": 0.15,\n    \"step_penalty_per_step\": 0.03,\n})\n</code></pre>"},{"location":"policies/reward/#comparing-reward-policies","title":"Comparing Reward Policies","text":"<p>The following table summarizes the key behavioral differences:</p> Scenario Default Strict Lenient Research Successful action +0.8 +0.5 +0.7 ~+0.4 Failed action -0.2 -0.6 +0.1 ~-0.15 Failed with error -0.3 -0.9 +0.1 ~-0.2 Successful final +1.0 +0.8 +1.0 ~+0.7 Failed final -0.2 -0.6 +0.5 ~-0.25 <p>Approximate values</p> <p>The Research column shows approximate values because its calculations depend on additional factors like code length, output content, execution time, and step number. The values above assume a simple action at step 0 with no code or output.</p>"},{"location":"policies/reward/#when-to-use-each-policy","title":"When to Use Each Policy","text":"Policy Best For Default General-purpose tasks, balanced exploration/exploitation Strict Production systems, safety-critical code, CI/CD pipelines Lenient Research, experimentation, early-stage prototyping Research Reward function research, ablation studies, paper analysis"},{"location":"policies/reward/#creating-a-custom-reward-policy","title":"Creating a Custom Reward Policy","text":"<pre><code>from rlm_code.rlm.policies import (\n    RewardPolicy,\n    PolicyRegistry,\n    RewardSignal,\n    ActionResult,\n    PolicyContext,\n)\nfrom typing import Any\n\n\n@PolicyRegistry.register_reward(\"domain_specific\")\nclass DomainSpecificRewardPolicy(RewardPolicy):\n    \"\"\"Reward policy for a specific domain (e.g., SQL generation).\"\"\"\n\n    name = \"domain_specific\"\n    description = \"Rewards correct SQL query generation\"\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        return {\n            \"syntax_valid_bonus\": 0.3,\n            \"correct_result_bonus\": 0.6,\n            \"injection_penalty\": 0.9,\n            \"performance_bonus\": 0.1,\n        }\n\n    def calculate(\n        self,\n        action: dict[str, Any],\n        result: ActionResult,\n        context: PolicyContext,\n    ) -&gt; RewardSignal:\n        config = {**self.get_default_config(), **self.config}\n        components = {}\n        total = 0.0\n\n        code = action.get(\"code\", \"\")\n\n        # Check for SQL injection patterns\n        if any(kw in code.lower() for kw in [\"drop table\", \"delete from\", \"; --\"]):\n            components[\"injection\"] = -config[\"injection_penalty\"]\n            total -= config[\"injection_penalty\"]\n\n        # Reward valid SQL syntax\n        if result.success and not result.error:\n            components[\"syntax_valid\"] = config[\"syntax_valid_bonus\"]\n            total += config[\"syntax_valid_bonus\"]\n\n        # Reward correct results\n        expected = context.variables.get(\"expected_result\")\n        if expected and result.output.strip() == str(expected).strip():\n            components[\"correct_result\"] = config[\"correct_result_bonus\"]\n            total += config[\"correct_result_bonus\"]\n\n        # Performance bonus for fast queries\n        if result.duration_ms &lt; 100:\n            components[\"performance\"] = config[\"performance_bonus\"]\n            total += config[\"performance_bonus\"]\n\n        return RewardSignal(\n            value=max(-1.0, min(1.0, total)),\n            components=components,\n            explanation=f\"SQL reward: {total:.2f} with {len(components)} components\",\n        )\n\n\n# Use it\npolicy = PolicyRegistry.get_reward(\"domain_specific\")\n</code></pre>"},{"location":"policies/termination/","title":"Termination Policies","text":""},{"location":"policies/termination/#overview","title":"Overview","text":"<p>Termination policies determine when the agent should stop executing and return a final answer. Without a termination policy, the agent would run until it exhausts its step budget. Termination policies detect completion signals -- explicit patterns in output, reward thresholds, confidence levels, or combinations thereof -- and extract the agent's final answer.</p> <p>All termination policies inherit from <code>TerminationPolicy</code> and implement the <code>should_terminate()</code> method.</p>"},{"location":"policies/termination/#base-class","title":"Base Class","text":""},{"location":"policies/termination/#terminationpolicy","title":"TerminationPolicy","text":"<pre><code>class TerminationPolicy(Policy):\n    \"\"\"Policy for determining when to terminate execution.\"\"\"\n\n    name = \"termination_base\"\n    description = \"Base termination policy\"\n\n    @abstractmethod\n    def should_terminate(\n        self,\n        result: ActionResult,\n        context: PolicyContext,\n    ) -&gt; tuple[bool, str | None]:\n        \"\"\"\n        Check if execution should terminate.\n\n        Args:\n            result: Latest action result\n            context: Execution context\n\n        Returns:\n            Tuple of (should_terminate, final_answer_if_any)\n        \"\"\"\n        ...\n</code></pre> Method Return Type Description <code>should_terminate(result, context)</code> <code>(bool, str \\| None)</code> Returns whether to stop and the extracted final answer (or <code>None</code>) <p>The return value is a tuple:</p> <ul> <li>First element (<code>bool</code>): <code>True</code> if execution should stop, <code>False</code> to continue</li> <li>Second element (<code>str | None</code>): The extracted final answer if terminating, or <code>None</code></li> </ul>"},{"location":"policies/termination/#built-in-implementations","title":"Built-in Implementations","text":""},{"location":"policies/termination/#finalpatternterminationpolicy","title":"FinalPatternTerminationPolicy","text":"<p>Registration name: <code>\"final_pattern\"</code></p> <p>Detects <code>FINAL()</code> and <code>FINAL_VAR()</code> patterns in action output, matching the termination mechanism described in the RLM paper. This is the default termination policy. It uses configurable regex patterns to detect termination signals and extracts the final answer from the pattern match.</p> <pre><code>from rlm_code.rlm.policies import PolicyRegistry\n\npolicy = PolicyRegistry.get_termination(\"final_pattern\")\n</code></pre>"},{"location":"policies/termination/#default-configuration","title":"Default Configuration","text":"Parameter Default Description <code>final_patterns</code> (see below) List of regex patterns to detect termination <code>case_sensitive</code> <code>False</code> Whether pattern matching is case-sensitive <code>extract_answer</code> <code>True</code> Whether to extract the answer from the matched group <p>Default patterns:</p> <pre><code>[\n    r\"FINAL\\s*\\(\\s*['\\\"](.+?)['\\\"]\\s*\\)\",    # FINAL('answer') or FINAL(\"answer\")\n    r\"FINAL\\s*\\(\\s*(.+?)\\s*\\)\",              # FINAL(answer) without quotes\n    r\"FINAL_VAR\\s*\\(\\s*['\\\"](\\w+)['\\\"]\\s*\\)\", # FINAL_VAR('variable_name')\n]\n</code></pre>"},{"location":"policies/termination/#behavior","title":"Behavior","text":"<p>The policy checks for termination in two ways:</p> <ol> <li>Action type check: If <code>result.action_type == \"final\"</code>, terminates immediately with <code>result.output</code> as the answer.</li> <li>Pattern matching: Scans <code>result.output</code> against each pattern in <code>final_patterns</code>. On first match:<ul> <li>If <code>extract_answer</code> is True, extracts the captured group as the answer</li> <li>For <code>FINAL_VAR</code> patterns, looks up the variable name in <code>context.variables</code> and returns its value</li> <li>If <code>extract_answer</code> is False, returns the full output</li> </ul> </li> </ol> <pre><code>from rlm_code.rlm.policies.base import ActionResult, PolicyContext\n\n# Example: FINAL() in output\nresult = ActionResult(\n    action_type=\"code\",\n    success=True,\n    output=\"After computing, the answer is FINAL('42')\",\n)\ncontext = PolicyContext(task=\"What is 6*7?\")\n\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = True\n# answer = \"42\"\n</code></pre> <pre><code># Example: FINAL_VAR() with variable lookup\nresult = ActionResult(\n    action_type=\"code\",\n    success=True,\n    output=\"FINAL_VAR('result')\",\n)\ncontext = PolicyContext(\n    task=\"Compute the sum\",\n    variables={\"result\": 4950},\n)\n\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = True\n# answer = \"4950\"  (looked up from context.variables)\n</code></pre> <p>Paper compliance</p> <p>The <code>FINAL()</code> and <code>FINAL_VAR()</code> patterns are the termination mechanism specified in the RLM paper. Using this policy ensures your agent follows the standard RLM protocol. The patterns are designed to be unambiguous even when embedded in natural language output.</p>"},{"location":"policies/termination/#custom-pattern-configuration","title":"Custom Pattern Configuration","text":"<p>You can add domain-specific termination patterns:</p> <pre><code>policy = PolicyRegistry.get_termination(\"final_pattern\", config={\n    \"final_patterns\": [\n        r\"FINAL\\s*\\(\\s*['\\\"](.+?)['\\\"]\\s*\\)\",       # Standard FINAL()\n        r\"ANSWER:\\s*(.+?)$\",                          # Custom: \"ANSWER: 42\"\n        r\"SOLUTION\\s*=\\s*(.+?)$\",                     # Custom: \"SOLUTION = 42\"\n    ],\n    \"case_sensitive\": False,\n    \"extract_answer\": True,\n})\n</code></pre>"},{"location":"policies/termination/#rewardthresholdterminationpolicy","title":"RewardThresholdTerminationPolicy","text":"<p>Registration name: <code>\"reward_threshold\"</code></p> <p>Terminates when the cumulative reward reaches a configurable threshold, or when the agent hits a streak of consecutive negative rewards. This policy is useful for optimization-focused tasks where you want the agent to stop once it has achieved a \"good enough\" result.</p> <pre><code>policy = PolicyRegistry.get_termination(\"reward_threshold\", config={\n    \"min_reward_threshold\": 0.9,\n})\n</code></pre>"},{"location":"policies/termination/#default-configuration_1","title":"Default Configuration","text":"Parameter Default Description <code>min_reward_threshold</code> <code>0.8</code> Cumulative reward threshold that triggers termination <code>max_negative_streak</code> <code>3</code> Stop after this many consecutive negative rewards <code>require_final_action</code> <code>False</code> If True, only terminate on final-type actions"},{"location":"policies/termination/#behavior_1","title":"Behavior","text":"<p>The policy maintains two internal state variables:</p> <ul> <li><code>_cumulative_reward</code>: Running sum of rewards from <code>context.metrics[\"last_reward\"]</code></li> <li><code>_negative_streak</code>: Count of consecutive steps with negative reward</li> </ul> <p>Termination triggers in two cases:</p> <ol> <li>Reward threshold reached: <code>_cumulative_reward &gt;= min_reward_threshold</code><ul> <li>If <code>require_final_action</code> is True, only terminates when <code>result.action_type == \"final\"</code></li> <li>Answer is the result output or a summary string</li> </ul> </li> <li>Negative streak: <code>_negative_streak &gt;= max_negative_streak</code><ul> <li>Terminates with a message indicating consecutive failures</li> </ul> </li> </ol> <pre><code>from rlm_code.rlm.policies.base import ActionResult, PolicyContext\n\n# Step 1: reward = 0.4\ncontext = PolicyContext(metrics={\"last_reward\": 0.4})\nresult = ActionResult(action_type=\"code\", success=True, output=\"progress...\")\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = False (cumulative = 0.4, threshold = 0.8)\n\n# Step 2: reward = 0.5\ncontext = PolicyContext(metrics={\"last_reward\": 0.5})\nresult = ActionResult(action_type=\"code\", success=True, output=\"more progress...\")\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = True (cumulative = 0.9, threshold = 0.8)\n# answer = \"Reward threshold reached: 0.90\"\n</code></pre> <p>Call <code>reset()</code> to clear cumulative state between episodes:</p> <pre><code>policy.reset()\n</code></pre> <p>Stateful policy</p> <p>The RewardThresholdTerminationPolicy accumulates reward across calls. You must call <code>reset()</code> between independent episodes to avoid carrying over state. The cumulative reward is read from <code>context.metrics[\"last_reward\"]</code> -- ensure your reward policy populates this field.</p> <p>Require final action</p> <p>Setting <code>require_final_action=True</code> prevents premature termination when the cumulative reward threshold is reached mid-computation. The agent will continue until it explicitly produces a final-type action, giving it the opportunity to formulate a proper answer.</p>"},{"location":"policies/termination/#confidenceterminationpolicy","title":"ConfidenceTerminationPolicy","text":"<p>Registration name: <code>\"confidence\"</code></p> <p>Terminates when the model's self-reported confidence in its answer exceeds a threshold. Confidence is read from <code>result.metadata</code>. This policy also enforces a minimum number of steps before allowing termination, preventing the agent from short-circuiting on the first step.</p> <pre><code>policy = PolicyRegistry.get_termination(\"confidence\", config={\n    \"confidence_threshold\": 0.95,\n    \"min_steps_before_termination\": 3,\n})\n</code></pre>"},{"location":"policies/termination/#default-configuration_2","title":"Default Configuration","text":"Parameter Default Description <code>confidence_threshold</code> <code>0.85</code> Minimum confidence level to trigger termination <code>min_steps_before_termination</code> <code>2</code> Minimum steps before termination is allowed <code>confidence_key</code> <code>\"confidence\"</code> Key to read from <code>result.metadata</code> <code>fallback_to_final_pattern</code> <code>True</code> Fall back to FinalPatternTerminationPolicy if confidence is below threshold"},{"location":"policies/termination/#behavior_2","title":"Behavior","text":"<ol> <li>Minimum steps check: If <code>context.step &lt; min_steps_before_termination</code>, always returns <code>(False, None)</code></li> <li>Confidence check: Reads <code>result.metadata[confidence_key]</code>. If it meets the threshold, terminates with <code>result.output</code></li> <li>Fallback (optional): If <code>fallback_to_final_pattern</code> is True and confidence is below threshold, delegates to <code>FinalPatternTerminationPolicy</code> to check for explicit <code>FINAL()</code> patterns</li> </ol> <pre><code>from rlm_code.rlm.policies.base import ActionResult, PolicyContext\n\n# Step 0: blocked by min_steps\nresult = ActionResult(\n    action_type=\"code\",\n    success=True,\n    output=\"42\",\n    metadata={\"confidence\": 0.99},\n)\ncontext = PolicyContext(step=0)\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = False (step 0 &lt; min_steps 2)\n\n# Step 3: confidence met\ncontext = PolicyContext(step=3)\nshould_stop, answer = policy.should_terminate(result, context)\n# should_stop = True\n# answer = \"42\"\n\n# Step 3: low confidence, but FINAL() in output (fallback)\nresult2 = ActionResult(\n    action_type=\"code\",\n    success=True,\n    output=\"I think it might be FINAL('42')\",\n    metadata={\"confidence\": 0.4},\n)\nshould_stop, answer = policy.should_terminate(result2, PolicyContext(step=3))\n# should_stop = True (via fallback to FinalPatternTerminationPolicy)\n# answer = \"42\"\n</code></pre> <p>Populating confidence</p> <p>The confidence value must be placed in <code>result.metadata</code> by the execution engine or the model itself. Common approaches include:</p> <ul> <li>Having the model output a confidence score alongside its answer</li> <li>Computing confidence from token probabilities (logprobs)</li> <li>Using an ensemble of models and measuring agreement</li> </ul>"},{"location":"policies/termination/#compositeterminationpolicy","title":"CompositeTerminationPolicy","text":"<p>Registration name: <code>\"composite\"</code></p> <p>Combines multiple termination policies using either OR logic (terminate when any sub-policy triggers) or AND logic (terminate only when all sub-policies agree). This enables sophisticated termination conditions without writing custom code.</p> <pre><code>policy = PolicyRegistry.get_termination(\"composite\", config={\n    \"policies\": [\"final_pattern\", \"reward_threshold\"],\n    \"require_all\": False,  # OR logic\n})\n</code></pre>"},{"location":"policies/termination/#default-configuration_3","title":"Default Configuration","text":"Parameter Default Description <code>policies</code> <code>[\"final_pattern\", \"reward_threshold\"]</code> List of termination policy names to combine <code>require_all</code> <code>False</code> <code>False</code> = OR logic (any triggers), <code>True</code> = AND logic (all must agree)"},{"location":"policies/termination/#behavior_3","title":"Behavior","text":"<ul> <li>OR mode (<code>require_all=False</code>): Iterates through sub-policies and terminates on the first one that triggers. Returns that sub-policy's answer.</li> <li>AND mode (<code>require_all=True</code>): All sub-policies must agree to terminate. Returns the first non-None answer from the agreeing policies.</li> </ul> <pre><code># OR mode: stop on FINAL() or high reward\npolicy = PolicyRegistry.get_termination(\"composite\", config={\n    \"policies\": [\"final_pattern\", \"reward_threshold\"],\n    \"require_all\": False,\n})\n\n# AND mode: only stop when both confidence and FINAL() agree\npolicy = PolicyRegistry.get_termination(\"composite\", config={\n    \"policies\": [\"confidence\", \"final_pattern\"],\n    \"require_all\": True,\n})\n</code></pre> <p>Use cases for composite termination</p> Combination Mode Use Case <code>final_pattern</code> + <code>reward_threshold</code> OR Standard RLM with safety net for stuck agents <code>confidence</code> + <code>final_pattern</code> AND High-reliability tasks requiring both explicit answer and high confidence <code>final_pattern</code> + <code>confidence</code> + <code>reward_threshold</code> OR Maximum flexibility -- any signal can trigger termination"},{"location":"policies/termination/#comparison","title":"Comparison","text":"Policy Stateful Signal Source Answer Extraction Best For FinalPattern No Output patterns Regex capture groups Standard RLM, paper compliance RewardThreshold Yes Cumulative reward Output or threshold message Optimization tasks Confidence No Metadata confidence Output Model-aware termination Composite Depends Multiple sources First non-None answer Complex termination logic"},{"location":"policies/termination/#decision-guide","title":"Decision Guide","text":"<pre><code>Do you use FINAL() patterns?\n  YES --&gt; FinalPattern (default, paper-compliant)\n  NO  --&gt; Is reward-based stopping appropriate?\n            YES --&gt; RewardThreshold\n            NO  --&gt; Does your model report confidence?\n                      YES --&gt; Confidence\n                      NO  --&gt; Composite (combine multiple signals)\n\nNeed multiple termination conditions?\n  --&gt; Composite with OR mode (safety net)\n  --&gt; Composite with AND mode (high reliability)\n</code></pre>"},{"location":"policies/termination/#creating-a-custom-termination-policy","title":"Creating a Custom Termination Policy","text":"<pre><code>from rlm_code.rlm.policies import (\n    TerminationPolicy,\n    PolicyRegistry,\n    ActionResult,\n    PolicyContext,\n)\nfrom typing import Any\n\n\n@PolicyRegistry.register_termination(\"convergence\")\nclass ConvergenceTerminationPolicy(TerminationPolicy):\n    \"\"\"\n    Terminate when the agent's outputs converge (stop changing).\n    Detects when the last N outputs are similar, indicating\n    the agent has reached a stable answer.\n    \"\"\"\n\n    name = \"convergence\"\n    description = \"Stop when outputs converge (stabilize)\"\n\n    @classmethod\n    def get_default_config(cls) -&gt; dict[str, Any]:\n        return {\n            \"window_size\": 3,\n            \"similarity_threshold\": 0.9,\n            \"min_steps\": 3,\n        }\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        self._recent_outputs: list[str] = []\n\n    def should_terminate(self, result, context):\n        config = {**self.get_default_config(), **self.config}\n\n        # Track outputs\n        self._recent_outputs.append(result.output or \"\")\n\n        # Minimum steps\n        if context.step &lt; config[\"min_steps\"]:\n            return False, None\n\n        # Check last N outputs for convergence\n        window = self._recent_outputs[-config[\"window_size\"]:]\n        if len(window) &lt; config[\"window_size\"]:\n            return False, None\n\n        # Simple convergence: check if all outputs are the same\n        if len(set(window)) == 1:\n            return True, window[-1]\n\n        return False, None\n\n    def reset(self):\n        self._recent_outputs = []\n\n\n# Use it\npolicy = PolicyRegistry.get_termination(\"convergence\", config={\n    \"window_size\": 4,\n    \"min_steps\": 5,\n})\n</code></pre>"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#module-index","title":"Module Index","text":"<p>RLM Code is organized into the following top-level packages. Each module is documented in its corresponding section of this documentation.</p>"},{"location":"reference/#core-engine","title":"Core Engine","text":"Module Description Docs <code>rlm_code.rlm.runner</code> Multi-paradigm RLM runner with benchmark, comparison, and chat capabilities RLM Runner <code>rlm_code.rlm.pure_rlm_environment</code> Pure RLM environment with context-as-variable execution Environments <code>rlm_code.rlm.events</code> Event bus with 27+ event types, collector, and subscriber system Event System <code>rlm_code.rlm.termination</code> FINAL/FINAL_VAR detection, code block extraction, answer formatting Termination Patterns <code>rlm_code.rlm.memory_compaction</code> LLM and deterministic memory compaction strategies Memory Compaction <code>rlm_code.rlm.repl_types</code> REPLVariable, REPLEntry, REPLHistory, REPLResult data types REPL Types <code>rlm_code.rlm.trajectory</code> Trajectory event logging, viewing, and comparison Trajectory Logging <code>rlm_code.rlm.comparison</code> Paradigm comparison engine (Pure RLM vs CodeAct vs Traditional) Paradigm Comparison"},{"location":"reference/#policy-lab","title":"Policy Lab","text":"Module Description Docs <code>rlm_code.rlm.policies</code> Policy registry with hot-swappable reward, action, compaction, and termination policies Policy Lab <code>rlm_code.rlm.policies.reward</code> Reward policies: default, strict, lenient, research Reward Policies <code>rlm_code.rlm.policies.action_selection</code> Action selection policies: greedy, sampling, beam search, MCTS Action Selection <code>rlm_code.rlm.policies.compaction</code> Compaction policies: LLM, deterministic, sliding window, hierarchical Compaction Policies <code>rlm_code.rlm.policies.termination</code> Termination policies: final pattern, reward threshold, confidence, composite Termination Policies"},{"location":"reference/#hitl-approval","title":"HITL &amp; Approval","text":"Module Description Docs <code>rlm_code.rlm.approval</code> Approval gates, risk assessment, handlers, and audit logging HITL &amp; Approval <code>rlm_code.rlm.approval.gate</code> ApprovalGate orchestrator, ApprovalRequest, ApprovalResponse, ApprovalStatus Approval Gates <code>rlm_code.rlm.approval.policy</code> RiskAssessor, RiskRule, RiskAssessment, ToolRiskLevel, ApprovalPolicy Risk Assessment <code>rlm_code.rlm.approval.handlers</code> Console, AutoApprove, AutoDeny, Callback, Conditional, Queue handlers Approval Gates <code>rlm_code.rlm.approval.audit</code> ApprovalAuditLog, AuditEntry for compliance and debugging Audit Logging"},{"location":"reference/#observability","title":"Observability","text":"Module Description Docs <code>rlm_code.rlm.observability</code> Central observability manager with multi-sink architecture Observability <code>rlm_code.rlm.observability_sinks</code> All 7 sink implementations and the sink protocol Sink Architecture"},{"location":"reference/#benchmarks-evaluation","title":"Benchmarks &amp; Evaluation","text":"Module Description Docs <code>rlm_code.rlm.benchmarks</code> 10 preset benchmark suites with 33+ test cases, custom pack loading Benchmarks <code>rlm_code.rlm.leaderboard</code> Multi-metric leaderboard with ranking, filtering, trending, and export Leaderboard <code>rlm_code.rlm.session_replay</code> Session recording, replay, checkpointing, and comparison Session Replay"},{"location":"reference/#sandbox-runtimes","title":"Sandbox Runtimes","text":"Module Description Docs <code>rlm_code.sandbox.runtimes</code> Runtime registry, factory, health detection, and doctor checks Sandbox Runtimes <code>rlm_code.sandbox.superbox</code> Policy-driven runtime resolution and fallback orchestration Sandbox Runtimes <code>rlm_code.sandbox.runtimes.local_runtime</code> Local (no-isolation) sandbox runtime Local Runtime <code>rlm_code.sandbox.runtimes.docker_runtime</code> Docker sandbox runtime with security policy enforcement Docker Runtime <code>rlm_code.sandbox.runtimes.cloud</code> Modal, E2B, Daytona, and Apple Container cloud runtimes Cloud Runtimes"},{"location":"reference/#user-interface","title":"\ud83d\udda5\ufe0f User Interface","text":"Module Description Docs <code>rlm_code.ui.tui_app</code> Unified TUI with 5 tabs (RLM, Files, Details, Shell, Research) \ud83d\udccb Tab Reference <code>rlm_code.rlm.research_tui.widgets</code> Reusable research widgets (MetricsPanel, SparklineChart, etc.) \ud83d\udd2c Research Tab"},{"location":"reference/#integrations","title":"Integrations","text":"Module Description Docs <code>rlm_code.models</code> 12+ LLM provider adapters with unified interface LLM Providers <code>rlm_code.mcp</code> MCP server exposing all capabilities via Model Context Protocol MCP Server <code>rlm_code.rlm.frameworks</code> Framework adapters for DSPy and other agent frameworks Framework Adapters <code>rlm_code.harness</code> Tool-using coding harness runner and registry (<code>/harness</code>) CLI Reference"},{"location":"reference/#configuration","title":"Configuration","text":"Module Description Docs <code>rlm_code.core.config</code> Primary project configuration schema (<code>rlm_config.yaml</code>) and manager Configuration <code>rlm_code.rlm.config_schema</code> RLMConfig, PureRLMConfig, SandboxConfig, and related schemas Configuration"},{"location":"reference/#key-data-classes","title":"Key Data Classes","text":"<p>For quick reference, here are the most commonly used data classes across the codebase:</p> Class Module Description <code>RLMRunResult</code> <code>runner</code> Complete result of an RLM run <code>RLMBenchmarkResult</code> <code>runner</code> Benchmark suite execution results <code>RLMEventData</code> <code>events</code> Event payload with 20+ fields <code>REPLVariable</code> <code>repl_types</code> Typed variable in the REPL context <code>REPLHistory</code> <code>repl_types</code> Immutable history of REPL interactions <code>FinalOutput</code> <code>termination</code> Terminal answer exception <code>CompactionResult</code> <code>memory_compaction</code> Result of a compaction operation <code>TrajectoryEvent</code> <code>trajectory</code> Single event in a trajectory log <code>ParadigmResult</code> <code>comparison</code> Result from a single paradigm run <code>ComparisonResult</code> <code>comparison</code> Multi-paradigm comparison output <code>RiskAssessment</code> <code>approval.policy</code> Risk evaluation result <code>ApprovalRequest</code> <code>approval.gate</code> Request for action approval <code>ApprovalResponse</code> <code>approval.gate</code> Approval decision <code>AuditEntry</code> <code>approval.audit</code> Single audit log record <code>LeaderboardEntry</code> <code>leaderboard</code> Single leaderboard row with metrics <code>SessionSnapshot</code> <code>session_replay</code> Complete session state at a point in time <code>RLMRewardProfile</code> <code>pure_rlm_environment</code> 17-knob reward configuration <code>PolicyContext</code> <code>policies</code> Context passed to all policy decisions"},{"location":"sandbox/","title":"Sandbox Runtimes","text":"<p>RLM Code executes generated code through a sandbox layer with pluggable runtimes. The runtime selector is Superbox (<code>rlm_code.sandbox.superbox.Superbox</code>), which applies policy-based selection and fallback.</p>"},{"location":"sandbox/#runtime-architecture","title":"Runtime Architecture","text":"<pre><code>graph TD\n    Exec[ExecutionSandbox.execute] --&gt; SB[Superbox.resolve_runtime]\n    SB --&gt; R1[local]\n    SB --&gt; R2[docker]\n    SB --&gt; R3[apple-container]\n    SB --&gt; R4[modal]\n    SB --&gt; R5[e2b]\n    SB --&gt; R6[daytona]</code></pre> <p>Execution flow:</p> <ol> <li><code>ExecutionSandbox</code> builds request (<code>code_file</code>, timeout, workdir, env).</li> <li><code>Superbox</code> resolves runtime from configured runtime + fallback policy.</li> <li>Runtime executes code and returns normalized result.</li> <li>Runtime policy checks enforce mount and docker flag restrictions.</li> </ol>"},{"location":"sandbox/#supported-runtime-ids","title":"Supported Runtime IDs","text":"Runtime Isolation Notes <code>local</code> none Fastest; development only <code>monty</code> in-process sandbox Sandboxed Rust interpreter via <code>pydantic-monty</code>; Python subset <code>docker</code> container Recommended default for secure local execution <code>apple-container</code> container macOS-only, behind enable gate <code>modal</code> remote Requires Modal SDK/auth <code>e2b</code> remote Requires E2B SDK/auth <code>daytona</code> remote workspace Requires Daytona CLI/SDK <p><code>SUPPORTED_RUNTIMES</code> is defined in <code>rlm_code.sandbox.runtimes.registry</code>.</p>"},{"location":"sandbox/#superbox-profiles","title":"Superbox Profiles","text":"<p>Use <code>/sandbox profile</code> to apply runtime policy presets:</p> Profile Runtime Fallback order Pure RLM defaults <code>secure</code> <code>docker</code> <code>docker -&gt; daytona -&gt; e2b</code> <code>backend=docker</code>, strict on, unsafe exec off <code>dev</code> <code>docker</code> <code>docker -&gt; apple-container -&gt; local</code> <code>backend=docker</code>, strict off <code>custom</code> unchanged user-defined user-defined <p>Manual runtime/backend changes automatically mark profile as <code>custom</code>.</p>"},{"location":"sandbox/#sandbox-commands","title":"<code>/sandbox</code> Commands","text":"<pre><code>/sandbox status\n/sandbox doctor\n/sandbox use &lt;runtime&gt;\n/sandbox profile &lt;secure|dev|custom&gt;\n/sandbox backend &lt;exec|monty|docker&gt; [ack=I_UNDERSTAND_EXEC_IS_UNSAFE]\n/sandbox strict &lt;on|off&gt;\n/sandbox output-mode &lt;truncate|summarize|metadata&gt;\n/sandbox apple &lt;on|off&gt;\n</code></pre>"},{"location":"sandbox/#recommended-setup","title":"Recommended setup","text":"<pre><code>/sandbox profile secure\n/sandbox status\n/rlm doctor env=pure_rlm\n</code></pre>"},{"location":"sandbox/#unsafe-exec-opt-in","title":"Unsafe exec opt-in","text":"<p><code>/sandbox backend exec</code> is blocked unless you pass:</p> <pre><code>ack=I_UNDERSTAND_EXEC_IS_UNSAFE\n</code></pre>"},{"location":"sandbox/#config-keys-rlm_configyaml","title":"Config Keys (<code>rlm_config.yaml</code>)","text":"<pre><code>sandbox:\n  runtime: docker\n  superbox_profile: secure\n  superbox_auto_fallback: true\n  superbox_fallback_runtimes: [docker, daytona, e2b]\n\n  pure_rlm_backend: docker\n  pure_rlm_allow_unsafe_exec: false\n  pure_rlm_strict: true\n  pure_rlm_output_mode: summarize\n\n  default_timeout_seconds: 30\n  memory_limit_mb: 512\n  allowed_mount_roots: [\".\", \"/tmp\", \"/var/folders\"]\n  env_allowlist: []\n\n  docker:\n    image: python:3.11-slim\n    network_enabled: false\n    extra_args: []\n\n  apple_container_enabled: false\n</code></pre>"},{"location":"sandbox/#health-and-doctor","title":"Health and Doctor","text":""},{"location":"sandbox/#detect_runtime_health","title":"<code>detect_runtime_health()</code>","text":"<p>Returns <code>dict[str, RuntimeHealth]</code> for all runtime IDs.</p>"},{"location":"sandbox/#run_runtime_doctor","title":"<code>run_runtime_doctor()</code>","text":"<p>Performs deeper checks used by <code>/sandbox doctor</code>:</p> <ul> <li>runtime validity</li> <li>Docker CLI/daemon/image status</li> <li>mount policy safety</li> <li>env allowlist hygiene</li> <li>blocked docker flags</li> <li>temporary directory writeability</li> </ul>"},{"location":"sandbox/#docker-flag-safety","title":"Docker Flag Safety","text":"<p>Blocked flags include:</p> <ul> <li><code>--privileged</code></li> <li><code>--pid=host</code></li> <li><code>--network=host</code></li> <li><code>--ipc=host</code></li> <li><code>--uts=host</code></li> <li><code>--cap-add=ALL</code></li> <li><code>--volume</code>, <code>-v</code>, <code>--mount</code></li> </ul> <p>If configured in <code>sandbox.docker.extra_args</code>, runtime creation fails with <code>ConfigurationError</code>.</p>"},{"location":"sandbox/#monty-and-pure-rlm-backend","title":"Monty and Pure RLM Backend","text":"<p>Monty can now be used in both places:</p> <ul> <li>Superbox runtime via <code>/sandbox use monty</code></li> <li>Pure RLM interpreter backend via <code>/sandbox backend monty</code></li> </ul> <p>Use Monty when you want secure in-process execution without Docker.</p>"},{"location":"sandbox/#next","title":"Next","text":"<ul> <li>Docker Runtime</li> <li>Local Runtime</li> <li>Monty Interpreter</li> <li>Cloud Runtimes</li> </ul>"},{"location":"sandbox/cloud/","title":"Cloud Runtimes","text":"<p>RLM Code supports four non-local runtime backends beyond Docker: three cloud services (Modal, E2B, Daytona) and one macOS-native option (Apple Container). Each is an optional dependency that is loaded lazily when selected.</p>"},{"location":"sandbox/cloud/#backend-comparison","title":"Backend Comparison","text":"Backend Isolation Model Startup Latency Scalability Best For Modal Serverless VM ~2-5 s Excellent Scalable parallel runs E2B Cloud sandbox ~1-3 s Good Strong isolation, AI agent tasks Daytona Dev workspace ~5-15 s Moderate Persistent dev environments Apple Container macOS container ~1-3 s Single host macOS-native lightweight sandboxing"},{"location":"sandbox/cloud/#modal-serverless-compute","title":"Modal (Serverless Compute)","text":"<p>Modal provides fully isolated, serverless execution on Modal's cloud infrastructure. Code runs in ephemeral VMs with configurable CPU, memory, and custom Python environments.</p>"},{"location":"sandbox/cloud/#module","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.cloud.modal_runtime\n</code></pre>"},{"location":"sandbox/cloud/#classes","title":"Classes","text":"<pre><code>@dataclass\nclass ModalConfig:\n    timeout: int = 300            # Max execution time in seconds\n    memory_mb: int = 2048         # VM memory allocation\n    cpu: float = 1.0              # CPU allocation\n    image: str = \"python:3.11-slim\"\n    pip_packages: list[str] | None = None   # Extra pip packages\n    apt_packages: list[str] | None = None   # Extra apt packages\n\nclass ModalSandboxRuntime:\n    name = \"modal\"\n    def execute(self, request: RuntimeExecutionRequest) -&gt; RuntimeExecutionResult: ...\n    def set_lm_handler(self, handler: Any) -&gt; None: ...\n    def cleanup(self) -&gt; None: ...\n    @staticmethod\n    def check_health() -&gt; tuple[bool, str]: ...\n</code></pre>"},{"location":"sandbox/cloud/#setup","title":"Setup","text":"<pre><code># 1. Install the SDK\npip install modal\n\n# 2. Authenticate\nmodal setup\n\n# 3. Configure\n</code></pre> YAMLPython <pre><code>sandbox:\n  runtime: modal\n  modal:\n    timeout: 300\n    memory_mb: 2048\n    cpu: 1.0\n</code></pre> <pre><code>from rlm_code.sandbox.runtimes.registry import create_runtime\n\nruntime = create_runtime(\"modal\", sandbox_config=cfg)\n</code></pre>"},{"location":"sandbox/cloud/#how-it-works","title":"How It Works","text":"<ol> <li>The runtime lazily imports the <code>modal</code> SDK on first use.</li> <li>Builds a Modal <code>Image</code> with Debian Slim + optional pip/apt packages.</li> <li>Creates a Modal <code>App</code> and defines a <code>@app.function</code> that captures stdout/stderr.</li> <li>Uploads code and context, then calls <code>run_code.remote()</code>.</li> <li>Returns the result as a <code>RuntimeExecutionResult</code>.</li> </ol> <p>HTTP broker pattern</p> <p>For sub-query routing (e.g., LLM calls from inside the sandbox), Modal uses an embedded HTTP broker server. The <code>set_lm_handler()</code> method configures this routing.</p>"},{"location":"sandbox/cloud/#when-to-use-modal","title":"When to Use Modal","text":"<ul> <li>You need to run many evaluations or benchmarks in parallel.</li> <li>You want automatic scaling without managing infrastructure.</li> <li>Your code needs more memory or CPU than your local machine provides.</li> <li>You want reproducible environments via declarative image definitions.</li> </ul>"},{"location":"sandbox/cloud/#e2b-isolated-cloud-environments","title":"E2B (Isolated Cloud Environments)","text":"<p>E2B specializes in AI agent sandboxes with fast startup times and strong isolation. It uses a code interpreter model optimized for executing agent-generated code.</p>"},{"location":"sandbox/cloud/#module_1","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.cloud.e2b_runtime\n</code></pre>"},{"location":"sandbox/cloud/#classes_1","title":"Classes","text":"<pre><code>@dataclass\nclass E2BConfig:\n    timeout: int = 300            # Max execution time in seconds\n    template: str = \"Python3\"     # E2B template (or custom template ID)\n    api_key: str | None = None    # Falls back to E2B_API_KEY env var\n    cwd: str = \"/home/user\"       # Working directory inside sandbox\n\nclass E2BSandboxRuntime:\n    name = \"e2b\"\n    def execute(self, request: RuntimeExecutionRequest) -&gt; RuntimeExecutionResult: ...\n    def cleanup(self) -&gt; None: ...\n    @staticmethod\n    def check_health() -&gt; tuple[bool, str]: ...\n\nclass E2BPersistentSandbox:\n    \"\"\"Persistent sandbox for multi-turn interactions.\"\"\"\n    def start(self) -&gt; None: ...\n    def execute(self, code: str) -&gt; dict[str, Any]: ...\n    def stop(self) -&gt; None: ...\n</code></pre>"},{"location":"sandbox/cloud/#setup_1","title":"Setup","text":"<pre><code># 1. Install the SDK\npip install e2b-code-interpreter\n\n# 2. Set your API key\nexport E2B_API_KEY=your_api_key_here\n</code></pre> YAMLPython <pre><code>sandbox:\n  runtime: e2b\n  e2b:\n    timeout: 300\n    template: \"Python3\"\n</code></pre> <pre><code>from rlm_code.sandbox.runtimes.registry import create_runtime\n\nruntime = create_runtime(\"e2b\", sandbox_config=cfg)\n</code></pre>"},{"location":"sandbox/cloud/#how-it-works_1","title":"How It Works","text":"<ol> <li>The runtime lazily imports <code>e2b_code_interpreter.Sandbox</code>.</li> <li>Creates a sandbox instance with the configured template and API key.</li> <li>Optionally uploads context files (e.g., <code>.rlm_context.json</code>).</li> <li>Executes code via <code>sandbox.run_code()</code> and collects results.</li> <li>Returns stdout, stderr, and error information as a <code>RuntimeExecutionResult</code>.</li> </ol> <p>Persistent sandboxes</p> <p>Use <code>E2BPersistentSandbox</code> as a context manager for multi-turn interactions where state must persist across code executions: <pre><code>with E2BPersistentSandbox(config) as sandbox:\n    sandbox.execute(\"x = 42\")\n    result = sandbox.execute(\"print(x)\")  # prints 42\n</code></pre></p>"},{"location":"sandbox/cloud/#when-to-use-e2b","title":"When to Use E2B","text":"<ul> <li>You need the strongest isolation guarantees for untrusted code.</li> <li>You are building multi-turn agent interactions that need fast sandbox startup.</li> <li>You want a managed service with no Docker or VM infrastructure to maintain.</li> </ul>"},{"location":"sandbox/cloud/#daytona-development-environments","title":"Daytona (Development Environments)","text":"<p>Daytona specializes in reproducible, cloud-based development environments. It supports both a CLI and an SDK mode.</p>"},{"location":"sandbox/cloud/#module_2","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.cloud.daytona_runtime\n</code></pre>"},{"location":"sandbox/cloud/#classes_2","title":"Classes","text":"<pre><code>@dataclass\nclass DaytonaConfig:\n    workspace: str = \"default\"    # Workspace name\n    timeout: int = 300            # Max execution time in seconds\n    project: str | None = None    # Optional project reference\n    use_cli: bool = True          # True = CLI mode, False = SDK mode\n\nclass DaytonaSandboxRuntime:\n    name = \"daytona\"\n    def execute(self, request: RuntimeExecutionRequest) -&gt; RuntimeExecutionResult: ...\n    def cleanup(self) -&gt; None: ...\n    def start_workspace(self) -&gt; bool: ...\n    def stop_workspace(self) -&gt; bool: ...\n    @staticmethod\n    def check_health() -&gt; tuple[bool, str]: ...\n</code></pre>"},{"location":"sandbox/cloud/#setup_2","title":"Setup","text":"CLI mode (recommended)SDK modeYAML config <pre><code># Install Daytona CLI\n# See https://www.daytona.io/docs/installation\ncurl -sfL https://get.daytona.io | sh\n\n# Create a workspace\ndaytona workspace create default\n</code></pre> <pre><code>pip install daytona-sdk\n</code></pre> <pre><code>sandbox:\n  runtime: daytona\n  daytona:\n    workspace: \"default\"\n    timeout: 300\n</code></pre>"},{"location":"sandbox/cloud/#how-it-works_2","title":"How It Works","text":"<p>CLI mode (<code>use_cli: true</code>):</p> <ol> <li>Writes the code to a temporary file.</li> <li>Executes <code>daytona code exec &lt;workspace&gt; python &lt;file&gt;</code>.</li> <li>Captures stdout, stderr, and exit code.</li> <li>Cleans up the temporary file.</li> </ol> <p>SDK mode (<code>use_cli: false</code>):</p> <ol> <li>Imports <code>daytona_sdk.Daytona</code>.</li> <li>Gets or creates the named workspace.</li> <li>Runs the code via <code>workspace.run_command()</code>.</li> <li>Returns the result.</li> </ol>"},{"location":"sandbox/cloud/#workspace-management","title":"Workspace Management","text":"<p>Daytona workspaces persist by default, which makes them suitable for long-running development sessions:</p> <pre><code>runtime = DaytonaSandboxRuntime(config=DaytonaConfig(workspace=\"my-project\"))\n\n# Start workspace (idempotent)\nruntime.start_workspace()\n\n# Execute code (workspace stays alive between calls)\nresult = runtime.execute(request)\n\n# Stop workspace when done\nruntime.stop_workspace()\n</code></pre>"},{"location":"sandbox/cloud/#when-to-use-daytona","title":"When to Use Daytona","text":"<ul> <li>You want a persistent cloud development environment.</li> <li>Your workflow involves iterating on code over multiple sessions.</li> <li>You need workspace-level configuration (specific toolchains, packages).</li> </ul>"},{"location":"sandbox/cloud/#apple-container-runtime-macos","title":"Apple Container Runtime (macOS)","text":"<p>An experimental runtime for Apple's native container technology on macOS.</p>"},{"location":"sandbox/cloud/#module_3","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.apple_container_runtime\n</code></pre>"},{"location":"sandbox/cloud/#status","title":"Status","text":"<p>Experimental</p> <p>The Apple Container Runtime is currently a placeholder. The <code>execute()</code> method raises <code>ConfigurationError</code> with a message directing users to use Local or Docker instead. The <code>check_health()</code> method probes for the <code>container</code> CLI binary.</p>"},{"location":"sandbox/cloud/#health-check","title":"Health Check","text":"<pre><code>ok, detail = AppleContainerRuntime.check_health()\n# ok=True, detail=\"container v1.0.0\"   (if CLI is installed)\n# ok=False, detail=\"container CLI not found\"\n</code></pre>"},{"location":"sandbox/cloud/#configuration","title":"Configuration","text":"<pre><code>sandbox:\n  runtime: apple-container\n  apple_container_enabled: true  # Must be explicitly enabled\n</code></pre> <p>Gated by <code>apple_container_enabled</code></p> <p>Even when the CLI is available, you must set <code>sandbox.apple_container_enabled: true</code> in your config. This prevents accidental selection of an experimental runtime.</p>"},{"location":"sandbox/cloud/#cloud-runtime-health-checks","title":"Cloud Runtime Health Checks","text":"<p>All cloud runtimes participate in <code>detect_runtime_health()</code>:</p> <pre><code>from rlm_code.sandbox.runtimes.registry import detect_runtime_health\n\nhealth = detect_runtime_health()\n\nprint(health[\"modal\"])\n# RuntimeHealth(runtime='modal', available=True, detail='Modal SDK available (version 0.65.0)')\n\nprint(health[\"e2b\"])\n# RuntimeHealth(runtime='e2b', available=False, detail='E2B_API_KEY environment variable not set')\n\nprint(health[\"daytona\"])\n# RuntimeHealth(runtime='daytona', available=True, detail='Daytona CLI available (0.42.0)')\n</code></pre> <p>If the SDK is not installed, the health check reports the install command:</p> <pre><code>SDK not installed (pip install modal)\nSDK not installed (pip install e2b-code-interpreter)\nNot installed (pip install daytona-sdk or install CLI)\n</code></pre>"},{"location":"sandbox/cloud/#choosing-the-right-cloud-backend","title":"Choosing the Right Cloud Backend","text":"Requirement Recommended Backend Maximum parallel throughput Modal Strongest code isolation E2B Persistent workspace state Daytona No cloud dependency (macOS) Apple Container Cheapest for small runs Local or Docker Custom Python environment Modal or Docker"},{"location":"sandbox/docker/","title":"Docker Runtime","text":"<p>The Docker Runtime executes agent-generated code inside an ephemeral Docker container, providing process isolation, filesystem restrictions, configurable memory limits, and network policy controls.</p>"},{"location":"sandbox/docker/#module","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.docker_runtime\n</code></pre>"},{"location":"sandbox/docker/#class-dockersandboxruntime","title":"Class: <code>DockerSandboxRuntime</code>","text":"<pre><code>class DockerSandboxRuntime:\n    \"\"\"Executes code inside a Docker container.\"\"\"\n\n    name = \"docker\"\n\n    def __init__(\n        self,\n        image: str = \"python:3.11-slim\",\n        memory_limit_mb: int = 512,\n        cpus: float | None = 1.0,\n        network_enabled: bool = False,\n        extra_args: list[str] | None = None,\n    ):\n        ...\n\n    def execute(self, request: RuntimeExecutionRequest) -&gt; RuntimeExecutionResult:\n        ...\n\n    @staticmethod\n    def check_health(timeout_seconds: float = 2.5) -&gt; tuple[bool, str]:\n        ...\n\n    @staticmethod\n    def normalize_workdir(workdir: Path) -&gt; str:\n        ...\n</code></pre>"},{"location":"sandbox/docker/#how-it-works","title":"How It Works","text":"<p>For each <code>execute()</code> call, the runtime:</p> <ol> <li>Resolves the working directory to an absolute path.</li> <li>Builds a <code>docker run --rm</code> command with:<ul> <li>A bind mount of <code>workdir</code> to <code>/workspace</code> inside the container.</li> <li>Environment variables from <code>request.env</code> injected via <code>--env</code> flags.</li> <li>Network, memory, and CPU constraints applied.</li> <li>Any user-specified <code>extra_args</code> appended.</li> </ul> </li> <li>Runs the command with <code>subprocess.run()</code>, enforcing the configured timeout.</li> <li>Returns the container's exit code, stdout, and stderr as a    <code>RuntimeExecutionResult</code>.</li> </ol> <p>Ephemeral containers</p> <p>Every execution creates a fresh container (<code>--rm</code> flag). No state persists between steps unless the working directory is shared via bind mount.</p>"},{"location":"sandbox/docker/#configuration","title":"Configuration","text":"YAML configcreate_runtime()TUI command <pre><code>sandbox:\n  runtime: docker\n  docker:\n    image: \"python:3.11-slim\"\n    memory_limit_mb: 512\n    cpus: 1.0\n    network_enabled: false\n    extra_args: []\n</code></pre> <pre><code>from rlm_code.sandbox.runtimes.registry import create_runtime\n\nruntime = create_runtime(\"docker\", sandbox_config=cfg)\n</code></pre> <pre><code>/sandbox use docker\n</code></pre>"},{"location":"sandbox/docker/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>image</code> <code>str</code> <code>\"python:3.11-slim\"</code> Docker image to use for execution <code>memory_limit_mb</code> <code>int</code> <code>512</code> Container memory limit in MB (<code>--memory</code>) <code>cpus</code> <code>float</code> <code>1.0</code> CPU quota (<code>--cpus</code>) <code>network_enabled</code> <code>bool</code> <code>false</code> Whether to allow container networking <code>extra_args</code> <code>list[str]</code> <code>[]</code> Additional <code>docker run</code> arguments (policy-checked)"},{"location":"sandbox/docker/#docker-image-configuration","title":"Docker Image Configuration","text":"<p>Choose an image that matches the packages your agent code needs:</p> <pre><code># Minimal Python (fastest pull, smallest surface)\nsandbox:\n  docker:\n    image: \"python:3.11-slim\"\n\n# Full scientific Python stack\nsandbox:\n  docker:\n    image: \"python:3.11\"\n\n# Custom image with pre-installed packages\nsandbox:\n  docker:\n    image: \"myregistry/rlm-sandbox:latest\"\n</code></pre> <p>Pre-pull for speed</p> <p>The first execution pulls the image if it is not cached locally. Pre-pull with <code>docker pull python:3.11-slim</code> to avoid latency on the first run.</p>"},{"location":"sandbox/docker/#volume-mounts","title":"Volume Mounts","text":"<p>The runtime automatically mounts the working directory as a read-write bind mount:</p> <pre><code>host: &lt;workdir&gt;  --&gt;  container: /workspace:rw\n</code></pre> <p>The <code>allowed_mount_roots</code> configuration controls which host paths are permitted as bind-mount sources. By default, the project root (<code>.</code>) and <code>/tmp</code> are allowed.</p> <pre><code>sandbox:\n  allowed_mount_roots:\n    - \".\"\n    - \"/tmp\"\n</code></pre> <p>Explicit volume mounts are blocked</p> <p>The <code>--volume</code>, <code>-v</code>, and <code>--mount</code> flags in <code>extra_args</code> are blocked by the dangerous flag detector. Only the automatic workdir mount is permitted.</p>"},{"location":"sandbox/docker/#network-policy","title":"Network Policy","text":"<p>By default, container networking is disabled (<code>--network none</code>). This prevents agent-generated code from making outbound HTTP calls, exfiltrating data, or downloading arbitrary packages.</p> <pre><code># Enable networking (use with caution)\nsandbox:\n  docker:\n    network_enabled: true\n</code></pre> <p>Enable networking only when required</p> <p>Allowing network access means agent code can reach the internet, internal services, and cloud metadata endpoints. Only enable this when the task genuinely requires it.</p>"},{"location":"sandbox/docker/#memory-limits","title":"Memory Limits","text":"<p>The <code>memory_limit_mb</code> parameter sets a hard cap via Docker's <code>--memory</code> flag. If the container exceeds this limit, Docker kills it with an OOM signal.</p> <pre><code>sandbox:\n  docker:\n    memory_limit_mb: 1024  # 1 GB\n</code></pre>"},{"location":"sandbox/docker/#dangerous-flag-detection","title":"Dangerous Flag Detection","text":"<p>The registry maintains a blocklist of Docker flags that would weaken sandbox isolation. Both <code>create_runtime()</code> and <code>run_runtime_doctor()</code> enforce this policy.</p>"},{"location":"sandbox/docker/#blocked-flags","title":"Blocked Flags","text":"Flag Why It Is Blocked <code>--privileged</code> Grants full host device access to the container <code>--pid=host</code> Shares the host PID namespace <code>--network=host</code> Shares the host network stack (bypasses <code>--network</code>) <code>--ipc=host</code> Shares the host IPC namespace <code>--uts=host</code> Shares the host UTS namespace <code>--cap-add=ALL</code> Grants all Linux capabilities <code>--volume</code> / <code>-v</code> Arbitrary host mounts (use <code>allowed_mount_roots</code>) <code>--mount</code> Arbitrary mounts (use <code>allowed_mount_roots</code>) <p>Additionally, any argument starting with <code>--volume=</code> or <code>--mount=</code> is blocked.</p>"},{"location":"sandbox/docker/#what-happens-when-a-blocked-flag-is-detected","title":"What Happens When a Blocked Flag is Detected","text":"<pre><code>from rlm_code.sandbox.runtimes.registry import create_runtime\n\n# This raises ConfigurationError immediately:\ncreate_runtime(\"docker\", config_with_privileged)\n# ConfigurationError: Docker extra arg '--privileged' is blocked by sandbox policy.\n</code></pre> <p>Defence in depth</p> <p>The flag check runs at runtime creation time -- before any container is launched. Even if configuration is loaded from an untrusted source, the sandbox policy prevents privilege escalation.</p>"},{"location":"sandbox/docker/#health-check","title":"Health Check","text":"<p>The Docker Runtime provides a static <code>check_health()</code> method that probes the Docker daemon:</p> <pre><code>ok, detail = DockerSandboxRuntime.check_health()\n# ok=True, detail=\"docker daemon ready (server 24.0.7)\"\n</code></pre> <p>The check runs <code>docker info --format \"{{.ServerVersion}}\"</code> with a 2.5-second timeout and reports:</p> <ul> <li><code>docker CLI not found</code> -- Docker is not installed or not on PATH.</li> <li><code>docker check timed out</code> -- Daemon is unresponsive.</li> <li><code>docker daemon unavailable</code> -- Daemon returned an error.</li> <li><code>docker daemon ready (server X.Y.Z)</code> -- Ready to use.</li> </ul>"},{"location":"sandbox/docker/#setup","title":"Setup","text":""},{"location":"sandbox/docker/#1-install-docker","title":"1. Install Docker","text":"macOSLinuxWindows <pre><code>brew install --cask docker\n# Then open Docker Desktop\n</code></pre> <pre><code>curl -fsSL https://get.docker.com | sh\nsudo systemctl start docker\nsudo usermod -aG docker $USER\n</code></pre> <p>Download and install Docker Desktop.</p>"},{"location":"sandbox/docker/#2-pre-pull-the-image","title":"2. Pre-pull the Image","text":"<pre><code>docker pull python:3.11-slim\n</code></pre>"},{"location":"sandbox/docker/#3-configure-rlm-code","title":"3. Configure RLM Code","text":"<pre><code>sandbox:\n  runtime: docker\n  docker:\n    image: \"python:3.11-slim\"\n    memory_limit_mb: 512\n    network_enabled: false\n</code></pre>"},{"location":"sandbox/docker/#4-verify","title":"4. Verify","text":"<pre><code>/sandbox doctor\n</code></pre> <p>This runs <code>run_runtime_doctor()</code> and reports the status of every check:</p> <pre><code>[pass] configured_runtime: Runtime set to 'docker'.\n[pass] env_allowlist: 0 host env var(s) allowed.\n[pass] docker_cli: docker CLI found at /usr/local/bin/docker.\n[pass] docker_daemon: docker daemon ready (server 24.0.7)\n[pass] docker_image: Image 'python:3.11-slim' is available locally.\n[pass] docker_network_policy: Container networking is disabled.\n[pass] docker_extra_args: Docker extra args passed policy checks.\n[pass] mount_policy: Temp dir '/tmp' is allowed for bind mounts.\n[pass] temp_write_access: Writable temp directory: /tmp\n</code></pre>"},{"location":"sandbox/docker/#usage-example","title":"Usage Example","text":"<pre><code>from pathlib import Path\nfrom rlm_code.sandbox.runtimes.base import RuntimeExecutionRequest\nfrom rlm_code.sandbox.runtimes.docker_runtime import DockerSandboxRuntime\n\nruntime = DockerSandboxRuntime(\n    image=\"python:3.11-slim\",\n    memory_limit_mb=256,\n    network_enabled=False,\n)\n\nrequest = RuntimeExecutionRequest(\n    code_file=Path(\"/tmp/workspace/step.py\"),\n    workdir=Path(\"/tmp/workspace\"),\n    timeout_seconds=30,\n    python_executable=Path(\"python\"),  # ignored inside container\n    env={\"TASK_ID\": \"abc123\"},\n)\n\nresult = runtime.execute(request)\nprint(f\"Exit: {result.return_code}\")\nprint(f\"Stdout: {result.stdout}\")\nprint(f\"Stderr: {result.stderr}\")\n</code></pre>"},{"location":"sandbox/local/","title":"Local Runtime","text":"<p>The Local Runtime executes agent-generated code as a direct Python subprocess on the host machine. It is the simplest, fastest, and default sandbox backend.</p> <p>No isolation</p> <p>The Local Runtime provides no sandboxing. Code runs with the full privileges of the RLM Code process. Use it only for development and trusted workloads. For production or untrusted code, switch to Docker or a Cloud Runtime.</p>"},{"location":"sandbox/local/#module","title":"Module","text":"<pre><code>rlm_code.sandbox.runtimes.local_runtime\n</code></pre>"},{"location":"sandbox/local/#class-localsandboxruntime","title":"Class: <code>LocalSandboxRuntime</code>","text":"<pre><code>class LocalSandboxRuntime:\n    \"\"\"Executes code via local Python subprocess.\"\"\"\n\n    name = \"local\"\n\n    def execute(self, request: RuntimeExecutionRequest) -&gt; RuntimeExecutionResult:\n        ...\n</code></pre>"},{"location":"sandbox/local/#how-it-works","title":"How It Works","text":"<ol> <li>Receives a <code>RuntimeExecutionRequest</code> containing the code file path, working    directory, timeout, Python executable, and environment variables.</li> <li>Calls <code>subprocess.run()</code> with:<ul> <li>Command: <code>[python_executable, code_file]</code></li> <li>Capture: Both stdout and stderr</li> <li>Timeout: Enforced by <code>subprocess.run(timeout=...)</code></li> <li>CWD: Set to <code>request.workdir</code></li> <li>Env: Set to <code>request.env</code></li> <li>Check: <code>False</code> (non-zero exit does not raise)</li> </ul> </li> <li>Returns a <code>RuntimeExecutionResult</code> with the return code, stdout, and stderr.</li> </ol>"},{"location":"sandbox/local/#configuration","title":"Configuration","text":"<p>The Local Runtime requires no configuration. Selecting it is as simple as:</p> YAML configcreate_runtime()TUI command <pre><code>sandbox:\n  runtime: local\n</code></pre> <pre><code>from rlm_code.sandbox.runtimes.registry import create_runtime\n\nruntime = create_runtime(\"local\")\n</code></pre> <pre><code>/sandbox use local\n</code></pre>"},{"location":"sandbox/local/#configurable-timeout","title":"Configurable Timeout","text":"<p>The timeout is not a property of the runtime itself but of each <code>RuntimeExecutionRequest</code>. The caller (typically the RLM Runner) sets the timeout per step:</p> <pre><code>request = RuntimeExecutionRequest(\n    code_file=Path(\"/tmp/step.py\"),\n    workdir=Path.cwd(),\n    timeout_seconds=60,         # &lt;-- adjustable per request\n    python_executable=Path(\"python3\"),\n    env={\"PATH\": os.environ[\"PATH\"]},\n)\n</code></pre> <p>If execution exceeds <code>timeout_seconds</code>, Python's <code>subprocess.TimeoutExpired</code> exception propagates to the caller.</p>"},{"location":"sandbox/local/#use-case-development-and-testing","title":"Use Case: Development and Testing","text":"<p>The Local Runtime is ideal when:</p> <ul> <li>You are developing or debugging RLM policies locally.</li> <li>The code being executed is your own (trusted).</li> <li>You want the fastest possible execution with zero overhead.</li> <li>You need full access to the host filesystem and installed packages.</li> </ul> <p>Typical development workflow</p> <pre><code># Start RLM Code in local sandbox mode (default)\nrlm-code\n\n# In the TUI, run a task\n/rlm run \"Sort this list: [3, 1, 2]\"\n\n# The generated code runs directly on your machine\n</code></pre>"},{"location":"sandbox/local/#when-to-use-local-vs-docker","title":"When to Use Local vs Docker","text":"Consideration Local Docker Startup latency ~0 ms ~500-2000 ms (container start) Isolation None Full container isolation Host access Full filesystem + network Controlled mounts + network Reproducibility Depends on host env Pinned image = reproducible Package access Uses host Python packages Uses container image packages Security Trusts all generated code Blocks privilege escalation Recommended for Dev, testing, trusted code CI, benchmarks, untrusted code <p>Rule of thumb</p> <p>Use Local when you trust the code and want speed. Use Docker when you need isolation or reproducibility. Use Cloud when you need scale or strong multi-tenant isolation.</p>"},{"location":"sandbox/local/#health-check","title":"Health Check","text":"<p>The Local Runtime is always reported as available by <code>detect_runtime_health()</code>:</p> <pre><code>from rlm_code.sandbox.runtimes.registry import detect_runtime_health\n\nhealth = detect_runtime_health()\nprint(health[\"local\"])\n# RuntimeHealth(runtime='local', available=True, detail='always available')\n</code></pre> <p>There is no health probe because the runtime simply calls <code>subprocess.run()</code> with whatever Python interpreter the request specifies. If the interpreter does not exist, the error surfaces at execution time.</p>"},{"location":"sandbox/local/#limitations","title":"Limitations","text":"<ul> <li>No network restriction. Code can make arbitrary HTTP calls.</li> <li>No memory limit. A runaway process can consume all host memory.</li> <li>No filesystem restriction. Code can read/write anywhere the user can.</li> <li>No CPU limit. Only the timeout prevents infinite loops.</li> </ul> <p>For any scenario where these limitations matter, use the Docker Runtime or a Cloud Runtime.</p>"},{"location":"sandbox/monty/","title":"Monty Interpreter","text":"<p>The Monty Interpreter provides a sandboxed code execution backend using pydantic-monty, a minimal Python interpreter written in Rust by Pydantic. It is designed for executing LLM-generated code with strong isolation guarantees while preserving full compatibility with RLM's REPL loop.</p> <p>Experimental</p> <p>Monty is a new interpreter that supports a subset of Python. It cannot replace <code>LocalInterpreter</code> for all workloads, but provides a compelling option when sandbox safety, resource limits, and microsecond startup matter more than full Python compatibility.</p>"},{"location":"sandbox/monty/#module","title":"Module","text":"<pre><code>rlm_code.rlm.monty_interpreter\n  +-- MontyInterpreter       -- Sandboxed CodeInterpreter\n  +-- MontyCodeResult        -- Extended result with Monty metadata\n  +-- MontyCodeValidator     -- Standalone validation utility\n  +-- MontyExecutionStats    -- Aggregate session statistics\n  +-- create_rlm_monty_interpreter()  -- Factory for RLM-configured instances\n</code></pre>"},{"location":"sandbox/monty/#why-monty","title":"Why Monty?","text":"Feature LocalInterpreter (<code>exec()</code>) MontyInterpreter Sandbox safety None (full host access) No filesystem, network, imports, eval/exec Resource limits Timeout only Time, memory, allocation caps (Rust VM) Startup latency ~0 ms &lt;1 microsecond External fn dispatch Exception-based (FinalOutput) Coroutine-style <code>start()/resume()</code> Type checking None Optional static analysis via ty Snapshot serialization Not supported Freeze/resume execution state to bytes Python coverage Full CPython Subset (no imports, no classes, no stdlib)"},{"location":"sandbox/monty/#architecture","title":"Architecture","text":"<p>When the LLM emits a REPL code block, the execution flow is:</p> <pre><code>graph TD\n    LLM[\"LLM-generated code\"] --&gt; Execute[\"MontyInterpreter.execute()\"]\n    Execute --&gt; AST[\"AST parse: find referenced &amp; assigned vars\"]\n    AST --&gt; Augment[\"Append __rlm_collect__({...})\"]\n    Augment --&gt; Monty[\"pydantic_monty.Monty(augmented_code)\"]\n    Monty --&gt; Start[\"monty.start(inputs={...}, limits=...)\"]\n    Start --&gt; Check{Progress type?}\n    Check --&gt;|MontyComplete| Done[\"Execution finished\"]\n    Check --&gt;|MontySnapshot| Dispatch{\"External fn?\"}\n    Dispatch --&gt;|__rlm_collect__| Collect[\"Capture variables\"]\n    Dispatch --&gt;|FINAL / FINAL_VAR| Term[\"Terminate with answer\"]\n    Dispatch --&gt;|SUBMIT| Submit[\"Terminate with fields\"]\n    Dispatch --&gt;|SHOW_VARS| ShowVars[\"Return var listing\"]\n    Dispatch --&gt;|llm_query| LLMCall[\"Call host LLM\"]\n    Dispatch --&gt;|user tool| UserTool[\"Call registered handler\"]\n    Collect --&gt; Resume[\"snapshot.resume()\"]\n    ShowVars --&gt; Resume\n    LLMCall --&gt; Resume\n    UserTool --&gt; Resume\n    Resume --&gt; Check\n    Done --&gt; Update[\"Update persistent variables\"]\n    Term --&gt; Update\n    Submit --&gt; Update\n    Update --&gt; Result[\"Return MontyCodeResult\"]</code></pre>"},{"location":"sandbox/monty/#variable-persistence","title":"Variable Persistence","text":"<p>Monty has no persistent namespace across runs. To simulate REPL-style state across multiple code blocks, <code>MontyInterpreter</code> uses a three-part strategy:</p> <ol> <li>AST-parse the code to discover assigned and referenced variable names</li> <li>Inject known variables from previous steps via Monty's <code>inputs</code> mechanism</li> <li>Append <code>__rlm_collect__({...})</code> at the end of each code block to send    new/updated variables back to the host</li> </ol> <pre><code># Step 1: x = 10\n# MontyInterpreter discovers 'x' is assigned, collects it after execution\n\n# Step 2: y = x + 5\n# MontyInterpreter sees 'x' is referenced, injects it as an input\n# Discovers 'y' is assigned, collects both after execution\n</code></pre>"},{"location":"sandbox/monty/#external-function-dispatch","title":"External Function Dispatch","text":"<p>RLM tools (<code>llm_query</code>, <code>FINAL</code>, <code>FINAL_VAR</code>, <code>SUBMIT</code>, <code>SHOW_VARS</code>) are registered as Monty external functions. When code calls one of these, Monty pauses execution and yields a <code>MontySnapshot</code> to the host:</p> <pre><code>Code calls FINAL(\"42\")\n    -&gt; Monty pauses, yields MontySnapshot(function_name=\"FINAL\", args=[\"42\"])\n    -&gt; Host sees FINAL, captures answer, breaks the loop\n    -&gt; Execution terminates with final_output={\"answer\": \"42\", \"type\": \"direct\"}\n</code></pre> <p>This is cleaner than LocalInterpreter's exception-based dispatch (<code>FinalOutput</code>, <code>SubmitOutput</code>) because execution pauses cooperatively rather than unwinding the stack.</p>"},{"location":"sandbox/monty/#installation","title":"Installation","text":"<pre><code>pip install pydantic-monty\n</code></pre> <p>Or with the RLM Code extras:</p> <pre><code>pip install 'rlm-code[monty]'\n</code></pre>"},{"location":"sandbox/monty/#configuration","title":"Configuration","text":"YAML configPython <pre><code>rlm:\n  sandbox:\n    runtime: monty\n    timeout: 30\n\n    # Monty-specific settings\n    monty_type_check: false       # Enable pre-execution type checking\n    monty_max_allocations: null   # Max heap allocations (null = unlimited)\n    monty_max_memory: null        # Max heap memory in bytes (null = unlimited)\n</code></pre> <pre><code>from rlm_code.rlm import MontyInterpreter, create_rlm_monty_interpreter\n\n# Quick factory (recommended)\ninterp = create_rlm_monty_interpreter(\n    llm_query_fn=my_llm_query,\n    timeout=30,\n    max_memory=50_000_000,    # 50 MB\n    max_allocations=100_000,\n    type_check=True,\n)\n\n# Or manual construction\ninterp = MontyInterpreter(\n    timeout=30,\n    resource_limits={\n        \"max_duration_secs\": 30.0,\n        \"max_memory\": 50_000_000,\n    },\n    type_check=True,\n)\n</code></pre>"},{"location":"sandbox/monty/#usage","title":"Usage","text":""},{"location":"sandbox/monty/#basic-execution","title":"Basic Execution","text":"<pre><code>from rlm_code.rlm import MontyInterpreter\n\ninterp = MontyInterpreter()\ninterp.start()\n\nresult = interp.execute(\"x = 1 + 2\\nprint(x)\")\nprint(result.output)      # \"3\\n\"\nprint(result.variables)   # {\"x\": \"3\"}\nprint(result.error)       # None\n</code></pre>"},{"location":"sandbox/monty/#variable-persistence-across-steps","title":"Variable Persistence Across Steps","text":"<pre><code>interp = MontyInterpreter()\ninterp.start()\n\n# Step 1\ninterp.execute(\"a = 10\")\n\n# Step 2 -- 'a' is automatically injected\nresult = interp.execute(\"b = a * 2\\nprint(b)\")\nprint(result.output)      # \"20\\n\"\nprint(result.variables)   # {\"a\": \"10\", \"b\": \"20\"}\n</code></pre>"},{"location":"sandbox/monty/#external-functions-rlm-tools","title":"External Functions (RLM Tools)","text":"<pre><code>from rlm_code.rlm import create_rlm_monty_interpreter\n\ndef my_llm_query(prompt, model=None):\n    return \"The answer is 42\"\n\ninterp = create_rlm_monty_interpreter(llm_query_fn=my_llm_query)\n\n# Code calls llm_query() -- Monty pauses, host dispatches, resumes\nresult = interp.execute('answer = llm_query(\"What is 6*7?\")\\nprint(answer)')\nprint(result.output)  # \"The answer is 42\\n\"\n</code></pre>"},{"location":"sandbox/monty/#final-termination","title":"FINAL Termination","text":"<pre><code>interp = create_rlm_monty_interpreter()\n\nresult = interp.execute('FINAL(\"The answer is 42\")')\nprint(result.final_output)  # {\"answer\": \"The answer is 42\", \"type\": \"direct\"}\n</code></pre>"},{"location":"sandbox/monty/#submit-termination","title":"SUBMIT Termination","text":"<pre><code>result = interp.execute('SUBMIT(answer=\"42\", confidence=0.95)')\nprint(result.submit_fields)  # {\"answer\": \"42\", \"confidence\": 0.95}\n</code></pre>"},{"location":"sandbox/monty/#custom-external-functions","title":"Custom External Functions","text":"<pre><code>interp = MontyInterpreter()\ninterp.start()\n\n# Register a custom function\ninterp.register_external(\"fetch_data\", lambda key: {\"value\": key.upper()})\n\nresult = interp.execute('data = fetch_data(\"hello\")\\nprint(data)')\nprint(result.output)  # \"{'value': 'HELLO'}\\n\"\n</code></pre>"},{"location":"sandbox/monty/#checkpoint-and-restore","title":"Checkpoint and Restore","text":"<pre><code>interp = MontyInterpreter()\ninterp.start()\ninterp.execute(\"x = 42\")\n\n# Save state\ncheckpoint = interp.checkpoint()\n# {\"variables\": {\"x\": 42}, \"stats\": {...}}\n\n# Restore in a new interpreter (or different process)\ninterp2 = MontyInterpreter()\ninterp2.start()\ninterp2.restore(checkpoint)\nresult = interp2.execute(\"print(x)\")\nprint(result.output)  # \"42\\n\"\n</code></pre>"},{"location":"sandbox/monty/#code-validation","title":"Code Validation","text":"<p><code>MontyCodeValidator</code> uses Monty's Ruff-based parser and optional type checker to validate code before execution. This can be used as a pre-flight check even when using <code>LocalInterpreter</code> for actual execution.</p> <pre><code>from rlm_code.rlm import MontyCodeValidator\n\nvalidator = MontyCodeValidator(type_check=True)\n\n# Valid code\nok, err = validator.validate(\"x = 1 + 2\")\nassert ok is True\n\n# Syntax error\nok, err = validator.validate(\"x = \")\nassert ok is False\nprint(err)  # \"Syntax error: ...\"\n\n# With known variables\nok, err = validator.validate(\n    \"y = x + 1\",\n    known_vars={\"x\": int},\n    external_functions=[\"llm_query\"],\n)\n</code></pre>"},{"location":"sandbox/monty/#data-classes","title":"Data Classes","text":""},{"location":"sandbox/monty/#montycoderesult","title":"<code>MontyCodeResult</code>","text":"<p>Extended <code>CodeResult</code> with Monty-specific metadata.</p> Field Type Default Description <code>output</code> <code>str</code> -- Captured stdout <code>error</code> <code>str | None</code> <code>None</code> Error message if execution failed <code>variables</code> <code>dict[str, str]</code> <code>{}</code> Variable name -&gt; repr snapshot <code>final_output</code> <code>dict[str, Any] | None</code> <code>None</code> FINAL/FINAL_VAR result <code>submit_fields</code> <code>dict[str, Any] | None</code> <code>None</code> SUBMIT keyword arguments <code>type_errors</code> <code>str | None</code> <code>None</code> Type check warnings (if enabled) <code>resource_usage</code> <code>dict[str, Any]</code> <code>{}</code> Resource consumption data <code>execution_snapshots</code> <code>int</code> <code>0</code> Number of external fn pause/resume cycles"},{"location":"sandbox/monty/#montyexecutionstats","title":"<code>MontyExecutionStats</code>","text":"<p>Aggregate statistics across an interpreter session.</p> Field Type Default Description <code>total_executions</code> <code>int</code> <code>0</code> Total code blocks executed <code>total_external_calls</code> <code>int</code> <code>0</code> Total external function dispatches <code>total_time_secs</code> <code>float</code> <code>0.0</code> Cumulative execution time <code>type_check_failures</code> <code>int</code> <code>0</code> Type check failures (non-fatal) <code>syntax_errors</code> <code>int</code> <code>0</code> Syntax errors encountered <code>runtime_errors</code> <code>int</code> <code>0</code> Runtime errors encountered"},{"location":"sandbox/monty/#class-reference","title":"Class Reference","text":""},{"location":"sandbox/monty/#montyinterpreter","title":"<code>MontyInterpreter</code>","text":"Member Signature Description <code>start()</code> <code>() -&gt; None</code> Initialize interpreter session <code>shutdown()</code> <code>() -&gt; None</code> Clear state and release resources <code>execute()</code> <code>(code, variables=None) -&gt; MontyCodeResult</code> Execute code in sandbox <code>register_external()</code> <code>(name, handler) -&gt; None</code> Register a host-side external function <code>set_variable()</code> <code>(name, value) -&gt; None</code> Inject a variable <code>get_variable()</code> <code>(name) -&gt; Any</code> Retrieve a variable <code>validate_code()</code> <code>(code) -&gt; tuple[bool, str | None]</code> Validate without executing <code>checkpoint()</code> <code>() -&gt; dict[str, Any]</code> Serialize session state <code>restore()</code> <code>(state) -&gt; None</code> Restore from checkpoint <code>variables</code> <code>@property -&gt; dict[str, Any]</code> Read-only view of persistent variables <code>stats</code> <code>@property -&gt; MontyExecutionStats</code> Aggregate execution statistics <code>tools</code> <code>@property -&gt; list[Callable]</code> Registered user tools <code>namespace</code> <code>@property -&gt; dict[str, Any]</code> Compatibility alias for <code>variables</code>"},{"location":"sandbox/monty/#montycodevalidator","title":"<code>MontyCodeValidator</code>","text":"Member Signature Description <code>validate()</code> <code>(code, *, known_vars=None, external_functions=None) -&gt; tuple</code> Validate code syntax and types"},{"location":"sandbox/monty/#create_rlm_monty_interpreter","title":"<code>create_rlm_monty_interpreter()</code>","text":"<p>Factory function that creates a <code>MontyInterpreter</code> pre-configured with standard RLM external functions.</p> Parameter Type Default Description <code>llm_query_fn</code> <code>Callable | None</code> <code>None</code> Host-side <code>llm_query()</code> handler <code>llm_query_batched_fn</code> <code>Callable | None</code> <code>None</code> Host-side <code>llm_query_batched()</code> <code>tools</code> <code>list[Callable] | None</code> <code>None</code> Additional user tools <code>timeout</code> <code>int</code> <code>30</code> Max execution time per block (s) <code>max_memory</code> <code>int | None</code> <code>None</code> Max heap memory in bytes <code>max_allocations</code> <code>int | None</code> <code>None</code> Max heap allocations <code>type_check</code> <code>bool</code> <code>False</code> Enable pre-execution type checking <p>Pre-registered external functions: <code>FINAL</code>, <code>FINAL_VAR</code>, <code>SUBMIT</code>, <code>SHOW_VARS</code>, <code>llm_query</code> (if provided), <code>llm_query_batched</code> (if provided).</p>"},{"location":"sandbox/monty/#resource-limits","title":"Resource Limits","text":"<p>Monty enforces resource limits at the Rust VM level, providing hard guarantees that cannot be bypassed by the executed code.</p> <pre><code>from rlm_code.rlm import MontyInterpreter\n\ninterp = MontyInterpreter(\n    resource_limits={\n        \"max_duration_secs\": 5.0,       # 5 second timeout\n        \"max_memory\": 10_000_000,       # 10 MB heap\n        \"max_allocations\": 50_000,      # 50K allocations\n        \"max_recursion_depth\": 100,     # 100 frames\n    }\n)\n</code></pre> Limit Type Description <code>max_duration_secs</code> <code>float</code> Wall-clock timeout for execution <code>max_memory</code> <code>int</code> Maximum heap memory in bytes <code>max_allocations</code> <code>int</code> Maximum number of heap allocations <code>max_recursion_depth</code> <code>int</code> Maximum call stack depth <p>When any limit is exceeded, Monty raises <code>MontyRuntimeError</code> which is caught and returned in <code>MontyCodeResult.error</code>.</p>"},{"location":"sandbox/monty/#limitations","title":"Limitations","text":"<p>Monty is a subset Python interpreter. The following are not supported:</p> <ul> <li><code>import</code> statements (no stdlib, no third-party packages)</li> <li>Class definitions (<code>class Foo: ...</code>)</li> <li><code>match</code> statements</li> <li><code>eval()</code> / <code>exec()</code> / <code>compile()</code></li> <li>File I/O, network access, subprocess calls</li> <li>Decorators, metaclasses, descriptors</li> </ul> <p>These limitations are by design -- they are what make Monty safe for executing untrusted LLM-generated code.</p> <p>When to use Monty vs Local</p> <p>Use Monty when the LLM-generated code is primarily arithmetic, string manipulation, data transformation, and tool calls (the typical RLM pattern). Use Local when the code needs imports, classes, or full Python stdlib access.</p>"},{"location":"sandbox/monty/#comparison-with-localinterpreter","title":"Comparison with LocalInterpreter","text":"Aspect LocalInterpreter MontyInterpreter Implementation <code>exec()</code> with shared namespace Fresh <code>pydantic_monty.Monty</code> per step Variable persistence Native (shared dict) AST-based inject/collect cycle Termination handling Exception-based (<code>FinalOutput</code>) External function pause/resume Security model Trust the code Sandbox everything Error reporting Python tracebacks Monty-formatted errors Serialization Not supported <code>checkpoint()</code>/<code>restore()</code> Code validation Not available <code>validate_code()</code> via Ruff parser"},{"location":"sandbox/monty/#next-steps","title":"Next Steps","text":"<ul> <li>Local Runtime -- zero-config development sandbox</li> <li>Docker Runtime -- containerized isolation</li> <li>Framework Adapters -- DeepAgents, Pydantic AI, Google ADK</li> </ul>"},{"location":"security/","title":"Human-in-the-Loop &amp; Approval System","text":""},{"location":"security/#overview","title":"Overview","text":"<p>RLM Code agents execute arbitrary code in pursuit of their tasks. While sandboxing provides a first line of defense, some actions -- deleting files, making network requests, running privileged commands -- carry inherent risk that no sandbox can fully contain. The Human-in-the-Loop (HITL) and Approval System provides a safety layer that evaluates every agent action for risk, enforces approval policies, and maintains a complete audit trail of all decisions.</p> <p>This system answers a fundamental question: should the agent be allowed to do this?</p>"},{"location":"security/#why-safety-matters","title":"Why Safety Matters","text":"<p>Autonomous code execution introduces risks at multiple levels:</p> Risk Category Examples Potential Impact Data loss <code>rm -rf</code>, <code>DROP TABLE</code>, file overwrites Irreversible loss of files, databases, or state System compromise <code>sudo</code> commands, privilege escalation Security breaches, unauthorized access Network exfiltration HTTP POST to external services Data leaks, API key exposure Resource exhaustion Infinite loops, fork bombs System instability, denial of service Side effects Git force-push, package installation Environment corruption, team disruption <p>The core problem</p> <p>An LLM generating code cannot be fully trusted to avoid harmful actions. Even well-intentioned prompts can lead to destructive code through hallucination, misinterpretation, or emergent behavior. The approval system provides a programmatic safety net that works regardless of the model's intent.</p>"},{"location":"security/#the-approval-workflow","title":"The Approval Workflow","text":"<p>Every agent action passes through a structured approval workflow before execution:</p> <pre><code>Agent Action --&gt; Risk Assessment --&gt; Policy Check --&gt; Handler --&gt; Decision --&gt; Audit Log\n     |               |                   |              |            |            |\n     v               v                   v              v            v            v\n  dict with      RiskAssessor       ApprovalPolicy   Handler     approve/    AuditEntry\n  action/code    evaluates 40+      determines if    prompts     deny        logged to\n  fields         risk rules         approval needed  user/auto               file + memory\n</code></pre>"},{"location":"security/#step-by-step-flow","title":"Step-by-Step Flow","text":"<p>1. Agent Action</p> <p>The agent produces an action dictionary containing the action type, code to execute, and metadata:</p> <pre><code>action = {\n    \"action\": \"code\",\n    \"code\": \"import shutil; shutil.rmtree('/tmp/experiment')\",\n    \"reasoning\": \"Clean up temporary files\",\n}\n</code></pre> <p>2. Risk Assessment</p> <p>The <code>RiskAssessor</code> evaluates the action against 40+ configurable risk rules using pattern matching. Each triggered rule contributes to the overall risk level:</p> <pre><code>assessment = RiskAssessment(\n    level=ToolRiskLevel.HIGH,\n    reasons=[\"File deletion may cause data loss\"],\n    affected_resources=[\"file:/tmp/experiment\"],\n    reversible=False,\n    estimated_impact=\"Significant impact, may require manual intervention to undo\",\n    recommendations=[\"Review the action carefully before approving\"],\n)\n</code></pre> <p>See Risk Assessment for full documentation.</p> <p>3. Policy Check</p> <p>The <code>ApprovalPolicy</code> determines whether the assessed risk level requires human approval. Six policy modes are available, from fully permissive (<code>AUTO_APPROVE</code>) to fully restrictive (<code>CONFIRM_ALL</code>):</p> <pre><code># Only require approval for HIGH and CRITICAL actions\npolicy = ApprovalPolicy.CONFIRM_HIGH_RISK\n\n# This HIGH-risk action requires approval\nrequires_approval = True\n</code></pre> <p>See Approval Gates for full documentation.</p> <p>4. Handler</p> <p>If approval is required, an <code>ApprovalHandler</code> manages the approval interaction. Handlers range from interactive terminal prompts to automated callbacks for integration with external systems:</p> <pre><code># Console handler: prompts user in terminal\n# Auto handlers: approve or deny without interaction\n# Callback handler: delegates to custom function\n</code></pre> <p>See Approval Gates for handler documentation.</p> <p>5. Decision</p> <p>The handler returns an <code>ApprovalResponse</code> with the decision:</p> <pre><code>response = ApprovalResponse(\n    request_id=\"abc123\",\n    status=ApprovalStatus.APPROVED,\n    approved=True,\n    reason=\"User approved via console\",\n    approver=\"console_user\",\n)\n</code></pre> <p>6. Audit Log</p> <p>Every decision -- whether approved, denied, auto-approved, or timed out -- is recorded in the audit log for compliance and debugging:</p> <pre><code>entry = AuditEntry(\n    entry_id=\"abc123-2025-01-15\",\n    timestamp=\"2025-01-15T10:30:00Z\",\n    request_id=\"abc123\",\n    action_type=\"code\",\n    risk_level=\"high\",\n    approved=True,\n    status=\"approved\",\n    reason=\"User approved via console\",\n    approver=\"console_user\",\n    code_preview=\"import shutil; shutil.rmtree('/tmp/experiment')\",\n    affected_resources=[\"file:/tmp/experiment\"],\n)\n</code></pre> <p>See Audit Logging for full documentation.</p>"},{"location":"security/#architecture","title":"Architecture","text":"<p>The approval system consists of four components:</p> <pre><code>+-------------------+     +------------------+     +------------------+\n|   ApprovalGate    |----&gt;|  RiskAssessor    |     | ApprovalHandler  |\n|   (orchestrator)  |     |  (40+ rules)     |     | (interaction)    |\n|                   |     +------------------+     +------------------+\n|  check_action()   |                                      |\n|  request_approval |--------------------------------------+\n|  approve/deny     |\n+-------------------+\n        |\n        v\n+-------------------+\n| ApprovalAuditLog  |\n| (persistence)     |\n+-------------------+\n</code></pre> Component Module Responsibility <code>ApprovalGate</code> <code>approval.gate</code> Orchestrates the entire workflow <code>RiskAssessor</code> <code>approval.policy</code> Evaluates action risk using rules <code>ApprovalPolicy</code> <code>approval.policy</code> Determines approval requirements <code>ApprovalHandler</code> <code>approval.handlers</code> Manages human/automated approval <code>ApprovalAuditLog</code> <code>approval.audit</code> Records all decisions"},{"location":"security/#quick-start","title":"Quick Start","text":""},{"location":"security/#basic-setup","title":"Basic Setup","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    ConsoleApprovalHandler,\n    ApprovalAuditLog,\n)\n\n# Create audit log\naudit_log = ApprovalAuditLog(log_file=\"audit.jsonl\")\n\n# Create console handler for interactive approval\nhandler = ConsoleApprovalHandler(timeout_seconds=60)\n\n# Create approval gate\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_HIGH_RISK,\n    approval_handler=handler.handle,\n    audit_log=audit_log,\n)\n</code></pre>"},{"location":"security/#checking-an-action","title":"Checking an Action","text":"<pre><code># Agent produces an action\naction = {\n    \"action\": \"code\",\n    \"code\": \"os.remove('/important/file.txt')\",\n}\n\n# Check if approval is needed\nrequest = gate.check_action(action)\n\nif request.requires_approval:\n    # Request approval (async)\n    response = await gate.request_approval(request)\n    if response.approved:\n        # Execute the action\n        execute(action)\n    else:\n        print(f\"Action denied: {response.reason}\")\nelse:\n    # Low risk, execute directly\n    execute(action)\n</code></pre>"},{"location":"security/#non-interactive-setup","title":"Non-Interactive Setup","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    AutoDenyHandler,\n)\n\n# Deny all risky actions automatically (safest for CI/CD)\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_MEDIUM_AND_UP,\n    approval_handler=AutoDenyHandler().handle,\n)\n</code></pre>"},{"location":"security/#module-reference","title":"Module Reference","text":"Import Description <code>ApprovalGate</code> Central orchestrator for the approval workflow <code>ApprovalRequest</code> Represents a request for approval with risk assessment <code>ApprovalResponse</code> Represents the approval decision <code>ApprovalStatus</code> Enum of possible decision states <code>ApprovalPolicy</code> Enum of approval policy modes <code>RiskAssessor</code> Evaluates action risk using configurable rules <code>ToolRiskLevel</code> Enum of risk levels (SAFE through CRITICAL) <code>RiskAssessment</code> Data class containing risk evaluation results <code>ApprovalHandler</code> Base class for approval handlers <code>ConsoleApprovalHandler</code> Interactive terminal-based handler <code>AutoApproveHandler</code> Automatic approval (use with caution) <code>AutoDenyHandler</code> Automatic denial (strictest) <code>CallbackApprovalHandler</code> Custom callback-based handler <code>ApprovalAuditLog</code> Persistent audit log for compliance <code>AuditEntry</code> Single audit log entry <pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    ToolRiskLevel,\n    ApprovalRequest,\n    ApprovalResponse,\n    ApprovalStatus,\n    RiskAssessor,\n    RiskAssessment,\n    ConsoleApprovalHandler,\n    AutoApproveHandler,\n    AutoDenyHandler,\n    CallbackApprovalHandler,\n    ApprovalAuditLog,\n    AuditEntry,\n)\n</code></pre>"},{"location":"security/approval-gates/","title":"Approval Gates","text":""},{"location":"security/approval-gates/#overview","title":"Overview","text":"<p>The <code>ApprovalGate</code> is the central orchestrator of the approval workflow. It combines risk assessment, policy evaluation, and handler delegation into a single interface. When an agent produces an action, the gate assesses its risk, determines whether approval is required based on the active policy, routes approval requests to the configured handler, and logs all decisions.</p>"},{"location":"security/approval-gates/#approvalgate-class","title":"ApprovalGate Class","text":"<pre><code>class ApprovalGate:\n    \"\"\"Central approval gate for tool execution.\"\"\"\n\n    def __init__(\n        self,\n        policy: ApprovalPolicy = ApprovalPolicy.CONFIRM_HIGH_RISK,\n        risk_assessor: RiskAssessor | None = None,\n        approval_handler: Callable[[ApprovalRequest], Awaitable[ApprovalResponse]] | None = None,\n        audit_log: Any = None,\n    ):\n        ...\n</code></pre> Parameter Type Default Description <code>policy</code> <code>ApprovalPolicy</code> <code>CONFIRM_HIGH_RISK</code> The approval policy mode <code>risk_assessor</code> <code>RiskAssessor \\| None</code> <code>None</code> (creates default) Custom risk assessor with custom rules <code>approval_handler</code> <code>Callable \\| None</code> <code>None</code> Async function to handle approval requests <code>audit_log</code> <code>ApprovalAuditLog \\| None</code> <code>None</code> Audit log for recording decisions"},{"location":"security/approval-gates/#methods","title":"Methods","text":"Method Signature Description <code>check_action</code> <code>(action: dict) -&gt; ApprovalRequest</code> Assess an action and determine if approval is needed <code>request_approval</code> <code>async (request: ApprovalRequest) -&gt; ApprovalResponse</code> Request approval through the configured handler <code>approve</code> <code>(request_id: str, reason: str, approver: str) -&gt; ApprovalResponse</code> Manually approve a pending request <code>deny</code> <code>(request_id: str, reason: str, approver: str) -&gt; ApprovalResponse</code> Manually deny a pending request <code>get_pending_requests</code> <code>() -&gt; list[ApprovalRequest]</code> List all pending (unanswered) requests <code>set_policy</code> <code>(policy: ApprovalPolicy) -&gt; None</code> Change the approval policy at runtime <code>set_approval_handler</code> <code>(handler: Callable) -&gt; None</code> Change the approval handler at runtime <code>format_request_for_display</code> <code>(request: ApprovalRequest) -&gt; str</code> Format a request for human-readable display"},{"location":"security/approval-gates/#approvalrequest","title":"ApprovalRequest","text":"<p>Represents a request for approval of an action. Created by <code>ApprovalGate.check_action()</code>.</p> <pre><code>@dataclass\nclass ApprovalRequest:\n    request_id: str                          # Unique identifier (8-char UUID prefix)\n    action: dict[str, Any]                   # The action dictionary\n    risk_assessment: RiskAssessment          # Result of risk evaluation\n    requires_approval: bool                  # Whether approval is needed per policy\n    timestamp: str                           # ISO 8601 timestamp\n    context: dict[str, Any]                  # Additional context\n    timeout_seconds: int = 300               # Timeout for approval (default 5 minutes)\n</code></pre> Field Type Description <code>request_id</code> <code>str</code> Unique 8-character identifier for tracking <code>action</code> <code>dict[str, Any]</code> The action to be approved (contains <code>action</code>, <code>code</code>, etc.) <code>risk_assessment</code> <code>RiskAssessment</code> Full risk evaluation results <code>requires_approval</code> <code>bool</code> Whether this action needs approval based on current policy <code>timestamp</code> <code>str</code> When the request was created (UTC ISO 8601) <code>context</code> <code>dict[str, Any]</code> Additional contextual information <code>timeout_seconds</code> <code>int</code> Maximum wait time for approval (default 300 = 5 minutes)"},{"location":"security/approval-gates/#factory-method","title":"Factory Method","text":"<pre><code>request = ApprovalRequest.create(\n    action={\"action\": \"code\", \"code\": \"rm -rf /tmp/data\"},\n    assessment=risk_assessment,\n    requires_approval=True,\n    context={\"task\": \"cleanup\", \"step\": 3},\n)\n</code></pre>"},{"location":"security/approval-gates/#approvalresponse","title":"ApprovalResponse","text":"<p>Represents the decision made on an approval request.</p> <pre><code>@dataclass\nclass ApprovalResponse:\n    request_id: str                          # Matching request ID\n    status: ApprovalStatus                   # Decision status\n    approved: bool                           # Whether the action was approved\n    reason: str = \"\"                         # Explanation for the decision\n    modified_action: dict[str, Any] | None = None  # Optionally modified action\n    timestamp: str                           # When the decision was made\n    approver: str = \"\"                       # Who made the decision\n</code></pre> Field Type Description <code>request_id</code> <code>str</code> The request this response addresses <code>status</code> <code>ApprovalStatus</code> Status enum value (see below) <code>approved</code> <code>bool</code> Whether the action may proceed <code>reason</code> <code>str</code> Human-readable explanation <code>modified_action</code> <code>dict \\| None</code> If set, use this action instead of the original <code>timestamp</code> <code>str</code> When the decision was made (UTC ISO 8601) <code>approver</code> <code>str</code> Identifier of the decision-maker"},{"location":"security/approval-gates/#approvalstatus-enum","title":"ApprovalStatus Enum","text":"<pre><code>class ApprovalStatus(Enum):\n    PENDING       = \"pending\"        # Awaiting decision\n    APPROVED      = \"approved\"       # Explicitly approved by a human or handler\n    DENIED        = \"denied\"         # Explicitly denied\n    TIMEOUT       = \"timeout\"        # No response within timeout period\n    AUTO_APPROVED = \"auto_approved\"  # Automatically approved by policy (no handler needed)\n    AUTO_DENIED   = \"auto_denied\"    # Automatically denied by policy\n</code></pre> Status <code>approved</code> Trigger <code>PENDING</code> -- Request created, awaiting decision <code>APPROVED</code> <code>True</code> Human or handler explicitly approved <code>DENIED</code> <code>False</code> Human or handler explicitly denied <code>TIMEOUT</code> configurable No response within <code>timeout_seconds</code> <code>AUTO_APPROVED</code> <code>True</code> Policy determined no approval needed <code>AUTO_DENIED</code> <code>False</code> Auto-deny policy with no handler"},{"location":"security/approval-gates/#approvalpolicy-enum","title":"ApprovalPolicy Enum","text":"<p>The <code>ApprovalPolicy</code> enum defines six policy modes that control when actions require approval:</p> <pre><code>class ApprovalPolicy(Enum):\n    AUTO_APPROVE       = \"auto_approve\"         # Approve everything\n    AUTO_DENY          = \"auto_deny\"            # Deny everything requiring approval\n    CONFIRM_ALL        = \"confirm_all\"          # Confirm every action\n    CONFIRM_HIGH_RISK  = \"confirm_high_risk\"    # Only confirm HIGH/CRITICAL\n    CONFIRM_MEDIUM_AND_UP = \"confirm_medium_and_up\"  # Confirm MEDIUM+\n    CUSTOM             = \"custom\"               # Use custom rules\n</code></pre>"},{"location":"security/approval-gates/#policy-behavior-matrix","title":"Policy Behavior Matrix","text":"Policy SAFE LOW MEDIUM HIGH CRITICAL <code>AUTO_APPROVE</code> auto-approve auto-approve auto-approve auto-approve auto-approve <code>AUTO_DENY</code> auto-approve requires approval requires approval requires approval requires approval <code>CONFIRM_ALL</code> requires approval requires approval requires approval requires approval requires approval <code>CONFIRM_HIGH_RISK</code> auto-approve auto-approve auto-approve requires approval requires approval <code>CONFIRM_MEDIUM_AND_UP</code> auto-approve auto-approve requires approval requires approval requires approval <code>CUSTOM</code> depends on <code>assessment.requires_approval</code> <p>AUTO_APPROVE is dangerous</p> <p>The <code>AUTO_APPROVE</code> policy bypasses all safety checks. It should only be used in fully sandboxed environments where the agent cannot access any resources you care about. Never use this in production.</p> <p>Recommended policies</p> Environment Recommended Policy Development (trusted tasks) <code>CONFIRM_HIGH_RISK</code> Production <code>CONFIRM_MEDIUM_AND_UP</code> CI/CD pipelines <code>AUTO_DENY</code> with <code>AutoDenyHandler</code> Fully sandboxed <code>AUTO_APPROVE</code> (only if sandbox is airtight) Research (interactive) <code>CONFIRM_ALL</code> (review everything)"},{"location":"security/approval-gates/#approval-handlers","title":"Approval Handlers","text":"<p>Handlers manage the actual approval interaction. All handlers implement the <code>ApprovalHandler</code> base class:</p> <pre><code>class ApprovalHandler(ABC):\n    @abstractmethod\n    async def handle(self, request: ApprovalRequest) -&gt; ApprovalResponse:\n        \"\"\"Handle an approval request.\"\"\"\n        ...\n</code></pre>"},{"location":"security/approval-gates/#consoleapprovalhandler","title":"ConsoleApprovalHandler","text":"<p>Interactive terminal-based handler that displays the approval request and prompts the user for a decision.</p> <pre><code>from rlm_code.rlm.approval import ConsoleApprovalHandler\n\nhandler = ConsoleApprovalHandler(\n    timeout_seconds=300,      # 5 minutes before timeout\n    default_on_timeout=False, # Deny on timeout (safer)\n)\n</code></pre> Parameter Default Description <code>timeout_seconds</code> <code>300</code> Maximum wait time for user response <code>default_on_timeout</code> <code>False</code> Whether to approve (<code>True</code>) or deny (<code>False</code>) on timeout <p>When triggered, it displays a formatted request:</p> <pre><code>============================================================\n=== Approval Request [abc12345] ===\nRisk Level: HIGH\n\nAction:\n  Type: code\n  Code:\n    import shutil\n    shutil.rmtree('/tmp/experiment')\n\nRisk Assessment:\n  - File deletion may cause data loss\n\nAffected Resources:\n  - file:/tmp/experiment\n\nWARNING: This action may not be reversible!\n\nRecommendations:\n  - Review the action carefully before approving\n  - This action cannot be easily undone\n\nOptions: [A]pprove, [D]eny, [S]kip\n============================================================\n\nYour decision [A/D/S]:\n</code></pre> <p>The handler accepts these inputs:</p> Input Aliases Result <code>a</code> <code>approve</code>, <code>yes</code>, <code>y</code> <code>APPROVED</code> <code>d</code> <code>deny</code>, <code>no</code>, <code>n</code> <code>DENIED</code> <code>s</code> <code>skip</code> <code>DENIED</code> (with \"skipped\" reason)"},{"location":"security/approval-gates/#autoapprovehandler","title":"AutoApproveHandler","text":"<p>Automatically approves all requests. Use only in sandboxed environments.</p> <pre><code>from rlm_code.rlm.approval import AutoApproveHandler\n\nhandler = AutoApproveHandler(reason=\"Auto-approved for testing\")\n</code></pre> <p>Use with extreme caution</p> <p>This handler approves every action regardless of risk level. Only use it in fully isolated sandbox environments or during testing with non-destructive actions.</p>"},{"location":"security/approval-gates/#autodenyhandler","title":"AutoDenyHandler","text":"<p>Automatically denies all requests that require approval. The safest handler for non-interactive environments.</p> <pre><code>from rlm_code.rlm.approval import AutoDenyHandler\n\nhandler = AutoDenyHandler(reason=\"Auto-denied per security policy\")\n</code></pre>"},{"location":"security/approval-gates/#callbackapprovalhandler","title":"CallbackApprovalHandler","text":"<p>Delegates approval decisions to a custom async callback function, enabling integration with external systems such as web UIs, Slack bots, or approval APIs.</p> <pre><code>from rlm_code.rlm.approval import CallbackApprovalHandler\n\nasync def my_approval_callback(request: ApprovalRequest) -&gt; bool:\n    \"\"\"Custom approval logic.\"\"\"\n    # Example: auto-approve if only file reads\n    if request.risk_assessment.level.value in (\"safe\", \"low\"):\n        return True\n    # Example: check an external approval service\n    response = await external_api.check_approval(\n        action=request.action,\n        risk=request.risk_assessment.level.value,\n    )\n    return response.approved\n\nhandler = CallbackApprovalHandler(\n    callback=my_approval_callback,\n    reason_callback=lambda req, approved: (\n        f\"Approved by external service\" if approved\n        else f\"Denied by external service\"\n    ),\n)\n</code></pre> Parameter Type Description <code>callback</code> <code>Callable[[ApprovalRequest], Awaitable[bool]]</code> Async function returning True (approve) or False (deny) <code>reason_callback</code> <code>Callable[[ApprovalRequest, bool], str] \\| None</code> Optional function to generate the reason string <p>Error handling</p> <p>If the callback raises an exception, the handler automatically denies the request and includes the error message in the reason field. This fail-safe ensures that handler errors never result in unintended approvals.</p>"},{"location":"security/approval-gates/#conditionalapprovalhandler","title":"ConditionalApprovalHandler","text":"<p>Routes requests based on risk level: auto-approves low-risk actions and delegates higher-risk ones to another handler.</p> <pre><code>from rlm_code.rlm.approval.handlers import ConditionalApprovalHandler\n\nhandler = ConditionalApprovalHandler(\n    high_risk_handler=ConsoleApprovalHandler(),\n    auto_approve_below=\"medium\",  # auto-approve SAFE and LOW\n)\n</code></pre> Parameter Type Description <code>high_risk_handler</code> <code>ApprovalHandler</code> Handler for actions above the threshold <code>auto_approve_below</code> <code>str</code> Risk level at or below which to auto-approve (<code>\"safe\"</code>, <code>\"low\"</code>, or <code>\"medium\"</code>)"},{"location":"security/approval-gates/#queueapprovalhandler","title":"QueueApprovalHandler","text":"<p>Queues approval requests for batch processing. Useful for non-interactive modes where approvals are handled in bulk.</p> <pre><code>from rlm_code.rlm.approval.handlers import QueueApprovalHandler\n\nhandler = QueueApprovalHandler(default_timeout=60)\n\n# Later, process the queue\npending = handler.get_pending()\nhandler.approve_all(reason=\"Batch approved after review\")\n# or\nhandler.deny_all(reason=\"Batch denied\")\n# or approve individually\nhandler.respond(request_id=\"abc123\", approved=True, reason=\"Reviewed and approved\")\n</code></pre> Method Description <code>get_pending()</code> List all pending requests in the queue <code>respond(request_id, approved, reason)</code> Respond to a specific queued request <code>approve_all(reason)</code> Approve all pending requests <code>deny_all(reason)</code> Deny all pending requests"},{"location":"security/approval-gates/#complete-setup-examples","title":"Complete Setup Examples","text":""},{"location":"security/approval-gates/#interactive-development","title":"Interactive Development","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    ConsoleApprovalHandler,\n    ApprovalAuditLog,\n)\n\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_HIGH_RISK,\n    approval_handler=ConsoleApprovalHandler(timeout_seconds=120).handle,\n    audit_log=ApprovalAuditLog(log_file=\"dev_audit.jsonl\"),\n)\n\n# Use in agent loop\nasync def execute_action(action):\n    request = gate.check_action(action)\n    if request.requires_approval:\n        response = await gate.request_approval(request)\n        if not response.approved:\n            return f\"Denied: {response.reason}\"\n    return run_code(action[\"code\"])\n</code></pre>"},{"location":"security/approval-gates/#cicd-pipeline-non-interactive","title":"CI/CD Pipeline (Non-Interactive)","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    AutoDenyHandler,\n    ApprovalAuditLog,\n)\n\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_MEDIUM_AND_UP,\n    approval_handler=AutoDenyHandler(\n        reason=\"CI/CD: risky actions not permitted\"\n    ).handle,\n    audit_log=ApprovalAuditLog(log_file=\"ci_audit.jsonl\"),\n)\n</code></pre>"},{"location":"security/approval-gates/#external-approval-service-integration","title":"External Approval Service Integration","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    CallbackApprovalHandler,\n    ApprovalAuditLog,\n)\n\nasync def slack_approval(request):\n    \"\"\"Send approval request to Slack and wait for response.\"\"\"\n    message = (\n        f\"*Approval Required* [{request.request_id}]\\n\"\n        f\"Risk: {request.risk_assessment.level.value}\\n\"\n        f\"Action: {request.action.get('action')}\\n\"\n        f\"Code: ```{request.action.get('code', '')[:200]}```\"\n    )\n    channel_response = await slack_client.post_message(\n        channel=\"#agent-approvals\",\n        text=message,\n    )\n    # Wait for reaction (thumbsup = approve, thumbsdown = deny)\n    reaction = await slack_client.wait_for_reaction(\n        channel_response.ts,\n        timeout=300,\n    )\n    return reaction == \"thumbsup\"\n\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_HIGH_RISK,\n    approval_handler=CallbackApprovalHandler(callback=slack_approval).handle,\n    audit_log=ApprovalAuditLog(log_file=\"production_audit.jsonl\"),\n)\n</code></pre>"},{"location":"security/approval-gates/#runtime-policy-switching","title":"Runtime Policy Switching","text":"<pre><code># Start permissive\ngate = ApprovalGate(policy=ApprovalPolicy.CONFIRM_HIGH_RISK)\n\n# Tighten security for sensitive phase\ngate.set_policy(ApprovalPolicy.CONFIRM_MEDIUM_AND_UP)\n\n# Switch to different handler\ngate.set_approval_handler(ConsoleApprovalHandler(timeout_seconds=60).handle)\n</code></pre>"},{"location":"security/audit/","title":"Audit Logging","text":""},{"location":"security/audit/#overview","title":"Overview","text":"<p>The <code>ApprovalAuditLog</code> records every approval decision made by the system, providing a persistent, queryable trail for compliance, debugging, and operational analysis. Every action that passes through an <code>ApprovalGate</code> -- whether approved, denied, auto-approved, or timed out -- generates an <code>AuditEntry</code> that captures the full context: what was requested, who decided, why, and when.</p> <p>The audit log supports both in-memory storage for fast querying and file-based persistence for long-term retention.</p>"},{"location":"security/audit/#approvalauditlog-class","title":"ApprovalAuditLog Class","text":"<pre><code>class ApprovalAuditLog:\n    \"\"\"Audit log for approval decisions.\"\"\"\n\n    def __init__(\n        self,\n        log_file: str | Path | None = None,\n        max_memory_entries: int = 1000,\n    ):\n        ...\n</code></pre> Parameter Type Default Description <code>log_file</code> <code>str \\| Path \\| None</code> <code>None</code> Path to JSONL file for persistent storage. Parent directories are created automatically <code>max_memory_entries</code> <code>int</code> <code>1000</code> Maximum entries to keep in memory. Oldest are evicted when exceeded"},{"location":"security/audit/#methods","title":"Methods","text":"Method Signature Description <code>log</code> <code>(request, response) -&gt; AuditEntry</code> Record an approval decision <code>get_entries</code> <code>(limit, approved_only, denied_only, risk_level) -&gt; list[AuditEntry]</code> Query entries with filters <code>get_summary</code> <code>() -&gt; dict</code> Get aggregate statistics <code>export_report</code> <code>(output_path) -&gt; None</code> Export a Markdown compliance report <code>load_from_file</code> <code>() -&gt; int</code> Load entries from the log file into memory <code>clear</code> <code>() -&gt; None</code> Clear in-memory entries (does not affect file)"},{"location":"security/audit/#auditentry-data-class","title":"AuditEntry Data Class","text":"<p>Each audit entry captures the complete context of an approval decision:</p> <pre><code>@dataclass\nclass AuditEntry:\n    entry_id: str                    # Unique entry identifier\n    timestamp: str                   # ISO 8601 timestamp of the decision\n    request_id: str                  # Matching approval request ID\n    action_type: str                 # Action type (e.g., \"code\", \"final\")\n    risk_level: str                  # Risk level string (e.g., \"high\", \"critical\")\n    approved: bool                   # Whether the action was approved\n    status: str                      # ApprovalStatus value (e.g., \"approved\", \"auto_denied\")\n    reason: str                      # Explanation for the decision\n    approver: str                    # Who/what made the decision\n    code_preview: str                # First 200 chars of the code (truncated)\n    affected_resources: list[str]    # Resources impacted by the action\n    metadata: dict[str, Any]         # Additional data (reversible, risk_reasons)\n</code></pre> Field Type Description <code>entry_id</code> <code>str</code> Composite ID: <code>\"{request_id}-{date}\"</code> <code>timestamp</code> <code>str</code> When the decision was made (UTC ISO 8601) <code>request_id</code> <code>str</code> The original approval request's ID <code>action_type</code> <code>str</code> What kind of action was requested <code>risk_level</code> <code>str</code> Risk level as a string value <code>approved</code> <code>bool</code> Final boolean decision <code>status</code> <code>str</code> Detailed status (e.g., <code>\"approved\"</code>, <code>\"auto_denied\"</code>, <code>\"timeout\"</code>) <code>reason</code> <code>str</code> Human-readable explanation <code>approver</code> <code>str</code> Identity of the decision-maker (e.g., <code>\"console_user\"</code>, <code>\"auto_approve_handler\"</code>) <code>code_preview</code> <code>str</code> Truncated code (first 200 characters + <code>\"...\"</code> if longer) <code>affected_resources</code> <code>list[str]</code> Resources from the risk assessment <code>metadata</code> <code>dict</code> Extra data including <code>reversible</code> flag and <code>risk_reasons</code> list"},{"location":"security/audit/#factory-method","title":"Factory Method","text":"<p>Entries are normally created automatically by the audit log, but can be constructed manually:</p> <pre><code>entry = AuditEntry.from_request_response(request, response)\n</code></pre>"},{"location":"security/audit/#serialization","title":"Serialization","text":"<pre><code>entry_dict = entry.to_dict()\n# Returns a plain dictionary suitable for JSON serialization\n</code></pre>"},{"location":"security/audit/#setup","title":"Setup","text":""},{"location":"security/audit/#in-memory-only","title":"In-Memory Only","text":"<pre><code>from rlm_code.rlm.approval import ApprovalAuditLog\n\n# Memory-only audit log (no file persistence)\naudit_log = ApprovalAuditLog(max_memory_entries=500)\n</code></pre>"},{"location":"security/audit/#with-file-persistence","title":"With File Persistence","text":"<pre><code># Persistent audit log (JSONL format)\naudit_log = ApprovalAuditLog(\n    log_file=\"logs/approval_audit.jsonl\",\n    max_memory_entries=1000,\n)\n</code></pre> <p>The log file uses JSON Lines format -- one JSON object per line. Parent directories are created automatically if they do not exist.</p> <p>JSONL format</p> <p>Each line in the log file is a complete JSON object representing one <code>AuditEntry</code>: <pre><code>{\"entry_id\": \"abc12345-2025-01-15\", \"timestamp\": \"2025-01-15T10:30:00+00:00\", \"request_id\": \"abc12345\", \"action_type\": \"code\", \"risk_level\": \"high\", \"approved\": true, \"status\": \"approved\", \"reason\": \"User approved via console\", \"approver\": \"console_user\", \"code_preview\": \"import shutil; shutil.rmtree('/tmp/data')\", \"affected_resources\": [\"file:/tmp/data\"], \"metadata\": {\"reversible\": false, \"risk_reasons\": [\"File deletion may cause data loss\"]}}\n</code></pre></p>"},{"location":"security/audit/#integration-with-approvalgate","title":"Integration with ApprovalGate","text":"<pre><code>from rlm_code.rlm.approval import ApprovalGate, ApprovalPolicy, ApprovalAuditLog\n\naudit_log = ApprovalAuditLog(log_file=\"audit.jsonl\")\n\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_HIGH_RISK,\n    audit_log=audit_log,\n)\n\n# All decisions through this gate are automatically logged\nrequest = gate.check_action({\"action\": \"code\", \"code\": \"os.remove('file.txt')\"})\nresponse = await gate.request_approval(request)\n# Audit entry created automatically\n</code></pre>"},{"location":"security/audit/#logging-decisions","title":"Logging Decisions","text":"<p>The <code>log()</code> method is called automatically by <code>ApprovalGate</code> but can also be called directly:</p> <pre><code>from rlm_code.rlm.approval import (\n    ApprovalAuditLog,\n    ApprovalRequest,\n    ApprovalResponse,\n    ApprovalStatus,\n    RiskAssessment,\n    ToolRiskLevel,\n)\n\naudit_log = ApprovalAuditLog(log_file=\"audit.jsonl\")\n\n# Log is called automatically by ApprovalGate, but can be manual:\nentry = audit_log.log(request, response)\nprint(f\"Logged: {entry.entry_id} - {entry.status}\")\n</code></pre> <p>Fail-safe logging</p> <p>File writes in the audit log use silent error handling. If the log file cannot be written (permissions, disk full, etc.), the error is silently ignored and the in-memory log continues to function. This ensures that audit logging never disrupts the agent's execution.</p>"},{"location":"security/audit/#querying-entries","title":"Querying Entries","text":""},{"location":"security/audit/#basic-queries","title":"Basic Queries","text":"<pre><code># Get all entries\nall_entries = audit_log.get_entries()\n\n# Get last 10 entries\nrecent = audit_log.get_entries(limit=10)\n\n# Get only approved entries\napproved = audit_log.get_entries(approved_only=True)\n\n# Get only denied entries\ndenied = audit_log.get_entries(denied_only=True)\n\n# Filter by risk level\nhigh_risk = audit_log.get_entries(risk_level=\"high\")\ncritical = audit_log.get_entries(risk_level=\"critical\")\n</code></pre>"},{"location":"security/audit/#query-parameters","title":"Query Parameters","text":"Parameter Type Default Description <code>limit</code> <code>int \\| None</code> <code>None</code> Maximum number of entries to return (from most recent) <code>approved_only</code> <code>bool</code> <code>False</code> Only return approved entries <code>denied_only</code> <code>bool</code> <code>False</code> Only return denied entries <code>risk_level</code> <code>str \\| None</code> <code>None</code> Filter by risk level string (<code>\"safe\"</code>, <code>\"low\"</code>, <code>\"medium\"</code>, <code>\"high\"</code>, <code>\"critical\"</code>) <p>Filter precedence</p> <p>If both <code>approved_only</code> and <code>denied_only</code> are set to <code>True</code>, <code>approved_only</code> takes precedence and <code>denied_only</code> is ignored.</p>"},{"location":"security/audit/#example-investigating-denied-actions","title":"Example: Investigating Denied Actions","text":"<pre><code>denied_entries = audit_log.get_entries(denied_only=True, risk_level=\"critical\")\n\nfor entry in denied_entries:\n    print(f\"[{entry.timestamp}] {entry.action_type}\")\n    print(f\"  Risk: {entry.risk_level}\")\n    print(f\"  Reason: {entry.reason}\")\n    print(f\"  Code: {entry.code_preview}\")\n    print(f\"  Resources: {', '.join(entry.affected_resources)}\")\n    print()\n</code></pre>"},{"location":"security/audit/#summary-statistics","title":"Summary Statistics","text":"<p>The <code>get_summary()</code> method provides aggregate statistics across all logged entries:</p> <pre><code>summary = audit_log.get_summary()\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"total\": 150,\n    \"approved\": 120,\n    \"denied\": 30,\n    \"approval_rate\": 0.8,\n    \"by_risk_level\": {\n        \"safe\": {\"total\": 80, \"approved\": 80, \"denied\": 0},\n        \"low\": {\"total\": 30, \"approved\": 28, \"denied\": 2},\n        \"medium\": {\"total\": 25, \"approved\": 10, \"denied\": 15},\n        \"high\": {\"total\": 12, \"approved\": 2, \"denied\": 10},\n        \"critical\": {\"total\": 3, \"approved\": 0, \"denied\": 3},\n    },\n}\n</code></pre> Field Type Description <code>total</code> <code>int</code> Total number of logged decisions <code>approved</code> <code>int</code> Number of approved actions <code>denied</code> <code>int</code> Number of denied actions <code>approval_rate</code> <code>float</code> Ratio of approved to total (0.0 to 1.0) <code>by_risk_level</code> <code>dict</code> Breakdown by risk level with per-level totals, approved, and denied counts"},{"location":"security/audit/#empty-log-summary","title":"Empty Log Summary","text":"<p>If no entries have been logged:</p> <pre><code>{\n    \"total\": 0,\n    \"approved\": 0,\n    \"denied\": 0,\n    \"approval_rate\": 0.0,\n    \"by_risk_level\": {},\n}\n</code></pre>"},{"location":"security/audit/#exporting-compliance-reports","title":"Exporting Compliance Reports","text":"<p>The <code>export_report()</code> method generates a Markdown compliance report:</p> <pre><code>audit_log.export_report(\"reports/compliance_report.md\")\n</code></pre> <p>The generated report includes:</p> <pre><code># Approval Audit Report\nGenerated: 2025-01-15T18:30:00+00:00\n\n## Summary\n- Total decisions: 150\n- Approved: 120\n- Denied: 30\n- Approval rate: 80.0%\n\n## By Risk Level\n- SAFE: 80 total, 80 approved (100%)\n- LOW: 30 total, 28 approved (93%)\n- MEDIUM: 25 total, 10 approved (40%)\n- HIGH: 12 total, 2 approved (17%)\n- CRITICAL: 3 total, 0 approved (0%)\n\n## Recent Entries\n\n- [2025-01-15T18:29:45] APPROVED code (low) - File read operation approved\n- [2025-01-15T18:29:30] DENIED code (high) - User denied via console\n- [2025-01-15T18:29:15] AUTO_APPROVED code (safe) - No approval required per po\n...\n</code></pre> <p>Compliance use cases</p> <p>The exported report is useful for:</p> <ul> <li>SOC 2 audits: Demonstrating that risky actions require approval</li> <li>Incident investigation: Understanding the sequence of approved/denied actions</li> <li>Security reviews: Identifying patterns in approval decisions</li> <li>Team reporting: Sharing agent activity summaries</li> </ul>"},{"location":"security/audit/#loading-from-file","title":"Loading from File","text":"<p>If the audit log has file persistence, you can reload entries from disk:</p> <pre><code>audit_log = ApprovalAuditLog(log_file=\"audit.jsonl\")\n\n# Load previous entries from file\nloaded_count = audit_log.load_from_file()\nprint(f\"Loaded {loaded_count} entries from file\")\n\n# Now get_entries() includes loaded entries\nall_entries = audit_log.get_entries()\n</code></pre> <p>Memory limits</p> <p>Loaded entries are subject to <code>max_memory_entries</code>. If the file contains more entries than the limit, only the most recent entries are kept in memory.</p> <p>Error handling</p> <p>Malformed lines in the log file are silently skipped. This ensures that a single corrupted entry does not prevent loading the rest of the audit trail.</p>"},{"location":"security/audit/#clearing-the-log","title":"Clearing the Log","text":"<pre><code># Clear in-memory entries (file is NOT affected)\naudit_log.clear()\n\n# After clear, get_entries() returns empty\nentries = audit_log.get_entries()  # []\n\n# But the file still contains all previous entries\n# You can reload them:\nloaded = audit_log.load_from_file()\n</code></pre>"},{"location":"security/audit/#complete-example","title":"Complete Example","text":"<pre><code>from rlm_code.rlm.approval import (\n    ApprovalGate,\n    ApprovalPolicy,\n    ApprovalAuditLog,\n    ConsoleApprovalHandler,\n    RiskAssessor,\n)\nfrom rlm_code.rlm.approval.policy import RiskRule, ToolRiskLevel\n\n\n# 1. Set up audit log with file persistence\naudit_log = ApprovalAuditLog(\n    log_file=\"logs/session_audit.jsonl\",\n    max_memory_entries=2000,\n)\n\n# 2. Set up risk assessor with custom rules\nassessor = RiskAssessor()\nassessor.add_rule(RiskRule(\n    name=\"env_var_modification\",\n    pattern=r\"os\\.environ\\[\",\n    risk_level=ToolRiskLevel.MEDIUM,\n    reason=\"Environment variable modification detected\",\n    reversible=True,\n))\n\n# 3. Set up gate with all components\ngate = ApprovalGate(\n    policy=ApprovalPolicy.CONFIRM_MEDIUM_AND_UP,\n    risk_assessor=assessor,\n    approval_handler=ConsoleApprovalHandler(timeout_seconds=120).handle,\n    audit_log=audit_log,\n)\n\n\n# 4. Agent execution loop\nasync def agent_loop(actions):\n    for action in actions:\n        request = gate.check_action(action)\n        if request.requires_approval:\n            response = await gate.request_approval(request)\n            if not response.approved:\n                print(f\"Skipping: {response.reason}\")\n                continue\n        # Execute approved action\n        execute(action)\n\n\n# 5. After execution, analyze the audit trail\nsummary = audit_log.get_summary()\nprint(f\"Total decisions: {summary['total']}\")\nprint(f\"Approval rate: {summary['approval_rate']:.0%}\")\nprint(f\"Denied high-risk: {summary['by_risk_level'].get('high', {}).get('denied', 0)}\")\n\n# 6. Query specific entries\ndenied_critical = audit_log.get_entries(denied_only=True, risk_level=\"critical\")\nfor entry in denied_critical:\n    print(f\"DENIED CRITICAL: {entry.code_preview}\")\n    print(f\"  Reason: {entry.reason}\")\n    print(f\"  Resources: {entry.affected_resources}\")\n\n# 7. Export compliance report\naudit_log.export_report(\"reports/session_compliance.md\")\n</code></pre>"},{"location":"security/risk-assessment/","title":"Risk Assessment","text":""},{"location":"security/risk-assessment/#overview","title":"Overview","text":"<p>The <code>RiskAssessor</code> evaluates agent actions against a comprehensive set of risk rules to determine their potential for harm. It uses pattern matching -- both regex and callable-based -- to scan action code for dangerous operations, extract affected resources, estimate impact, and produce actionable recommendations. The default rule set covers 40+ common risk patterns across file operations, network access, system commands, database operations, and more.</p>"},{"location":"security/risk-assessment/#riskassessor-class","title":"RiskAssessor Class","text":"<pre><code>class RiskAssessor:\n    \"\"\"Assesses risk of tool actions.\"\"\"\n\n    def __init__(\n        self,\n        rules: list[RiskRule] | None = None,\n        custom_assessor: Callable[[dict[str, Any]], RiskAssessment] | None = None,\n    ):\n        self.rules = rules or self.DEFAULT_RULES.copy()\n        self.custom_assessor = custom_assessor\n</code></pre> Parameter Type Default Description <code>rules</code> <code>list[RiskRule] \\| None</code> <code>DEFAULT_RULES</code> List of risk rules to evaluate against <code>custom_assessor</code> <code>Callable \\| None</code> <code>None</code> Custom assessor function that bypasses rule-based evaluation"},{"location":"security/risk-assessment/#methods","title":"Methods","text":"Method Signature Description <code>assess</code> <code>(action: dict) -&gt; RiskAssessment</code> Evaluate the risk of an action <code>add_rule</code> <code>(rule: RiskRule) -&gt; None</code> Add a custom risk rule <code>remove_rule</code> <code>(name: str) -&gt; bool</code> Remove a rule by name. Returns <code>True</code> if found and removed"},{"location":"security/risk-assessment/#toolrisklevel-enum","title":"ToolRiskLevel Enum","text":"<p>Risk is classified into five levels, from harmless to potentially catastrophic:</p> <pre><code>class ToolRiskLevel(Enum):\n    SAFE     = \"safe\"      # No risk, auto-approve\n    LOW      = \"low\"       # Minor risk, usually approve\n    MEDIUM   = \"medium\"    # Moderate risk, consider carefully\n    HIGH     = \"high\"      # High risk, require confirmation\n    CRITICAL = \"critical\"  # Critical risk, require explicit approval\n</code></pre> Level Typical Actions Approval Recommendation SAFE <code>print()</code>, display operations Auto-approve LOW File reads, variable inspection Usually approve MEDIUM File writes, subprocess calls, pip install, git commit Consider carefully HIGH File deletion, <code>sudo</code>, network POST/PUT/DELETE, git force-push Require confirmation CRITICAL <code>rm -rf</code>, <code>DROP DATABASE</code>, disk formatting Require explicit approval"},{"location":"security/risk-assessment/#riskassessment-data-class","title":"RiskAssessment Data Class","text":"<p>The output of a risk evaluation:</p> <pre><code>@dataclass\nclass RiskAssessment:\n    level: ToolRiskLevel                     # Overall risk level (highest triggered)\n    reasons: list[str]                       # Why this risk level was assigned\n    affected_resources: list[str]            # Resources that may be impacted\n    reversible: bool = True                  # Whether the action can be undone\n    estimated_impact: str = \"\"               # Human-readable impact estimate\n    recommendations: list[str]               # Suggested precautions\n</code></pre> Field Type Description <code>level</code> <code>ToolRiskLevel</code> The highest risk level among all triggered rules <code>reasons</code> <code>list[str]</code> Explanations from each triggered rule <code>affected_resources</code> <code>list[str]</code> Extracted resources (files, URLs, tables) with type prefixes <code>reversible</code> <code>bool</code> <code>True</code> only if all triggered rules are reversible <code>estimated_impact</code> <code>str</code> Human-readable impact summary based on risk level <code>recommendations</code> <code>list[str]</code> Actionable advice (e.g., \"Review carefully before approving\")"},{"location":"security/risk-assessment/#the-requires_approval-property","title":"The <code>requires_approval</code> Property","text":"<pre><code>@property\ndef requires_approval(self) -&gt; bool:\n    \"\"\"Check if this risk level typically requires approval.\"\"\"\n    return self.level in (ToolRiskLevel.HIGH, ToolRiskLevel.CRITICAL)\n</code></pre> <p>This property is used by the <code>CUSTOM</code> approval policy mode.</p>"},{"location":"security/risk-assessment/#riskrule-data-class","title":"RiskRule Data Class","text":"<p>Individual rules that the assessor evaluates:</p> <pre><code>@dataclass\nclass RiskRule:\n    name: str                                          # Unique rule identifier\n    pattern: str | Callable[[dict[str, Any]], bool]    # Regex pattern or callable\n    risk_level: ToolRiskLevel                          # Risk level when triggered\n    reason: str                                        # Human-readable explanation\n    reversible: bool = True                            # Whether the action is reversible\n</code></pre> Field Type Description <code>name</code> <code>str</code> Unique identifier for the rule (used for <code>remove_rule</code>) <code>pattern</code> <code>str \\| Callable</code> Regex pattern matched against <code>action[\"code\"]</code>, or a callable that receives the full action dict <code>risk_level</code> <code>ToolRiskLevel</code> Risk level assigned when this rule triggers <code>reason</code> <code>str</code> Explanation shown to the user <code>reversible</code> <code>bool</code> Whether the action can be undone"},{"location":"security/risk-assessment/#default-risk-rules","title":"Default Risk Rules","text":"<p>The <code>RiskAssessor</code> ships with a comprehensive set of default rules organized by risk level:</p>"},{"location":"security/risk-assessment/#critical-risk-rules","title":"CRITICAL Risk Rules","text":"Rule Name Pattern Reason Reversible <code>rm_recursive</code> <code>rm\\s+-rf?\\s+</code> Recursive file deletion can cause irreversible data loss No <code>drop_database</code> <code>DROP\\s+(DATABASE\\|TABLE\\|SCHEMA)</code> Database deletion is typically irreversible No <code>format_disk</code> <code>(mkfs\\|format\\|fdisk)</code> Disk formatting destroys all data No"},{"location":"security/risk-assessment/#high-risk-rules","title":"HIGH Risk Rules","text":"Rule Name Pattern Reason Reversible <code>file_delete</code> <code>(os\\.remove\\|os\\.unlink\\|shutil\\.rmtree\\|Path.*\\.unlink)</code> File deletion may cause data loss No <code>git_force_push</code> <code>git\\s+push\\s+.*(-f\\|--force)</code> Force push can overwrite remote history No <code>git_reset_hard</code> <code>git\\s+reset\\s+--hard</code> Hard reset discards uncommitted changes No <code>sudo_command</code> <code>sudo\\s+</code> Elevated privileges can affect system stability Yes <code>network_request</code> <code>(requests\\.(post\\|put\\|delete\\|patch)\\|urllib\\|httpx\\.(post\\|put\\|delete))</code> Modifying external resources via network No"},{"location":"security/risk-assessment/#medium-risk-rules","title":"MEDIUM Risk Rules","text":"Rule Name Pattern Reason Reversible <code>file_write</code> <code>(open\\(.*['\\\"]w\\|\\.write\\(\\|Path.*\\.write_)</code> File modification may overwrite existing content Yes <code>subprocess_exec</code> <code>(subprocess\\.(run\\|call\\|Popen)\\|os\\.system)</code> Executing system commands Yes <code>git_commit</code> <code>git\\s+commit</code> Creating git commits Yes <code>pip_install</code> <code>pip\\s+install</code> Installing packages may affect environment Yes"},{"location":"security/risk-assessment/#low-risk-rules","title":"LOW Risk Rules","text":"Rule Name Pattern Reason Reversible <code>file_read</code> <code>(open\\(.*['\\\"]r\\|\\.read\\(\\|Path.*\\.read_)</code> Reading files Yes"},{"location":"security/risk-assessment/#safe-rules","title":"SAFE Rules","text":"Rule Name Pattern Reason Reversible <code>print_output</code> <code>print\\(</code> Output display only Yes <p>Rule evaluation</p> <p>All regex patterns are evaluated with <code>re.IGNORECASE</code>. When multiple rules trigger, the assessor uses the highest risk level among them. The <code>reversible</code> flag is <code>True</code> only if all triggered rules are reversible.</p>"},{"location":"security/risk-assessment/#assessment-process","title":"Assessment Process","text":"<p>When <code>assess()</code> is called, the following process occurs:</p> <pre><code>1. If custom_assessor is set, delegate entirely to it and return\n2. Extract code and action_type from the action dict\n3. For each rule:\n   a. If pattern is a string: regex search against code (case-insensitive)\n   b. If pattern is a callable: call with the full action dict\n   c. If match: add to triggered_rules list\n4. If no rules triggered: return SAFE assessment\n5. Determine highest risk level among triggered rules\n6. Collect all reasons from triggered rules\n7. Check if all triggered rules are reversible\n8. Extract affected resources from code (files, URLs, tables)\n9. Generate estimated impact text based on risk level\n10. Generate recommendations for HIGH/CRITICAL levels\n11. Return RiskAssessment\n</code></pre>"},{"location":"security/risk-assessment/#resource-extraction","title":"Resource Extraction","text":"<p>The assessor automatically extracts potentially affected resources from code:</p> Resource Type Detection Pattern Example Files Quoted paths, <code>Path()</code> calls <code>file:/tmp/data.csv</code> URLs <code>http://</code> or <code>https://</code> patterns <code>url:https://api.example.com/data</code> Database tables <code>FROM</code>, <code>INTO</code>, <code>UPDATE</code>, <code>DROP</code> clauses <code>table:users</code> <p>Resources are prefixed with their type and limited to 10 total to prevent excessive output.</p>"},{"location":"security/risk-assessment/#impact-estimation","title":"Impact Estimation","text":"Risk Level Estimated Impact <code>CRITICAL</code> \"Potentially severe and irreversible impact\" <code>HIGH</code> \"Significant impact, may require manual intervention to undo\" <code>MEDIUM</code> \"Moderate impact, generally reversible\" <code>LOW</code> \"Minor impact, easily reversible\" <code>SAFE</code> \"No significant impact expected\""},{"location":"security/risk-assessment/#examples","title":"Examples","text":""},{"location":"security/risk-assessment/#basic-assessment","title":"Basic Assessment","text":"<pre><code>from rlm_code.rlm.approval import RiskAssessor, ToolRiskLevel\n\nassessor = RiskAssessor()\n\n# SAFE action\nresult = assessor.assess({\"action\": \"code\", \"code\": \"print('hello')\"})\nassert result.level == ToolRiskLevel.SAFE\n\n# MEDIUM action (file write)\nresult = assessor.assess({\n    \"action\": \"code\",\n    \"code\": \"with open('/tmp/output.txt', 'w') as f: f.write('data')\",\n})\nassert result.level == ToolRiskLevel.MEDIUM\nassert \"File modification may overwrite existing content\" in result.reasons\nassert result.reversible is True\n\n# CRITICAL action (recursive delete)\nresult = assessor.assess({\n    \"action\": \"code\",\n    \"code\": \"import subprocess; subprocess.run(['rm', '-rf', '/home/user/data'])\",\n})\nassert result.level == ToolRiskLevel.CRITICAL\nassert result.reversible is False\nassert \"file:/home/user/data\" in result.affected_resources\n</code></pre>"},{"location":"security/risk-assessment/#multiple-rules-triggered","title":"Multiple Rules Triggered","text":"<pre><code># This code triggers both file_delete (HIGH) and subprocess_exec (MEDIUM)\nresult = assessor.assess({\n    \"action\": \"code\",\n    \"code\": \"\"\"\nimport subprocess\nimport os\nsubprocess.run(['make', 'clean'])\nos.remove('/tmp/build.log')\n\"\"\",\n})\n# level = HIGH (highest of MEDIUM and HIGH)\n# reasons = [\n#     \"File deletion may cause data loss\",        # file_delete\n#     \"Executing system commands\",                  # subprocess_exec\n# ]\n# reversible = False (file_delete is not reversible)\n</code></pre>"},{"location":"security/risk-assessment/#custom-risk-rules","title":"Custom Risk Rules","text":""},{"location":"security/risk-assessment/#adding-pattern-based-rules","title":"Adding Pattern-Based Rules","text":"<pre><code>from rlm_code.rlm.approval import RiskAssessor\nfrom rlm_code.rlm.approval.policy import RiskRule, ToolRiskLevel\n\nassessor = RiskAssessor()\n\n# Add a rule for detecting API key exposure\nassessor.add_rule(RiskRule(\n    name=\"api_key_exposure\",\n    pattern=r\"(API_KEY|SECRET_KEY|PRIVATE_KEY|ACCESS_TOKEN)\\s*=\",\n    risk_level=ToolRiskLevel.HIGH,\n    reason=\"Potential hardcoded API key or secret detected\",\n    reversible=True,\n))\n\n# Add a rule for database modifications\nassessor.add_rule(RiskRule(\n    name=\"db_insert\",\n    pattern=r\"INSERT\\s+INTO\",\n    risk_level=ToolRiskLevel.MEDIUM,\n    reason=\"Database insertion detected\",\n    reversible=True,\n))\n\n# Add a rule for Docker operations\nassessor.add_rule(RiskRule(\n    name=\"docker_run\",\n    pattern=r\"docker\\s+(run|exec|build)\",\n    risk_level=ToolRiskLevel.MEDIUM,\n    reason=\"Docker container operation detected\",\n    reversible=True,\n))\n</code></pre>"},{"location":"security/risk-assessment/#adding-callable-based-rules","title":"Adding Callable-Based Rules","text":"<p>For complex risk patterns that cannot be expressed as a single regex, use callable rules:</p> <pre><code>def check_large_file_operation(action: dict) -&gt; bool:\n    \"\"\"Flag operations on files larger than 100MB.\"\"\"\n    code = action.get(\"code\", \"\")\n    # Check for known large file paths\n    large_paths = [\"/data/warehouse/\", \"/backup/\", \"/var/log/\"]\n    return any(path in code for path in large_paths)\n\nassessor.add_rule(RiskRule(\n    name=\"large_file_operation\",\n    pattern=check_large_file_operation,\n    risk_level=ToolRiskLevel.HIGH,\n    reason=\"Operation on potentially large file/directory\",\n    reversible=True,\n))\n\n\ndef check_multiple_system_calls(action: dict) -&gt; bool:\n    \"\"\"Flag code with more than 3 subprocess calls.\"\"\"\n    code = action.get(\"code\", \"\")\n    import re\n    matches = re.findall(r\"subprocess\\.(run|call|Popen)|os\\.system\", code)\n    return len(matches) &gt; 3\n\nassessor.add_rule(RiskRule(\n    name=\"excessive_system_calls\",\n    pattern=check_multiple_system_calls,\n    risk_level=ToolRiskLevel.HIGH,\n    reason=\"Multiple system command executions detected (potential script injection)\",\n    reversible=True,\n))\n</code></pre>"},{"location":"security/risk-assessment/#removing-rules","title":"Removing Rules","text":"<pre><code># Remove a rule by name\nremoved = assessor.remove_rule(\"print_output\")\n# removed = True (rule existed and was removed)\n\nremoved = assessor.remove_rule(\"nonexistent\")\n# removed = False (no rule with that name)\n</code></pre>"},{"location":"security/risk-assessment/#replacing-the-entire-rule-set","title":"Replacing the Entire Rule Set","text":"<pre><code>from rlm_code.rlm.approval.policy import RiskRule, ToolRiskLevel\n\n# Start from scratch with a minimal rule set\nminimal_rules = [\n    RiskRule(\n        name=\"any_file_operation\",\n        pattern=r\"(open|Path|os\\.(remove|unlink)|shutil)\",\n        risk_level=ToolRiskLevel.MEDIUM,\n        reason=\"File operation detected\",\n        reversible=True,\n    ),\n    RiskRule(\n        name=\"any_network\",\n        pattern=r\"(requests|urllib|httpx|socket)\",\n        risk_level=ToolRiskLevel.HIGH,\n        reason=\"Network operation detected\",\n        reversible=False,\n    ),\n]\n\nassessor = RiskAssessor(rules=minimal_rules)\n</code></pre>"},{"location":"security/risk-assessment/#custom-assessor-function","title":"Custom Assessor Function","text":"<p>For complete control over risk assessment, provide a custom assessor function that bypasses the rule engine entirely:</p> <pre><code>from rlm_code.rlm.approval import RiskAssessor, RiskAssessment, ToolRiskLevel\n\ndef my_custom_assessor(action: dict) -&gt; RiskAssessment:\n    \"\"\"Domain-specific risk assessment for a financial application.\"\"\"\n    code = action.get(\"code\", \"\")\n\n    # Financial-specific checks\n    if \"transfer\" in code.lower() or \"withdraw\" in code.lower():\n        return RiskAssessment(\n            level=ToolRiskLevel.CRITICAL,\n            reasons=[\"Financial transaction detected\"],\n            affected_resources=[\"financial_system\"],\n            reversible=False,\n            estimated_impact=\"Direct financial impact\",\n            recommendations=[\n                \"Verify transaction amount and recipient\",\n                \"Check authorization level\",\n            ],\n        )\n\n    if \"balance\" in code.lower() or \"account\" in code.lower():\n        return RiskAssessment(\n            level=ToolRiskLevel.MEDIUM,\n            reasons=[\"Account data access detected\"],\n            affected_resources=[\"account_database\"],\n            reversible=True,\n            estimated_impact=\"Potential PII exposure\",\n            recommendations=[\"Ensure proper access logging\"],\n        )\n\n    return RiskAssessment(\n        level=ToolRiskLevel.SAFE,\n        reasons=[\"No financial operations detected\"],\n        reversible=True,\n    )\n\nassessor = RiskAssessor(custom_assessor=my_custom_assessor)\n</code></pre> <p>Custom assessor responsibility</p> <p>When using a custom assessor function, the default rules are completely bypassed. Your custom function is solely responsible for all risk evaluation. Ensure it covers all relevant risk categories for your application.</p>"},{"location":"tui/","title":"\ud83d\udda5\ufe0f Terminal User Interface","text":"<p>RLM Code ships with a single unified TUI built on Textual and Rich. It provides a complete research-grade development environment with 5 tabs including a dedicated \ud83d\udd2c Research tab for experiment tracking, trajectory viewing, benchmarks, session replay, and live event streaming.</p>"},{"location":"tui/#launch","title":"\ud83d\ude80 Launch","text":"<pre><code>rlm-code\n</code></pre> <p>That's it. One command, one TUI, everything in one place.</p> <p>\ud83d\udce6 Dependency</p> <p>Textual is a required dependency of RLM Code and is installed automatically with <code>pip install rlm-code</code>.</p>"},{"location":"tui/#the-five-tabs","title":"\ud83d\uddc2\ufe0f The Five Tabs","text":"Tab Shortcut F-Key Purpose \ud83d\udd01 RLM <code>Ctrl+1</code> <code>F2</code> Converse with LLMs, run slash commands \ud83d\udcc1 Files <code>Ctrl+2</code> <code>F3</code> Browse project tree, syntax-highlighted preview (draggable split in one-screen mode) \ud83d\udccb Details <code>Ctrl+3</code> <code>F4</code> Status panel, snapshot diff viewer \u26a1 Shell <code>Ctrl+4</code> <code>F5</code> Persistent stateful shell (env preserved) \ud83d\udd2c Research <code>Ctrl+5</code> <code>F6</code> Dashboard, trajectory, benchmarks, replay, events <p>Switch tabs with keyboard shortcuts, <code>Tab</code> / <code>Shift+Tab</code> to cycle, or click the focus bar buttons below the header.</p>"},{"location":"tui/#layout-modes","title":"\ud83d\udcd0 Layout Modes","text":""},{"location":"tui/#one-screen-mode-default","title":"One-Screen Mode (default)","text":"<p>Only the active tab is visible, maximizing screen real estate. Toggle with <code>Ctrl+O</code> or <code>/layout single</code>.</p>"},{"location":"tui/#multi-pane-mode","title":"Multi-Pane Mode","text":"<p>All panes visible simultaneously. Toggle with <code>Ctrl+O</code> or <code>/layout multi</code>. Individual panes can be shown/hidden with <code>/pane</code>.</p>"},{"location":"tui/#research-tab","title":"\ud83d\udd2c Research Tab","text":"<p>The Research tab is where experiment data lives. It has 5 internal sub-tabs:</p> Sub-Tab What It Shows \ud83d\udcca Dashboard Run ID, status, reward, steps, tokens, cost, reward sparkline \ud83d\udcc8 Trajectory Step-by-step timeline showing action, reward, tokens, success \ud83c\udfc6 Benchmarks Leaderboard table from <code>/rlm bench</code> runs \u23ea Replay Step-through controls for time-travel debugging \ud83d\udce1 Events Live event stream from the RLM event bus <p>\ud83d\udd2c See It in Action</p> <ol> <li>Run <code>/rlm bench preset=dspy_quick</code> in the RLM tab</li> <li>Press <code>Ctrl+5</code> to switch to Research</li> <li>Dashboard populates with real run metrics and sparkline</li> <li>Click Trajectory to see the step-by-step breakdown</li> </ol> <p>See \ud83d\udd2c Research Tab for full details.</p> <p>Mode Routing in RLM</p> <p>In Local/BYOK connection modes, the RLM tab can auto-route likely coding prompts to the harness loop. ACP is supported via <code>/connect acp</code>, but ACP keeps harness auto-routing off by default. For controlled pure-recursive experiments, set <code>RLM_TUI_HARNESS_AUTO=0</code> before launching the TUI.</p>"},{"location":"tui/#keyboard-shortcuts","title":"\u2328\ufe0f Keyboard Shortcuts","text":""},{"location":"tui/#tab-switching","title":"\ud83d\uddc2\ufe0f Tab Switching","text":"Shortcut Action <code>Ctrl+1</code> / <code>F2</code> \ud83d\udd01 RLM <code>Ctrl+2</code> / <code>F3</code> \ud83d\udcc1 Files <code>Ctrl+3</code> / <code>F4</code> \ud83d\udccb Details <code>Ctrl+4</code> / <code>F5</code> \u26a1 Shell <code>Ctrl+5</code> / <code>F6</code> \ud83d\udd2c Research <code>Tab</code> Cycle to next tab <code>Shift+Tab</code> Cycle to previous tab <code>Escape</code> Back to RLM"},{"location":"tui/#actions","title":"\u26a1 Actions","text":"Shortcut Action <code>F7</code> / <code>Ctrl+Y</code> \ud83d\udccb Copy last response <code>Ctrl+O</code> \ud83d\udd00 Toggle one-screen mode <code>Ctrl+K</code> \ud83d\udd0d Open command palette <code>Ctrl+G</code> \ud83d\udd01 Focus RLM input <code>Ctrl+L</code> \ud83e\uddf9 Clear logs <code>Ctrl+R</code> \ud83d\udd04 Refresh preview <code>Ctrl+Q</code> \ud83d\udeaa Quit"},{"location":"tui/#pane-toggles-multi-pane-mode","title":"\ud83d\udccc Pane Toggles (Multi-Pane Mode)","text":"Shortcut Action <code>Ctrl+B</code> Toggle Files pane <code>Ctrl+J</code> Toggle Details pane <code>Ctrl+T</code> Toggle Shell pane"},{"location":"tui/#theme","title":"\ud83c\udfa8 Theme","text":"<p>The TUI uses a true-black background (<code>#010101</code>) with a purple accent palette inspired by the research aesthetic:</p> Element Color Hex Background Near-black <code>#010101</code> Pane borders Purple-blue <code>#2f6188</code> Accent Purple <code>#7c3aed</code> Active accent Bright purple <code>#a78bfa</code> Title text Cyan <code>#8de7ff</code> RLM text Light blue-white <code>#dce7f3</code>"},{"location":"tui/#widget-library","title":"\ud83e\udde9 Widget Library","text":"<p>Both standard panes and the Research tab draw from a shared widget library:</p> <ul> <li>\ud83c\udfad Animated: ThinkingSpinner, ProgressPulse, SparklineChart, TypewriterText, RewardFlash, StatusIndicator</li> <li>\ud83d\udcca Panels: FileBrowser, CodePreview, ResponseArea, PromptBox, MetricsPanel, TimelinePanel, LeaderboardPanel</li> </ul> <p>See \ud83e\udde9 Widgets for the full API reference.</p>"},{"location":"tui/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>\ud83d\udccb Tab Reference: Detailed docs for each tab (RLM, Files, Details, Shell)</li> <li>\ud83d\udd2c Research Tab: Dashboard, trajectory, replay, events</li> <li>\ud83e\udde9 Widgets: Full widget API reference</li> <li>\ud83c\udfa8 Theme System: Colors, icons, animation constants</li> </ul>"},{"location":"tui/research/","title":"\ud83d\udd2c Research Tab","text":"<p>The Research tab is the 5th tab in the RLM Code TUI, accessible via <code>Ctrl+5</code> / <code>F6</code>. It provides a dedicated space for experiment tracking, trajectory viewing, benchmarks, session replay, and live event streaming, all wired to real data from the RLM runner.</p>"},{"location":"tui/research/#how-to-access","title":"\ud83d\udccd How to Access","text":"Method Action \u2328\ufe0f Keyboard <code>Ctrl+5</code> or <code>F6</code> \ud83d\uddb1\ufe0f Click Click \ud83d\udd2c Research in the focus bar \ud83d\udcac Command <code>/view research</code>"},{"location":"tui/research/#sub-tabs","title":"\ud83d\uddc2\ufe0f Sub-Tabs","text":"<p>The Research tab organizes data across 5 sub-tabs, each shown as a button bar at the top of the pane. Click a sub-tab to switch views.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83d\udd2c Research                                         \u2502\n\u2502 [Dashboard] [Trajectory] [Benchmarks] [Replay] [Events] \u2502\n\u2502                                                     \u2502\n\u2502  \u250c\u2500 Content area (changes per sub-tab) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2502                                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tui/research/#dashboard","title":"\ud83d\udcca Dashboard","text":"<p>The default sub-tab. Shows a high-level summary of the most recent RLM run.</p>"},{"location":"tui/research/#widgets","title":"Widgets","text":"Widget What It Shows \ud83c\udff7\ufe0f MetricsPanel Run ID, status (color-coded), cumulative reward, step count, tokens, cost, duration \ud83d\udcc8 SparklineChart ASCII reward curve using Unicode block characters (<code>\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588</code>) \ud83d\udcdd Summary One-line result summary of the run"},{"location":"tui/research/#how-it-populates","title":"How It Populates","text":"<ol> <li>Run <code>/rlm run \"your task\"</code> or <code>/rlm bench preset=dspy_quick</code> in the RLM tab</li> <li>When the run completes, the Dashboard auto-populates via <code>build_run_visualization()</code></li> <li>The MetricsPanel updates its reactive properties (run_id, status, reward, steps, etc.)</li> <li>The SparklineChart fills with cumulative reward values from the trajectory</li> </ol> <p>\ud83d\udcca Live Updates</p> <p>During an active run, the SparklineChart updates in real-time as each iteration completes and emits an <code>ITERATION_END</code> event.</p>"},{"location":"tui/research/#data-source","title":"Data Source","text":"<pre><code>from rlm_code.rlm.visualizer import build_run_visualization\n\nviz = build_run_visualization(run_path=run_path, run_dir=run_path.parent)\n# viz[\"run_id\"], viz[\"status\"], viz[\"total_reward\"],\n# viz[\"step_count\"], viz[\"reward_curve\"], viz[\"timeline\"]\n</code></pre>"},{"location":"tui/research/#trajectory","title":"\ud83d\udcc8 Trajectory","text":"<p>Step-by-step timeline of the RLM run, showing what the agent did at each step.</p>"},{"location":"tui/research/#table-columns","title":"Table Columns","text":"Column Description \ud83d\udd22 Step Step number \u26a1 Action Action type (e.g., <code>code_generation</code>, <code>validation</code>) \ud83c\udfc6 Reward Step reward (color-coded: \ud83d\udfe2 positive, \ud83d\udd34 negative) \ud83d\udd24 Tokens Tokens consumed in this step \u2705 Success Whether the step succeeded"},{"location":"tui/research/#how-it-populates_1","title":"How It Populates","text":"<p>After any <code>/rlm run</code> or <code>/rlm bench</code> command, the trajectory is extracted from <code>build_run_visualization()[\"timeline\"]</code> and rendered as a Rich table.</p>"},{"location":"tui/research/#benchmarks","title":"\ud83c\udfc6 Benchmarks","text":"<p>Displays the leaderboard table from benchmark runs.</p>"},{"location":"tui/research/#what-you-see","title":"What You See","text":"<p>A Rich table ranked by reward, showing:</p> Column Description \ud83c\udfc5 Rank Position on the leaderboard \ud83c\udff7\ufe0f ID Run identifier \ud83c\udf0d Environment pure_rlm, codeact, or generic \ud83e\udd16 Model Model used \ud83c\udfc6 Reward Average reward (color-coded) \ud83d\udcca Completion Completion rate \ud83d\udd22 Steps Step count \ud83d\udd24 Tokens Total tokens"},{"location":"tui/research/#how-it-populates_2","title":"How It Populates","text":"<p>Run <code>/rlm bench preset=&lt;name&gt;</code> then switch to Research \u2192 Benchmarks. The data comes from:</p> <pre><code>from rlm_code.rlm.leaderboard import Leaderboard\n\nlb = Leaderboard(workdir=Path.cwd() / \".rlm_code\", auto_load=True)\ntable = lb.format_rich_table(limit=15)\n</code></pre>"},{"location":"tui/research/#replay","title":"\u23ea Replay","text":"<p>Step-through controls for time-travel debugging of any RLM run.</p>"},{"location":"tui/research/#controls","title":"Controls","text":"Button Action <code>\\|&lt;</code> \u23ee\ufe0f Jump to first step <code>&lt;</code> \u25c0\ufe0f Step backward <code>&gt;</code> \u25b6\ufe0f Step forward <code>&gt;\\|</code> \u23ed\ufe0f Jump to last step"},{"location":"tui/research/#what-you-see_1","title":"What You See","text":"<ul> <li>Step position: <code>Step 3/8</code> indicator</li> <li>Step detail: Action code with syntax highlighting, output, reward, cumulative reward</li> <li>Reward curve: SparklineChart showing the full reward trajectory with current position</li> </ul>"},{"location":"tui/research/#how-it-populates_3","title":"How It Populates","text":"<p>Run <code>/rlm status</code> to get a run id, then <code>/rlm replay &lt;run_id&gt;</code>. The TUI automatically switches to Research \u2192 Replay and loads the session:</p> <pre><code>from rlm_code.rlm.session_replay import SessionReplayer\n\nreplayer = SessionReplayer.from_jsonl(run_path)\nreplayer.step_forward()    # advance one step\nreplayer.step_backward()   # go back one step\nreplayer.goto_step(n)      # jump to step n\n</code></pre>"},{"location":"tui/research/#reward-color-coding","title":"Reward Color Coding","text":"Reward Color &gt;= 0.8 \ud83d\udfe2 Bright green &gt;= 0.5 \ud83d\udfe2 Green &gt;= 0.3 \ud83d\udfe1 Yellow &gt;= 0.0 \ud83d\udfe0 Orange &lt; 0.0 \ud83d\udd34 Red"},{"location":"tui/research/#events","title":"\ud83d\udce1 Events","text":"<p>Live event stream from the RLM event bus, showing real-time progress during active runs.</p>"},{"location":"tui/research/#what-you-see_2","title":"What You See","text":"<p>A <code>RichLog</code> widget that streams formatted events with timestamps:</p> <pre><code>[14:23:01] \ud83d\udfe2 RUN_START - Starting run abc123 (pure_rlm)\n[14:23:02] \ud83d\udd35 ITERATION_START - Step 1/8\n[14:23:04] \ud83d\udfe1 LLM_CALL - Calling claude-sonnet-4-20250514 (450 tokens)\n[14:23:06] \ud83d\udfe2 ITERATION_END - Step 1 complete (reward: +0.15)\n[14:23:08] \ud83d\udfe2 RUN_END - Run complete (total reward: 0.72)\n</code></pre>"},{"location":"tui/research/#event-types","title":"Event Types","text":"<p>The event bus supports 27+ event types including:</p> Event Description <code>RUN_START</code> / <code>RUN_END</code> \ud83c\udfc1 Run lifecycle <code>ITERATION_START</code> / <code>ITERATION_END</code> \ud83d\udd04 Step lifecycle <code>LLM_CALL</code> / <code>LLM_RESPONSE</code> \ud83e\udd16 Model interactions <code>SANDBOX_EXEC</code> \ud83d\udce6 Code execution <code>REWARD_COMPUTED</code> \ud83c\udfc6 Reward calculation <code>MEMORY_COMPACTED</code> \ud83e\uddf9 Memory compaction <code>APPROVAL_REQUESTED</code> \ud83d\udd12 HITL gates"},{"location":"tui/research/#thread-safety","title":"Thread Safety","text":"<p>Events flow from the RLM runner (which runs in a worker thread) to the UI via <code>call_from_thread()</code> for thread-safe rendering:</p> <pre><code>def _on_raw_rlm_event(self, event):\n    self.call_from_thread(self._on_rlm_event, event)\n</code></pre>"},{"location":"tui/research/#integration-with-slash-commands","title":"\ud83d\udd17 Integration with Slash Commands","text":"<p>The Research tab auto-updates when you run RLM commands in the RLM tab:</p> Command What Updates <code>/rlm run \"...\"</code> \ud83d\udcca Dashboard + \ud83d\udcc8 Trajectory + \ud83d\udce1 Events <code>/rlm bench preset=...</code> \ud83d\udcca Dashboard + \ud83d\udcc8 Trajectory + \ud83c\udfc6 Benchmarks + \ud83d\udce1 Events <code>/rlm replay</code> \u23ea Replay (auto-switches to Replay sub-tab) <code>/rlm bench compare ...</code> \ud83c\udfc6 Benchmarks + compare summary"},{"location":"tui/research/#visual-design","title":"\ud83c\udfa8 Visual Design","text":"<p>The Research tab inherits the TUI's purple-accented dark theme with additional styling for research-specific elements:</p> Element Style Sub-tab buttons Active = <code>primary</code> variant, Inactive = <code>default</code> Metrics panel Titled Rich Panel with color-coded status Sparkline Unicode block chars with reward-based colors Event log Black background, light text, markup-enabled Replay controls Compact button row with step position indicator"},{"location":"tui/research/#widgets-used","title":"\ud83d\udcca Widgets Used","text":"Widget Module Purpose <code>MetricsPanel</code> <code>rlm_code.rlm.research_tui.widgets.panels</code> Run dashboard metrics <code>SparklineChart</code> <code>rlm_code.rlm.research_tui.widgets.animated</code> Reward curve visualization <code>RichLog</code> <code>textual.widgets</code> Event stream display <code>Static</code> <code>textual.widgets</code> Trajectory table, summary, replay detail <code>Button</code> <code>textual.widgets</code> Sub-tab buttons, replay controls"},{"location":"tui/tabs/","title":"\ud83d\udccb Tab Reference","text":"<p>The RLM Code TUI has 5 tabs, each accessible via keyboard shortcuts or the focus bar. This page documents the first four standard tabs. For the Research tab, see \ud83d\udd2c Research Tab.</p>"},{"location":"tui/tabs/#rlm-tab","title":"\ud83d\udd01 RLM Tab","text":"<p>The central hub for interacting with LLMs and running slash commands.</p> Shortcut <code>Ctrl+1</code> / <code>F2</code> Module <code>rlm_code.ui.tui_app</code>"},{"location":"tui/tabs/#whats-inside","title":"What's Inside","text":"<ul> <li>Conversation log: <code>RichLog</code> widget showing the conversation</li> <li>RLM input: Text input at the bottom for messages and commands</li> <li>Status strip: Compact one-line status bar above the conversation</li> </ul>"},{"location":"tui/tabs/#message-rendering","title":"Message Rendering","text":"Sender Border Style \ud83d\udc64 You Blue (<code>#59b9ff</code>) White text \ud83e\udd16 Assistant Green (<code>#6fd897</code>) Markdown with model name + elapsed time"},{"location":"tui/tabs/#slash-commands","title":"Slash Commands","text":"<p>Type any <code>/command</code> in the RLM input. All 50+ slash commands work here. Unknown commands are delegated to the full <code>SlashCommandHandler</code>.</p> <p>\u26a1 Shell Shortcut</p> <p>Prefix any message with <code>!</code> to run it as a shell command without switching tabs: <code>!git status</code></p>"},{"location":"tui/tabs/#files-tab","title":"\ud83d\udcc1 Files Tab","text":"<p>Project file browser with syntax-highlighted code preview.</p> Shortcut <code>Ctrl+2</code> / <code>F3</code>"},{"location":"tui/tabs/#whats-inside_1","title":"What's Inside","text":"<ul> <li>Directory tree: Textual <code>DirectoryTree</code> rooted at the working directory</li> <li>Code preview: Syntax-highlighted file viewer with Monokai theme</li> <li>Draggable split: In one-screen Files view, drag the vertical splitter between tree and preview to resize both areas</li> </ul>"},{"location":"tui/tabs/#supported-languages","title":"Supported Languages","text":"Extension Language Extension Language <code>.py</code> Python <code>.ts</code> TypeScript <code>.js</code> JavaScript <code>.tsx</code> TSX <code>.json</code> JSON <code>.yaml</code> / <code>.yml</code> YAML <code>.toml</code> TOML <code>.md</code> Markdown <code>.sh</code> Bash <code>.txt</code> Plain text <p>Click a file in the tree to preview it. Line numbers and indent guides are enabled.</p>"},{"location":"tui/tabs/#resizing-the-files-layout","title":"Resizing the Files Layout","text":"<ul> <li>Switch to Files (<code>Ctrl+2</code>) in one-screen mode.</li> <li>Drag the vertical divider between the left file tree and right preview panel.</li> <li>The sidebar width updates live; preview width adjusts automatically.</li> </ul>"},{"location":"tui/tabs/#details-tab","title":"\ud83d\udccb Details Tab","text":"<p>Status panel and diff viewer.</p> Shortcut <code>Ctrl+3</code> / <code>F4</code>"},{"location":"tui/tabs/#whats-inside_2","title":"What's Inside","text":"<ul> <li>Status panel: Rich table with workspace, model, provider, ACP, mode, layout info</li> <li>Diff viewer: Unified diff between a snapshot and the current file state</li> </ul>"},{"location":"tui/tabs/#snapshot-diff-workflow","title":"Snapshot / Diff Workflow","text":"<pre><code>/snapshot          # Take a baseline snapshot\n# ... make edits ...\n/diff              # View what changed\n</code></pre> <p>The diff uses the <code>diff</code> syntax highlighter with Monokai theme.</p>"},{"location":"tui/tabs/#shell-tab","title":"\u26a1 Shell Tab","text":"<p>Terminal-first shell workspace for running commands.</p> Shortcut <code>Ctrl+4</code> / <code>F5</code>"},{"location":"tui/tabs/#whats-inside_3","title":"What's Inside","text":"<ul> <li>PTY terminal: Interactive terminal pane with keyboard input, history, and tab completion behavior from your shell</li> <li>Prompt line: Minimal <code>&gt;</code> typing line shown in the terminal status area</li> </ul>"},{"location":"tui/tabs/#persistent-state","title":"\ud83d\udd27 Persistent State","text":"<p>Powered by <code>PersistentShell</code>, a long-running shell process that preserves:</p> <ul> <li>\u2705 Environment variables set by previous commands</li> <li>\u2705 Working directory changes (<code>cd</code>)</li> <li>\u2705 Shell aliases and functions (within the session)</li> <li>\u2705 Exit codes for inline shell shortcuts (<code>!cmd</code>, <code>&gt;cmd</code>) and shell command runner</li> </ul>"},{"location":"tui/tabs/#shell-detection","title":"Shell Detection","text":"<p>Inline shell execution uses a marker-compatible shell backend (<code>zsh</code>, <code>bash</code>, or <code>sh</code>) for reliable completion detection. PTY shell view uses your configured/default shell in interactive mode.</p> <p>This keeps shell output predictable while preserving interactive behavior.</p> <p>When a shell command modifies selected files, preview/diff panes are refreshed automatically.</p>"},{"location":"tui/tabs/#command-palette","title":"\ud83d\udd0d Command Palette","text":"<p>Open with <code>Ctrl+K</code>.</p> <p>A fuzzy-search modal that lists all available slash commands. Type to filter, arrow keys to navigate, Enter to select.</p>"},{"location":"tui/tabs/#available-palette-commands","title":"Available Palette Commands","text":"<pre><code>/help  /workflow  /connect  /models  /status  /sandbox  /rlm  /rml  /harness\n/clear  /snapshot  /diff  /view  /layout  /pane\n/copy  /focus  /exit\n</code></pre> <p>Features:</p> <ul> <li>\ud83d\udd0e Fuzzy text matching</li> <li>\u2b06\ufe0f\u2b07\ufe0f Arrow-key navigation</li> <li>\u23ce Enter to select, Esc to close</li> <li>Up to 16 results displayed</li> </ul>"},{"location":"tui/tabs/#connect-wizard","title":"\ud83d\udd17 Connect Wizard","text":"<p>Launched with <code>/connect</code> (no arguments).</p> <p>A multi-step keyboard-driven picker that guides you through:</p> <ol> <li>\ud83d\udd0c Connection mode: Local models, BYOK cloud providers, or ACP profiles</li> <li>\ud83c\udfe2 Provider selection: Available providers with live/preset status</li> <li>\ud83e\udd16 Model selection: Provider-specific model list</li> </ol>"},{"location":"tui/tabs/#connection-modes","title":"Connection Modes","text":"Mode Description \ud83c\udfe0 Local Ollama, LM Studio, vLLM, SGLang, MLX \ud83d\udd11 BYOK OpenAI, Anthropic, Gemini, DeepSeek, Groq \ud83d\udd17 ACP Agent Coding Profile connections"},{"location":"tui/tabs/#direct-connection","title":"Direct Connection","text":"<pre><code>/connect &lt;provider&gt; &lt;model&gt; [api-key] [base-url]\n</code></pre> <p>Examples:</p> <pre><code>/connect ollama llama3.2:3b\n/connect openai gpt-4o sk-...\n/connect anthropic claude-sonnet-4-5-20250929\n</code></pre> <p>\ud83d\udd04 Auto-Connect</p> <p>If your <code>rlm_config.yaml</code> specifies a <code>default_model</code>, the TUI automatically connects to it on startup.</p>"},{"location":"tui/tabs/#greeting-detection","title":"\ud83d\udc4b Greeting Detection","text":"<p>The TUI detects simple greetings (hi, hello, hey, yo, sup) and responds instantly without an LLM call:</p> <pre><code>Hey. I am here and ready. Tell me what you want to build.\n</code></pre> <p>This avoids unnecessary API calls for trivial interactions.</p>"},{"location":"tui/tabs/#slash-command-reference","title":"\ud83d\udccb Slash Command Reference","text":"Command Description <code>/help</code> \ud83d\udcd6 Show all commands and shortcuts <code>/workflow</code> \ud83e\udded Show recommended RLM workflow steps <code>/connect</code> \ud83d\udd17 Launch connect wizard <code>/connect &lt;provider&gt; &lt;model&gt; ...</code> \ud83d\udd17 Direct model connection <code>/models</code> \ud83d\udccb List all providers and models <code>/status</code> \ud83d\udcca Refresh status panel <code>/sandbox</code> \ud83d\udce6 Sandbox status, doctor, runtime switch, profile/backend controls <code>/rlm</code> \ud83e\udde0 RLM runner (run, bench, status, replay, doctor, chat, observability) <code>/rml</code> \ud83e\udde0 Alias for <code>/rlm</code> <code>/harness</code> \ud83d\udee0 Tool-using coding harness (<code>tools</code>, <code>doctor</code>, <code>run</code>) <code>/clear</code> \ud83e\uddf9 Clear chat and shell logs <code>/snapshot [file]</code> \ud83d\udcf8 Take baseline snapshot for diffing <code>/diff [file]</code> \ud83d\udd0d Show diff against snapshot <code>/view &lt;tab&gt;</code> \ud83d\uddc2\ufe0f Switch active tab <code>/layout &lt;single\\|multi&gt;</code> \ud83d\udcd0 Switch layout mode <code>/pane &lt;name&gt; [show\\|hide]</code> \ud83d\udccc Toggle individual panes <code>/focus &lt;chat\\|default&gt;</code> \ud83c\udfaf Focus mode <code>/copy</code> \ud83d\udccb Copy last response to clipboard <code>/shell &lt;cmd&gt;</code> \u26a1 Run shell command <code>/exit</code> \ud83d\udeaa Quit the TUI"},{"location":"tui/tabs/#environment-variables","title":"\u2699\ufe0f Environment Variables","text":"Variable Default Description <code>RLM_TUI_HISTORY_ITEMS</code> <code>4</code> \ud83d\udcdd Number of history items in context <code>RLM_TUI_HISTORY_ITEM_CHARS</code> <code>320</code> \ud83d\udcdd Max chars per history item <code>RLM_TUI_HISTORY_TOTAL_CHARS</code> <code>1800</code> \ud83d\udcdd Max total chars for history <code>RLM_TUI_THINK_TICK</code> <code>0.08</code> \u23f1\ufe0f Thinking animation refresh interval (sec) <code>RLM_TUI_EVENT_FLUSH_SECONDS</code> <code>0.12</code> \ud83d\udce1 Event log batch flush cadence <code>RLM_TUI_EVENT_BATCH_LIMIT</code> <code>32</code> \ud83d\udce1 Max events per flush batch <code>RLM_TUI_ACP_DISCOVERY_TIMEOUT_SECONDS</code> <code>0.45</code> \ud83d\udd0c ACP discovery timeout <code>RLM_TUI_ACP_CACHE_TTL_SECONDS</code> <code>30</code> \ud83d\udd0c ACP discovery cache TTL <code>RLM_TUI_HARNESS_AUTO</code> <code>1</code> \ud83d\udee0 Enable automatic harness routing for coding tasks <code>RLM_TUI_HARNESS_AUTO_MCP</code> <code>1</code> \ud83d\udee0 Include MCP tools in auto harness route <code>RLM_TUI_HARNESS_AUTO_STEPS</code> <code>8</code> \ud83d\udee0 Max steps for auto harness runs <code>RLM_TUI_HARNESS_PREVIEW_STEPS</code> <code>6</code> \ud83d\udee0 Steps shown in harness preview <code>RLM_TUI_INPUT_DEBOUNCE_SECONDS</code> <code>0.0</code> \u2328\ufe0f Input debounce delay <code>RLM_TUI_CHAT_MAX_LINES</code> <code>2200</code> \ud83d\udd01 RLM log line cap <code>RLM_TUI_TOOL_MAX_LINES</code> <code>1600</code> \ud83e\uddf0 Tool log line cap <code>RLM_TUI_EVENT_MAX_LINES</code> <code>3200</code> \ud83d\udce1 Event log line cap"},{"location":"tui/tabs/#copy-to-clipboard","title":"\ud83d\udccb Copy to Clipboard","text":"<p>Copy the last assistant response with <code>F7</code>, <code>Ctrl+Y</code>, <code>/copy</code>, or the Copy button.</p> Platform Tool \ud83c\udf4e macOS <code>pbcopy</code> \ud83d\udc27 Linux <code>wl-copy</code>, <code>xclip</code>, or <code>xsel</code>"},{"location":"tui/theme/","title":"Theme System","text":"<p>The Research TUI theme system centralizes all visual styling into a single module, providing color constants, icon dictionaries, animation parameters, helper functions, and a complete Textual CSS stylesheet.</p>"},{"location":"tui/theme/#module","title":"Module","text":"<pre><code>rlm_code.rlm.research_tui.theme\n</code></pre>"},{"location":"tui/theme/#colorpalette-dataclass","title":"<code>ColorPalette</code> Dataclass","text":"<p>A frozen dataclass containing all color constants used throughout the TUI. The default instance is available as the module-level <code>COLORS</code> variable.</p> <pre><code>from rlm_code.rlm.research_tui.theme import COLORS\n\nprint(COLORS.primary)        # \"#7c3aed\"\nprint(COLORS.success)        # \"#22c55e\"\nprint(COLORS.bg_pure)        # \"#000000\"\n</code></pre>"},{"location":"tui/theme/#color-constants","title":"Color Constants","text":""},{"location":"tui/theme/#backgrounds","title":"Backgrounds","text":"Constant Hex Usage <code>bg_pure</code> <code>#000000</code> Main screen background <code>bg_surface</code> <code>#0d1117</code> Panel backgrounds, sidebar <code>bg_elevated</code> <code>#161b22</code> Code blocks, elevated surfaces <code>bg_highlight</code> <code>#21262d</code> Hover states, selection highlights"},{"location":"tui/theme/#borders","title":"Borders","text":"Constant Hex Usage <code>border_default</code> <code>#30363d</code> Default panel and widget borders <code>border_focus</code> <code>#58a6ff</code> Focused element borders <code>border_muted</code> <code>#21262d</code> Subtle/background borders"},{"location":"tui/theme/#primary-purple-thinkingactive","title":"Primary (Purple -- Thinking/Active)","text":"Constant Hex Usage <code>primary_dark</code> <code>#5b21b6</code> Deep purple for backgrounds <code>primary</code> <code>#7c3aed</code> Standard purple accent <code>primary_bright</code> <code>#a855f7</code> Bright purple for highlights <code>primary_glow</code> <code>#c084fc</code> Glow/shimmer effects"},{"location":"tui/theme/#semantic-colors","title":"Semantic Colors","text":"Constant Hex Usage <code>success</code> <code>#22c55e</code> Positive outcomes, completions <code>success_dark</code> <code>#166534</code> Success backgrounds <code>success_bright</code> <code>#4ade80</code> Bright success highlights <code>error</code> <code>#ef4444</code> Failures, errors <code>error_dark</code> <code>#991b1b</code> Error backgrounds <code>error_bright</code> <code>#f87171</code> Bright error highlights <code>warning</code> <code>#f59e0b</code> Warnings, caution states <code>warning_dark</code> <code>#92400e</code> Warning backgrounds <code>warning_bright</code> <code>#fbbf24</code> Bright warning highlights <code>info</code> <code>#3b82f6</code> Informational states <code>info_dark</code> <code>#1e40af</code> Info backgrounds <code>info_bright</code> <code>#60a5fa</code> Bright info highlights <code>cyan</code> <code>#06b6d4</code> Special highlights <code>cyan_dark</code> <code>#0e7490</code> Cyan backgrounds <code>cyan_bright</code> <code>#22d3ee</code> Bright cyan highlights"},{"location":"tui/theme/#text","title":"Text","text":"Constant Hex Usage <code>text_primary</code> <code>#f8f8f2</code> Main body text <code>text_secondary</code> <code>#8b949e</code> Secondary labels, descriptions <code>text_muted</code> <code>#6e7681</code> Muted text, placeholders <code>text_dim</code> <code>#484f58</code> Very subtle text, decorations"},{"location":"tui/theme/#syntax-highlighting-dracula-inspired","title":"Syntax Highlighting (Dracula-Inspired)","text":"Constant Hex Token Type <code>syntax_keyword</code> <code>#ff79c6</code> Keywords (pink) <code>syntax_string</code> <code>#f1fa8c</code> Strings (yellow) <code>syntax_number</code> <code>#bd93f9</code> Numbers (purple) <code>syntax_function</code> <code>#50fa7b</code> Functions (green) <code>syntax_comment</code> <code>#6272a4</code> Comments (gray-blue) <code>syntax_class</code> <code>#8be9fd</code> Classes (cyan) <code>syntax_operator</code> <code>#ff79c6</code> Operators (pink) <code>syntax_variable</code> <code>#f8f8f2</code> Variables (white)"},{"location":"tui/theme/#icons-dictionary","title":"<code>ICONS</code> Dictionary","text":"<p>A dictionary of 30+ Unicode icons used throughout the TUI.</p> <pre><code>from rlm_code.rlm.research_tui.theme import ICONS\n\nprint(ICONS[\"success\"])   # \"check\"\nprint(ICONS[\"error\"])     # \"cross\"\nprint(ICONS[\"terminal\"])  # \"&gt;\"\n</code></pre>"},{"location":"tui/theme/#full-icon-reference","title":"Full Icon Reference","text":"Key Character Description <code>success</code> <code>check</code> Success/complete <code>error</code> <code>cross</code> Error/failure <code>warning</code> <code>warn</code> Warning <code>info</code> <code>i</code> Information <code>pending</code> <code>o</code> Pending/waiting <code>running</code> <code>*</code> Active/running <code>paused</code> <code>half</code> Paused <code>thinking</code> <code>thought</code> LLM processing <code>code</code> <code>&lt;&gt;</code> Code reference <code>file</code> <code>page</code> File <code>folder</code> <code>folder</code> Closed folder <code>folder_open</code> <code>open</code> Open folder <code>terminal</code> <code>&gt;</code> Terminal prompt <code>play</code> <code>play</code> Start/play <code>pause</code> <code>pause</code> Pause <code>stop</code> <code>stop</code> Stop <code>skip</code> <code>skip</code> Skip forward <code>back</code> <code>back</code> Skip backward <code>refresh</code> <code>loop</code> Refresh <code>settings</code> <code>gear</code> Settings <code>search</code> <code>mag</code> Search <code>filter</code> <code>grid</code> Filter <code>sort</code> <code>updown</code> Sort <code>expand</code> <code>down</code> Expand <code>collapse</code> <code>right</code> Collapse <code>link</code> <code>link</code> Hyperlink <code>copy</code> <code>clip</code> Copy <code>save</code> <code>disk</code> Save <code>load</code> <code>inbox</code> Load <code>export</code> <code>outbox</code> Export <code>chart</code> <code>chart</code> Chart/graph <code>clock</code> <code>timer</code> Time/duration <code>token</code> <code>abc</code> Token count <code>reward</code> <code>star</code> Reward value <code>step</code> <code>arrow</code> Step indicator <code>branch</code> <code>fork</code> Branch <code>merge</code> <code>merge</code> Merge <code>diff</code> <code>+-</code> Diff"},{"location":"tui/theme/#box-characters","title":"<code>BOX</code> Characters","text":"<p>Box-drawing characters for manual border rendering.</p> <pre><code>from rlm_code.rlm.research_tui.theme import BOX\n\n# Standard\nprint(BOX[\"tl\"] + BOX[\"h\"] * 10 + BOX[\"tr\"])  # top-left + horizontal + top-right\n\n# Rounded\nprint(BOX[\"tlr\"] + BOX[\"h\"] * 10 + BOX[\"trr\"])\n</code></pre> Key Char Description Key Char Description <code>h</code> <code>--</code> Horizontal <code>hd</code> <code>==</code> Double horizontal <code>v</code> <code>|</code> Vertical <code>vd</code> <code>||</code> Double vertical <code>tl</code> <code>+</code> Top-left <code>tld</code> <code>+=</code> Double top-left <code>tr</code> <code>+</code> Top-right <code>trd</code> <code>=+</code> Double top-right <code>bl</code> <code>+</code> Bottom-left <code>bld</code> <code>+=</code> Double bottom-left <code>br</code> <code>+</code> Bottom-right <code>brd</code> <code>=+</code> Double bottom-right <code>t</code> <code>T</code> Top tee <code>tlr</code> <code>(</code> Rounded top-left <code>b</code> <code>T</code> Bottom tee <code>trr</code> <code>)</code> Rounded top-right <code>l</code> <code>|-</code> Left tee <code>blr</code> <code>(</code> Rounded bottom-left <code>r</code> <code>-|</code> Right tee <code>brr</code> <code>)</code> Rounded bottom-right <code>c</code> <code>+</code> Cross"},{"location":"tui/theme/#animation-constants","title":"Animation Constants","text":""},{"location":"tui/theme/#spinner-frames","title":"Spinner Frames","text":"<pre><code>from rlm_code.rlm.research_tui.theme import (\n    SPINNER_DOTS,     # Braille dots: [\"...\", \"...\", ...]\n    SPINNER_BRAILLE,  # Braille full: [\"...\", \"...\", ...]\n    SPINNER_ARROWS,   # Arrow cycle: [\"&lt;\", \"nw\", \"^\", \"ne\", \"&gt;\", ...]\n    SPINNER_MOON,     # Moon phases (emoji)\n    SPINNER_PULSE,    # Quarter circles\n)\n</code></pre> Constant Frames Description <code>SPINNER_DOTS</code> 10 Braille dot spinner (default) <code>SPINNER_BRAILLE</code> 8 Full braille characters <code>SPINNER_ARROWS</code> 8 Directional arrows <code>SPINNER_MOON</code> 8 Moon phase emoji <code>SPINNER_PULSE</code> 4 Quarter-circle rotation"},{"location":"tui/theme/#sparkline-characters","title":"Sparkline Characters","text":"<pre><code>SPARKLINE_CHARS = \" ........\"  # 9 levels: space through full block\n</code></pre> <p>Nine Unicode block characters providing smooth value-to-height mapping.</p>"},{"location":"tui/theme/#thinking-gradient","title":"Thinking Gradient","text":"<pre><code>THINKING_GRADIENT = [\n    \"#6d28d9\", \"#7c3aed\", \"#8b5cf6\", \"#a78bfa\",\n    \"#c4b5fd\", \"#a78bfa\", \"#8b5cf6\", \"#7c3aed\",\n]\n</code></pre> <p>An 8-color purple gradient that cycles during thinking animations.</p>"},{"location":"tui/theme/#reward-gradients","title":"Reward Gradients","text":"<pre><code>REWARD_POSITIVE_GRADIENT = [\"#166534\", \"#22c55e\", \"#4ade80\", \"#86efac\"]\nREWARD_NEGATIVE_GRADIENT = [\"#991b1b\", \"#ef4444\", \"#f87171\", \"#fca5a5\"]\n</code></pre>"},{"location":"tui/theme/#progress-bar-characters","title":"Progress Bar Characters","text":"<pre><code>PROGRESS_BLOCKS = [\"...\", \"...\", \"...\", \"...\"]   # 4 density levels\nPROGRESS_SMOOTH = [\"\", \".\", \"..\", \"...\", \"....\", \".....\", \"......\", \".......\", \"........\"]  # 9 sub-character widths\n</code></pre>"},{"location":"tui/theme/#helper-functions","title":"Helper Functions","text":""},{"location":"tui/theme/#sparklinevalues-width","title":"<code>sparkline(values, width)</code>","text":"<p>Generate an ASCII sparkline string from a list of float values.</p> <pre><code>from rlm_code.rlm.research_tui.theme import sparkline\n\nresult = sparkline([0.1, 0.4, 0.8, 0.6, 0.9], width=20)\nprint(result)  # \"               \u2582\u2585\u2588\u2586\u2588\"\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>values</code> <code>list[float]</code> -- Values to visualize <code>width</code> <code>int</code> <code>20</code> Width of the output string <p>Behaviour: Normalizes values to the min-max range, maps each to the nearest sparkline character, right-aligns to the specified width.</p>"},{"location":"tui/theme/#progress_barprogress-width-style","title":"<code>progress_bar(progress, width, style)</code>","text":"<p>Generate a progress bar string.</p> <pre><code>from rlm_code.rlm.research_tui.theme import progress_bar\n\nprint(progress_bar(0.65, width=20))\n# \"............         \"  (smooth sub-character rendering)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>progress</code> <code>float</code> -- Progress value (0.0 to 1.0) <code>width</code> <code>int</code> <code>20</code> Bar width in characters <code>style</code> <code>str</code> <code>\"smooth\"</code> <code>\"smooth\"</code> or <code>\"block\"</code>"},{"location":"tui/theme/#get_status_colorstatus","title":"<code>get_status_color(status)</code>","text":"<p>Map a status string to a color hex code.</p> <pre><code>from rlm_code.rlm.research_tui.theme import get_status_color\n\nprint(get_status_color(\"success\"))  # \"#22c55e\"\nprint(get_status_color(\"running\"))  # \"#a855f7\"\nprint(get_status_color(\"failed\"))   # \"#ef4444\"\n</code></pre> <p>Mapping:</p> Status Color <code>success</code>, <code>complete</code>, <code>completed</code>, <code>done</code>, <code>passed</code> <code>COLORS.success</code> <code>error</code>, <code>failed</code>, <code>failure</code> <code>COLORS.error</code> <code>warning</code> <code>COLORS.warning</code> <code>pending</code>, <code>waiting</code> <code>COLORS.text_muted</code> <code>running</code>, <code>active</code>, <code>thinking</code> <code>COLORS.primary_bright</code> <code>info</code> <code>COLORS.info</code> Other <code>COLORS.text_secondary</code>"},{"location":"tui/theme/#get_reward_colorreward","title":"<code>get_reward_color(reward)</code>","text":"<p>Map a reward float to a color hex code.</p> <pre><code>from rlm_code.rlm.research_tui.theme import get_reward_color\n\nprint(get_reward_color(0.85))  # \"#4ade80\" (success_bright)\nprint(get_reward_color(0.50))  # \"#22c55e\" (success)\nprint(get_reward_color(-0.1))  # \"#ef4444\" (error)\n</code></pre> Reward Range Color &gt;= 0.8 <code>success_bright</code> &gt;= 0.5 <code>success</code> &gt;= 0.3 <code>warning</code> &gt;= 0.0 <code>warning_dark</code> &lt; 0.0 <code>error</code>"},{"location":"tui/theme/#research_tui_css","title":"<code>RESEARCH_TUI_CSS</code>","text":"<p>A complete Textual CSS stylesheet string exported from the theme module, ready for use in any Textual <code>App</code>.</p> <pre><code>from rlm_code.rlm.research_tui.theme import RESEARCH_TUI_CSS\n\nclass MyApp(App):\n    CSS = RESEARCH_TUI_CSS\n</code></pre> <p>The stylesheet covers:</p> <ul> <li>Screen and base panel styling</li> <li>Sidebar layout and navigation items</li> <li>File browser and code preview panels</li> <li>Response area and prompt container</li> <li>Metrics panel and timeline items</li> <li>Status bar and indicators</li> <li>Thinking animation containers</li> <li>Sparkline styling</li> <li>Button variants (default, primary)</li> <li>Collapsible panels</li> <li>Tab bar styling</li> <li>Scrollbar theming</li> </ul> <p>All colors reference the <code>ColorPalette</code> hex values for consistency.</p>"},{"location":"tui/widgets/","title":"Widget Library","text":"<p>The Research TUI widget library provides two categories of reusable Textual widgets: Animated widgets for visual feedback and Panel widgets for structured content display.</p>"},{"location":"tui/widgets/#animated-widgets","title":"Animated Widgets","text":"<p>Module: <code>rlm_code.rlm.research_tui.widgets.animated</code></p> <p>These widgets extend Textual's <code>Static</code> widget and use interval timers for smooth frame-based animation.</p>"},{"location":"tui/widgets/#thinkingspinner","title":"ThinkingSpinner","text":"<p>A purple gradient spinner that indicates LLM processing is in progress.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import ThinkingSpinner\n\nspinner = ThinkingSpinner(status=\"Querying model...\")\n</code></pre> Property Type Default Description <code>is_spinning</code> <code>reactive[bool]</code> <code>False</code> Whether the animation is active <code>status_text</code> <code>reactive[str]</code> <code>\"Thinking...\"</code> Status message displayed next to icon <code>elapsed_seconds</code> <code>reactive[float]</code> <code>0.0</code> Seconds since <code>start()</code> was called <p>Methods:</p> Method Signature Description <code>start()</code> <code>start(status: str)</code> Start spinning with status text <code>stop()</code> <code>stop()</code> Stop the animation <p>Animation Details:</p> <ul> <li>Runs at 15 FPS (<code>set_interval(1/15, ...)</code>)</li> <li>Cycles through <code>SPINNER_DOTS</code> braille characters: <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code>, <code>\"</code></li> <li>Colors cycle through <code>THINKING_GRADIENT</code> (8 purple shades)</li> <li>Shows elapsed time in brackets: <code>[4.2s]</code></li> </ul> <p>Render output:</p> <pre><code>[thinking emoji] [spinner] Querying model...  [4.2s]\n</code></pre>"},{"location":"tui/widgets/#progresspulse","title":"ProgressPulse","text":"<p>A pulsing progress bar with percentage display. The active segment subtly brightens and dims.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import ProgressPulse\n\nbar = ProgressPulse(progress=0.65, label=\"Training\", width=30)\nbar.is_pulsing = True\n</code></pre> Property Type Default Description <code>progress</code> <code>reactive[float]</code> <code>0.0</code> Progress value (0.0 to 1.0) <code>label</code> <code>reactive[str]</code> <code>\"\"</code> Label displayed before the bar <code>is_pulsing</code> <code>reactive[bool]</code> <code>False</code> Whether the pulse animation is on <p>Render output:</p> <pre><code>Training \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 65%\n</code></pre> <p>The pulse animation runs at 20 FPS and modulates the brightness of filled segments between the <code>primary</code> and <code>primary_bright</code> theme colors.</p>"},{"location":"tui/widgets/#sparklinechart","title":"SparklineChart","text":"<p>An ASCII sparkline visualization for reward curves, using Unicode block characters for smooth value representation.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import SparklineChart\n\nchart = SparklineChart(\n    values=[0.1, 0.3, 0.5, 0.7, 0.6, 0.8],\n    width=40,\n    label=\"Reward\",\n    show_range=True,\n)\n</code></pre> Property Type Default Description <code>values</code> <code>reactive[list[float]]</code> <code>[]</code> Data values to visualize <p>Methods:</p> Method Signature Description <code>add_value()</code> <code>add_value(value: float)</code> Append a value (auto-trims) <p>Sparkline Characters:</p> <p>The 9-level sparkline character set provides smooth visualization:</p> <pre><code>\" \" \"\u2581\" \"\u2582\" \"\u2583\" \"\u2584\" \"\u2585\" \"\u2586\" \"\u2587\" \"\u2588\"\n</code></pre> <p>Color coding is based on absolute value:</p> Value Range Color &gt;= 0.7 <code>success_bright</code> (#4ade80) &gt;= 0.4 <code>success</code> (#22c55e) &gt;= 0.2 <code>warning</code> (#f59e0b) &gt;= 0.0 <code>warning_dark</code> (#92400e) &lt; 0.0 <code>error</code> (#ef4444) <p>Render output:</p> <pre><code>Reward \u2581\u2583\u2585\u2587\u2586\u2588 [0.10-0.80] now:0.80\n</code></pre>"},{"location":"tui/widgets/#typewritertext","title":"TypewriterText","text":"<p>Character-by-character text reveal animation, creating a typing effect.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import TypewriterText\n\ntext = TypewriterText(text=\"Analysis complete.\", speed=50.0)\ntext.start_typing()\n</code></pre> Property Type Default Description <code>full_text</code> <code>reactive[str]</code> <code>\"\"</code> Complete text to reveal <code>is_typing</code> <code>reactive[bool]</code> <code>False</code> Whether typing is in progress <p>Methods:</p> Method Signature Description <code>start_typing()</code> <code>start_typing(text: str|None)</code> Start the typing animation <code>reveal_all()</code> <code>reveal_all()</code> Show all text immediately <p>Animation Details:</p> <ul> <li>Runs at 60 FPS for smooth character-by-character reveal</li> <li>Speed is configurable in characters per second (default: 50)</li> <li>A blinking cursor (<code>|</code>) appears at the end during typing</li> </ul>"},{"location":"tui/widgets/#rewardflash","title":"RewardFlash","text":"<p>A widget that flashes color when the reward value changes, providing immediate visual feedback on reward deltas.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import RewardFlash\n\nflash = RewardFlash(reward=0.5, show_delta=True)\nflash.reward = 0.7  # triggers green flash\nflash.reward = 0.3  # triggers red flash\n</code></pre> Property Type Default Description <code>reward</code> <code>reactive[float]</code> <code>0.0</code> Current reward value <p>Flash behavior:</p> <ul> <li>Green flash (<code>success_bright</code> on <code>success_dark</code> background) when reward increases</li> <li>Red flash (<code>error_bright</code> on <code>error_dark</code> background) when reward decreases</li> <li>Flash duration: 300ms</li> <li>Delta threshold: changes smaller than 0.001 are ignored</li> <li>When <code>show_delta=True</code>, displays <code>+0.200</code> or <code>-0.200</code> alongside the value</li> </ul>"},{"location":"tui/widgets/#statusindicator","title":"StatusIndicator","text":"<p>A status dot with a label, color-coded by state.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.animated import StatusIndicator\n\nindicator = StatusIndicator(status=\"active\", label=\"MLflow\")\n</code></pre> Property Type Default Description <code>status</code> <code>reactive[str]</code> <code>\"inactive\"</code> Status identifier <code>label</code> <code>reactive[str]</code> <code>\"\"</code> Text label after the dot <p>Status color mapping:</p> Status Icon Color <code>active</code> <code>*</code> Green (<code>#22c55e</code>) <code>connected</code> <code>*</code> Green (<code>#22c55e</code>) <code>running</code> <code>*</code> Purple bright (<code>#a855f7</code>) <code>thinking</code> <code>\"</code> Purple bright (<code>#a855f7</code>) <code>pending</code> <code>o</code> Warning (<code>#f59e0b</code>) <code>waiting</code> <code>o</code> Muted (<code>#6e7681</code>) <code>inactive</code> <code>o</code> Dim (<code>#484f58</code>) <code>disabled</code> <code>o</code> Dim (<code>#484f58</code>) <code>error</code> <code>*</code> Red (<code>#ef4444</code>) <code>disconnected</code> <code>*</code> Red (<code>#ef4444</code>)"},{"location":"tui/widgets/#panel-widgets","title":"Panel Widgets","text":"<p>Module: <code>rlm_code.rlm.research_tui.widgets.panels</code></p> <p>Structured content display widgets with Rich rendering.</p>"},{"location":"tui/widgets/#filebrowser","title":"FileBrowser","text":"<p>A directory tree with file-type icons and expandable directories.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import FileBrowser\n\nbrowser = FileBrowser(root=\"/path/to/project\")\n</code></pre> Property Type Default Description <code>root_path</code> <code>reactive[Path]</code> <code>Path.cwd()</code> Root directory for browsing <code>selected_path</code> <code>reactive[None]</code> <code>None</code> Currently selected file <p>File-type icons:</p> Extension Icon Description <code>.py</code> snake Python <code>.js</code> scroll JavaScript <code>.ts</code> book TypeScript <code>.json</code> clip JSON <code>.yaml</code> / <code>.yml</code> clip YAML <code>.md</code> memo Markdown <code>.txt</code> page Plain text <code>.sh</code> / <code>.bash</code> gear Shell script <code>.css</code> art Stylesheet <code>.html</code> globe HTML Other file Generic file <p>Messages:</p> <ul> <li><code>FileBrowser.FileSelected(path: Path)</code> -- Posted when a file is selected.</li> </ul> <p>Methods:</p> Method Signature Description <code>toggle_directory()</code> <code>toggle_directory(path: Path)</code> Expand/collapse directory <p>The tree is built recursively up to 3 levels deep, with a maximum of 50 entries per directory. Hidden files (starting with <code>.</code>) are excluded.</p>"},{"location":"tui/widgets/#codepreview","title":"CodePreview","text":"<p>Syntax-highlighted code display with Dracula theme.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import CodePreview\n\npreview = CodePreview(code=\"print('hello')\", language=\"python\", title=\"main.py\")\n</code></pre> Property Type Default Description <code>code</code> <code>reactive[str]</code> <code>\"\"</code> Code content <code>language</code> <code>reactive[str]</code> <code>\"python\"</code> Syntax language <code>title</code> <code>reactive[str]</code> <code>\"Code Preview\"</code> Panel title <code>line_numbers</code> <code>reactive[bool]</code> <code>True</code> Show line numbers <p>Methods:</p> Method Signature Returns Description <code>load_file()</code> <code>load_file(path: Path)</code> <code>bool</code> Load file, detect language <p>Supported languages (16+):</p> <pre><code>LANGUAGE_MAP = {\n    \".py\": \"python\", \".js\": \"javascript\", \".ts\": \"typescript\",\n    \".json\": \"json\", \".yaml\": \"yaml\", \".yml\": \"yaml\",\n    \".md\": \"markdown\", \".sh\": \"bash\", \".bash\": \"bash\",\n    \".css\": \"css\", \".html\": \"html\", \".sql\": \"sql\",\n    \".rs\": \"rust\", \".go\": \"go\", \".java\": \"java\",\n    \".cpp\": \"cpp\", \".c\": \"c\",\n}\n</code></pre> <p>Rendering uses Rich <code>Syntax</code> with: - Theme: Dracula - Background: <code>#161b22</code> - Line numbers: enabled - Word wrap: enabled</p>"},{"location":"tui/widgets/#responsearea","title":"ResponseArea","text":"<p>A collapsible response display that detects and highlights embedded code blocks.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import ResponseArea\n\narea = ResponseArea(response=\"Here is the answer:\\n```python\\nprint(42)\\n```\", title=\"Step 3\")\narea.toggle()  # collapse/expand\n</code></pre> Property Type Default Description <code>response</code> <code>reactive[str]</code> <code>\"\"</code> Response text content <code>is_collapsed</code> <code>reactive[bool]</code> <code>False</code> Collapsed state <code>title</code> <code>reactive[str]</code> <code>\"Response\"</code> Panel title <p>Methods:</p> Method Signature Description <code>toggle()</code> <code>toggle()</code> Toggle between collapsed and expanded <p>Messages:</p> <ul> <li><code>ResponseArea.Toggled(collapsed: bool)</code> -- Posted on state change.</li> </ul> <p>Code block detection:</p> <p>The response text is parsed for triple-backtick code blocks. Each detected block is rendered with Dracula-theme syntax highlighting. Non-code text is rendered as styled Rich <code>Text</code>.</p>"},{"location":"tui/widgets/#promptbox","title":"PromptBox","text":"<p>A user input widget with a prompt symbol, command history, and submit handling.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import PromptBox\n\nprompt = PromptBox(prompt=\"$\", placeholder=\"Type a command...\")\n</code></pre> Property Type Default Description <code>prompt_text</code> <code>reactive[str]</code> <code>\"&gt;\"</code> Prompt symbol <code>placeholder</code> <code>reactive[str]</code> <code>\"Enter command or message...\"</code> Input placeholder <p>Messages:</p> <ul> <li><code>PromptBox.Submitted(value: str)</code> -- Posted when the user submits input.</li> </ul> <p>Command history is maintained internally. Submitted values are appended to <code>_history</code> and the history index is reset.</p>"},{"location":"tui/widgets/#metricspanel","title":"MetricsPanel","text":"<p>A run dashboard displaying key metrics for the current RLM episode.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import MetricsPanel\n\nmetrics = MetricsPanel()\nmetrics.run_id = \"abc123def456\"\nmetrics.status = \"running\"\nmetrics.reward = 0.72\nmetrics.steps = 4\nmetrics.max_steps = 8\nmetrics.tokens = 3200\nmetrics.cost = 0.0045\nmetrics.duration = 12.5\n</code></pre> Property Type Default Description <code>run_id</code> <code>reactive[str]</code> <code>\"\"</code> Run identifier <code>status</code> <code>reactive[str]</code> <code>\"pending\"</code> Run status <code>reward</code> <code>reactive[float]</code> <code>0.0</code> Cumulative reward <code>steps</code> <code>reactive[int]</code> <code>0</code> Current step count <code>max_steps</code> <code>reactive[int]</code> <code>10</code> Maximum step count <code>tokens</code> <code>reactive[int]</code> <code>0</code> Total tokens consumed <code>cost</code> <code>reactive[float]</code> <code>0.0</code> Estimated cost ($) <code>duration</code> <code>reactive[float]</code> <code>0.0</code> Elapsed time (seconds) <p>The panel renders as a titled Rich <code>Panel</code> with rows showing:</p> <ul> <li>Row 1: Run ID (truncated to 12 chars) + status with color-coded icon</li> <li>Row 2: Steps (current/max) + reward (color-coded) + tokens</li> <li>Row 3: Cost + duration</li> </ul>"},{"location":"tui/widgets/#timelinepanel","title":"TimelinePanel","text":"<p>A color-coded step timeline showing action, reward, tokens, and duration per step.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import TimelinePanel\n\ntimeline = TimelinePanel()\ntimeline.add_step({\n    \"success\": True,\n    \"action\": \"code_generation\",\n    \"reward\": 0.15,\n    \"tokens\": 450,\n    \"duration\": 2.3,\n})\n</code></pre> Property Type Default Description <code>steps</code> <code>reactive[list[dict]]</code> <code>[]</code> List of step records <p>Methods:</p> Method Signature Description <code>add_step()</code> <code>add_step(step: dict)</code> Append a step record <p>Each step is rendered as a single line:</p> <pre><code>[check] 1: code_generation  +0.15 [450 tok, 2.3s]\n[cross] 2: validation       -0.05 [120 tok, 0.8s]\n</code></pre> <ul> <li>Success icon: green checkmark</li> <li>Failure icon: red cross</li> <li>Reward: color-coded by <code>get_reward_color()</code></li> <li>Shows the last 10 steps</li> </ul>"},{"location":"tui/widgets/#leaderboardpanel","title":"LeaderboardPanel","text":"<p>A ranking table rendered with Rich <code>Table</code>.</p> <pre><code>from rlm_code.rlm.research_tui.widgets.panels import LeaderboardPanel\n\nboard = LeaderboardPanel()\nboard.entries = [\n    {\"id\": \"run_abc123\", \"environment\": \"pure_rlm\", \"reward\": 0.85, \"steps\": 4},\n    {\"id\": \"run_def456\", \"environment\": \"codeact\", \"reward\": 0.72, \"steps\": 6},\n]\n</code></pre> Property Type Default Description <code>entries</code> <code>reactive[list[dict]]</code> <code>[]</code> Leaderboard entries <p>Table columns:</p> Column Width Style Description <code>#</code> 3 Muted Rank number <code>ID</code> 10 Primary Run ID (first 8 chars) <code>Env</code> 10 Cyan Environment name <code>Reward</code> 8 Color-coded Reward value <code>Steps</code> 6 Secondary Step count <p>Shows the top 10 entries. Reward values are color-coded using <code>get_reward_color()</code>.</p>"}]}